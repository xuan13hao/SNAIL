<html><body link="black" alink="black" vlink="black" hlink="black">
<b>PMC65509</b><br>
Match-Only Integral Distribution (MOID) Algorithm for high-density oligonucleotide array analysis<br>
<br>
<br>
Background<br>
Genomics sequencing projects have rapidly generated tremendous amount of information. At the time of writing, the NCBI <database>UniGene</database> database [1]http://www.ncbi.nlm.nih.gov/UniGene contained 96,109 Homo sapiens clusters and 85,047 Mus musculus clusters. Predictions from the Human Genome Project [2] and Celera Genomics [3] suggest there are about 26,000?40,000 human genes. Other recent studies suggest that these numbers may be an underestimation and that the human genome appears more complicated [4]. Understanding the functions of such a large number of genes has been an unprecedented challenge for functional genomics research. As the array of hope in recent years, gene expression array technology has quickly grown into a powerful tool to chart a gene atlas in various biological sources and under various conditions in a massively parallel manner [5-7]. Facing the challenge of annotating such a huge amount of genomic data, increasing array information density and improving analysis algorithms have become two critical research areas to ensure that gene expression profiling proceeds in an efficient and cost effective manner.<br>
Take an Affymetrix high-density oligonucleotide GeneChip http://www.affymetrix.com for example. Firstly, its human U95 series chip consists of 5 chip types with 12,000 coding clusters each, which makes it expensive to profile all the human genes in samples of interest. Can a gene chip take more genes? Comparing its U95 chip and Human 6800 chip, Affymetrix has already increased chip information density by 20% by reducing the number of probe pairs per gene from 20 to 16. Since demand for higher information density has still not been met, it is of interest to study the probe number effect in detail. Secondly, most optional research efforts focus on the downstream statistical and clustering analysis. However, on the upstream side, Affymetrix chip users are still dependent on the <software>Microarray Suite</software>? software that comes with the measurement system to interpret raw data. The Affymetrix algorithm implemented in its <software>Microarray Suite</software> 4.0 package (referred hereafter as MAS4) uses empirical rules derived from its internal research data to assign absolute calls for the significance of gene presence and assign fold change calls for the significance of expression variations. Such discrete categorizations are not the most appropriate language to describe quantities of continuous nature. Although it is well known that fold change numbers have defined behaviors of uncertainty, there are very few studies in this area. How does one assign statistical significance to expression analysis results? This work presents our preliminary research results for the two questions raised above.<br>
The Affymetrix gene chip layout used in this study contains the same number of perfect match (PM) probes and mismatch (MM) probes. MAS4 uses differences between these two types of probes for gene expression signals. The primary goal of Match-Only Integral Distribution (MOID) algorithm is to discard mismatch information, which allows immediate doubling of the chip information density. In this study, the performance of both algorithms were benchmarked using 366 known fold change values derived from 34 spiking experiments. Their false positive tendencies were assessed by no-change expression experiments. Computer simulations were used to study their noise tolerances, and to determine the minimum number of probes required for chip analysis.<br>
The idea of using PM-only information is based on the following observations: MAS4 essentially discards the one-one correspondence between a PM and its MM partner (for details, see materials and methods on MAS4 algorithm for absolute analysis) and still gives satisfactory interpretation, which suggests the contribution of MM probes might be approximated in a nonspecific manner overall. After we designed the first mismatch-free gene chip (GNF-HS1) in July 1999, the match-only expression analysis idea was proposed in other independent studies as well. Li (submitted, 2001) adjusted their previous model-based analysis of oligonucleotide arrays [8] to PM-only calculations and found their results correlate well with that of using PM and MM information. In addition, the idea is endorsed by recent studies of Naef et al[9] and Irizarry et al (2002, in prepare) http://biosun01.biostat.jhsph.edu/~ririzarr/papers/.<br>
One difficulty in comparing algorithms for gene expression analysis is the lack of "known" results. Here we overcome the problem by resorting to a spiking set and set of no-change experiments, where results are unambiguous. Model-based methods [8] generally require a reasonable numbers of training experiments of the same chip type, among which probes under study give significantly large signals in at least some experiments. Considering the fact that the 34 spiking experiments used were obtained by three different chip types, and in addition some experiments are replicates under different concentrations and hybridization conditions, it is impractical to sufficiently train model-based algorithms in this study. We limit our study to MOID and MAS4, where training is not required.<br>
In the materials and methods section, we summarize Affymetrix chip technologies and describe the MOID algorithms in detail. MAS4 algorithms for both absolute expression analysis and comparison analysis are included afterwards. The benchmarks used for algorithm comparison were explained and comparison results were shown in the results session. Issues such as noise-tolerance of both algorithms and further reduction of the probe set size are also included. We discuss generalization of the normalization algorithm, which may be of interest to other researchers. Finally, the main differences between MAS4 and MOID are summarized in a tabular form as conclusions.<br>
<br>
Results and Discussion<br>
Spiking experiments<br>
Spiking experiments were done by adding to tissue samples a certain number of control genes with known concentrations. Since signal intensity is not exactly proportional to gene concentration across different probe sets, only the fold change values of each gene between comparisons are considered reliable and used to benchmark both algorithms. We are fortunate to have access to 34 spiking experiments done previously in GNF for a hybridization protocol study, where 366 independent known fold change numbers were derived. These experiments covered three Affymetrix chip types: Hu35kSubA, Hu6800, and Mu11kSubA. True experimental fold change values, fexp, range from 1.0 to 10.0. The raw data (34 CEL files) and experimental fold change values are available from http://carrier.gnf.org/publications/MOID.<br>
We applied both algorithms to these array data and the fold change numbers calculated were compared to known experimental values. Because of the way we defined the base and experiment, fexp are always no less than 1.0. For MAS4, we used all the default parameters used by the <software>Microarray Suite</software> 4.0 program in the calculation. For MOID, we found using 70 percentile for both pct and pctf (defined in materials and methods) gave optimal performance. Figure 1 shows the comparison results (data available from http://carrier.gnf.org/publications/MOID). Two algorithms give virtually similar performance.<br>
In the comparison, we defined relative error, Rerr, as the following to benchmark each algorithm,<br>
Rerr1 = &lt;|(fcalc - fexp&gt;)/fexp|&gt;all pairs,<br>
Rerr2 = &lt;|(1/fcalc - 1)/fexp) ? fexp|&gt;all pairs,<br>
and<br>
Rerr = &lt;|(fexp/fcalc - fcalc/fexp)|&gt;all pairs.<br>
Where fexp and fcalc are fold change values from experiment and calculation, respectively. Rerr1 and Rerr2 are defined symmetrically for relative errors of fexp and 1/fexp, reflecting the fact that base and experiment can be defined arbitrarily, and the final Rerr is defined as the average of the two. Each relative error number is an average over all the 366 data points.<br>
It seems the two algorithms have very close performance Table 1. They all have greater error margins for calculating down-regulated fold changes (captured by Rerr2) than up-regulated ones (captured by Rerr1). MAS4 is more asymmetric than MOID in this aspect.<br>
<br>
No-Change experiments<br>
Another set of experiments used for benchmarking algorithms is the comparison between two experiments where mRNA prepared from the same tissue sample was hybridized twice under slightly different conditions. Those fold change numbers that deviate largely from 1.0 are considered to be false positives. When we applied the algorithms to ten experiments done with either human brain or human lung replicate samples (same mRNA, slightly different hybridization conditions), the results were all similar. Figure 2 shows one of the typical comparison results. When only "Present" genes (defined by MAS4) were taken into account, 80% of fold change numbers were spread across 0.54 for MAS4 and only 0.30 for MOID. Clearly MOID assigns much less false positive fold changes than MAS4 does in this comparison. If all the genes were counted, the spreads of MAS4 and MOID results would be 0.89 and 0.29 for the same data sets. This test suggests MOID is more robust than MAS4.<br>
<br>
Reduced Probe Set Simulation<br>
In the studies above, we demonstrated the feasibility of a match-only gene chip design based on the MOID algorithm. In order to further increase chip information density, it is of common interest to understand how many probes are sufficient for expression analysis and push for the lower limit of the size of a probe set. This is done via computer simulations.<br>
In the simulation, for each probe set, a subset of nr probes were randomly chosen to be used in the calculation. Both the spiking and no-change calculations presented above were repeated with the selected subset, as we gradually reduced nr. Figure 3 and figure 4 show the results for the spiking calculation and no-change calculation, respectively. As the graph suggested, accuracy of MOID is essentially unaffected while reducing the number of probes down to ten. This result enables us to almost triple the amount of clusters one can put on a gene chip using MOID design. Combined with other new design ideas, MOID lays the foundation for the first universal human chip that contains 75,000 <database>UniGene</database> clusters (release 116). The results have recently been validated and led to many interesting discoveries, which provides indirect support for MOID (to be submitted).<br>
<br>
Noise Tolerance of both Algorithms<br>
Computer simulations also enable us to study the behaviors of both algorithms under noise disturbance. In simulations, a subset of nn probes were randomly chosen, their PM values were multiplied by ten to mimic the effect of serious noise effect. We repeated the calculations for both spiking experiments and no-change experiments, while gradually increasing nn. Figure 5 and figure 6 show the results for spiking calculation and no-change calculation, respectively. As the graph suggested, both MAS4 and MOID can resist noise perturbation up to 2 cells out of 16, the test favors MOID slightly.<br>
<br>
Probe-Specific Effect<br>
MOID is based on the assumption that the background for probes is mainly non-probe-specific. The results indicate that this assumption is sound overall. However, it is clear that by better understanding probe-specific behaviors, the analysis accuracy can be further improved. For instance, probe response factors might be derived by accumulating and studying many experiments. If there are a sufficient number of experiments, where the target gene is significantly present, probe response factors may be retrieved by some statistical modeling. A recent study [8] provides an important example in this direction. Researchers at Corimbia http://www.corimbia.com also developed some proprietary methods to identify "good" probes and assign different weights to them to improve data analysis. Efforts can be made along similar lines with the MOID algorithm in the future. If probe response factors can be calculated, the "bad" but "stable" probes can be scaled, and the distribution may be expected to be closer to normal, therefore expression levels and their uncertainties can be calculated in a more statistically sound manner.<br>
Future studies for more accurate background subtraction models may also improve the fold change distribution; therefore yield better statistics for fold change evaluation as well. The relative error in spiking experiments (28%) suggests that there is room for future improvement.<br>
<br>
LogP and Absolute Calls<br>
In the derivation of LogP (see materials and methods, Figure 7), it is assumed that the statistics of any absent probe can be described by the general background curve B. The P value should not be literally interpreted as accurate probabilities, since it is only as good as the underlining assumption. Users should also bear in mind that a P value is for the null hypothesis that the discrepancy between observables and generic mismatches is generated by noise, which is different from the likelihood of gene presence conditioned on the observed discrepancy. However, LogP values more or less serve as a statistical indicator for the sorting of genes by their significance. The discrete absolute calls in MAS4 are determined in a completely different empirically manner. Although higher LogP values have a higher tendency of being assigned "Absent" in MAS4, the exact correspondence between the two varies from experiment to experiment. Roughly, LogP values above -3.0 have a greater chance to be called "Absent" than "Present" by MAS4. Users should be aware not to over interpret this correspondence. One interesting observation we had in this study was that discrete calls by MAS4 are not as stable as LogP values. Sometimes a gene can be reassigned to "Present" from "Absent" by MAS4 due to a small perturbation in the underlying quantities without going through a "Marginal" transitional stage.<br>
<br>
Curve Normalization<br>
It is clear by observing various experiments that the response of intensity to signal varies in different intensity regions. Occasionally, the normalization constant nf does not cover well for all intensity values. A more generalized normalization procedure should use a normalization curve instead of a constant factor. The MOID normalization algorithm can be easily modified to the following (see [10] for a similar approach):<br>
Step 1: include all genes in the normalization gene list.<br>
Step 2: using the Ek values for all the genes in the list, generate integral intensity distributions for both experiments. A normalization curve NF(I) is constructed in such a way that the two integral distributions are identical after normalization.<br>
Step 3: normalize the data sets using NF obtained in the previous step; refine the gene list by further excluding those genes whose intensity values changed by more than a certain fold, &gt;fmax or &lt; 1/fmax, between the two data sets.<br>
Step 4: repeat Step 2 and update NF to NF', until one of the following conditions is met:<br>
1) the maximum number of iterations, Itrmax, is reached;<br>
2) max(|NF(I)/NF'(I) - 1|) ? max, where max is a small predefined threshold;<br>
3) size of the normalization gene list drops below a predefined threshold, Szmin.<br>
This algorithm offers a chance to correct non-linearity in the chip system to a certain extent. To demonstrate the algorithm, we applied the procedure to two measurements, where genomics DNA sample of yeast s288c strain were hybridized onto Affymetrix YG_S98 arrays. The second experiment was a repeat of the first one after about 50 days, where some of hybridization and scanning parameters had been changed over the course. It is shown in Figure 8 that the curve normalization procedure out-performed constant normalization as expected.<br>
<br>
<br>
Conclusions<br>
MOID algorithm allows at least double or even triple the information density of current (U95 human chip) Affymetrix high-density oligo nucleotide arrays without compromising analysis accuracy. Table 2 summarizes feature comparisons between MOID and MAS4.<br>
It should be noted that at the time of this study, Affymetrix U95 chip uses 16 probe pairs per set. As Affymetirx is planning on further reducing probe set size in their next design, the density improvement by using MOID may be less than the estimation given above.<br>
<br>
Materials and Methods<br>
The user should refer to the Affymetrix web site http://www.affymetrix.com and the documentations that come with the MAS4 software for technical details regarding Affymetrix array and MAS4 algorithm.<br>
Affymetrix Oligonucleotide Chip<br>
At the time of this study, Affymetrix synthesizes 25-mer oligonucleotides on a 640 ? 640 array with 20 ?m feature size using photolithographic fabrication techniques (some data used in this study were collected from some previous generation arrays of 540 ? 540 cells and 24 ?m feature size). Each cell (also called a probe) in the array contains the same oligonucleotide sequences. As shown in Figure 9, typically a set of 16?20 probes are designed for each targeting cluster (called a probe set, or sometimes a "gene".). An expression value will be derived per probe set as the result of analysis. Probes taken as fragments from a target sequence are called perfect matches (PM). Multiple matches per set serve as independent signal detectors and provide a possibility to capture statistical uncertainties. For each match, there is also a corresponding mismatch (MM) probe, whose sequence differs from its match by a single base in the middle. Mismatches are meant to detect cross hybridization components of their corresponding matches, and are used by MAS4 for probe-specific background subtraction. There are also several Affymetrix control sets on each chip used for quality references, which were used in the spiking experiments to validate and refine hybridization protocols.<br>
<br>
Intensity Distribution<br>
In an ideal case, all the probes in a set should give similar signals and serve as replicate measurements. One common rule in probe design is to select probes with similar melting temperatures to minimize the variances among probes. The chemistry involved in a hybridization experiment is often too complicated to be predicted computationally at the current stage, therefore in reality probe intensity distribution for a set is usually fairly wide and has a long tail, which causes all kinds of difficulty for data analysis.<br>
One usual attraction in expression analysis is to assume a normal distribution for probe intensities. This hypothesis can be examined by the following calculation. For each probe set, we applied a linear transformation to probe intensities, so that the mean and the standard deviation of the intensities in the set were normalized to zero and one, respectively. Then all the resulting match intensities were used to generate an overall distribution. Our calculation shows the normal distribution assumption is not an appropriate one. When similar analysis was applied to mismatch intensities and mismatch-subtracted match intensities, the conclusion stays more or less the same.<br>
Despite the fact that the intensity distribution of a set is non-Gaussian, it is still crucial to find out the distribution function for match or mismatch intensities to generate meaningful statistics tests, if such distributions actually exist. The authors tried several analytical functions on match and mismatch probe intensities. Unfortunately, the distributions seemed to be quite "bad", the tail portions even fade out slower than the extreme value distribution. It was found that distributions of mismatch signals also significantly depended on the sample and experimental conditions. One may reasonably suspect that cross hybridization, intensity saturation, overall sample concentration, chip production irregularities, and miscellaneous noise may all cause a change in the signal distribution. The study seems to suggest the distribution of cross hybridization should be taken from each experiment as a result of measurement instead of using an a priori determined analytical expression. This guideline is used in MOID algorithm.<br>
<br>
MOID algorithm: hypothesis and principles<br>
As mentioned in the intensity distribution analysis, MOID assumes cross hybridization behavior of probes is mainly non-probe-specific, and therefore all the match cells can share the same cross hybridization background. MOID primarily aims at saving 50% of the chip space currently dedicated to mismatch cells in the Affymetrix design under this study. It was also discussed before that cross hybridization distributions should be taken from experimental data instead of from some analytical function. Based upon the general belief that there is always a significant amount of genes unexpressed in any mRNA experiment, we will use the match cell signals from the 5% darkest probe sets in the current public Affymetrix arrays to prove the point that it is possible for MOID not to use any mismatch signal. For the next generation gene chip designed based on MOID algorithm, some designated probes may be used as generic mismatches to collect non-probe-specific cross hybridization data. In fact, on GNF-HS1, 16,460 probes from 476 viral genes are used for such a purpose.<br>
As discussed, the distributions of quantities in expression analysis are usually asymmetric and contain a long tail, regardless whether it is match intensity, mismatch intensity, or average intensity difference. For such abnormal distributions, concepts like mathematical average and standard deviation are not the most appropriate statistical terms to use. Pre-filtering the data, like what MAS4 does, certainly helps reshape the distribution, but by no means this can bring the distribution back to normal without throwing out a significant portion of measurements. The MOID algorithm is specifically designed to avoid these problems by using percentile instead of mean and using confidence intervals instead of standard deviation in all possible cases.<br>
<br>
MOID algorithm for significance of gene presence<br>
A non-probe-specific integral background distribution (representing noise, cross hybridization, and possible other factors), B(I), is derived from the intensities of the 5% darkest probe sets (or from generic mismatches in the future). I stands for intensity; B satisfies boundary conditions: B(0) = 0 and B(?) = 1. For each probe set k, the match signals are sorted and the integral distribution, Sk(I), is generated as well. Figure 7 is a schematic diagram.<br>
If the gene represented by a probe set is absent, the intensity data from matches are due to pure background contribution, therefore Sk is likely to be close to B. We choose the Kolmogorov-Smirnov test to determine likelihood that the observed signal distribution, Sk, could be explained by B. If we define dk max as the maximum vertical distance between B and Sk, according to K-S statistics, the probability of observing discrepancies greater than dk max is determined by a P value [11],<br>
where<br>
nk is the effective number of data points in the K-S statistics, which is the number of PM for gene k in our case. P carries the meaning of probability, therefore is a number between 0 and 1. Practically, Log10P, a negative value, is used as our final representation. In this way, MOID uses a continuous LogP criterion to replace MAS4 absolute calls. Those signal sets that can be easily explained by noise are assigned a LogP value closer to zero.<br>
<br>
MOID algorithm for expression level calculation<br>
MOID uses the horizontal distance between Sk and B to represent gene expression level (Figure 7). Since the darkest probes may be more likely caused by their poor binding properties to the target gene, and the brightest probes may be more likely caused by serious cross hybridization issues, different parts of the integral distribution tend to have different qualities. Therefore, the MOID algorithm uses the horizontal distance measured at a certain percentile, pct, instead of the whole curve. Based on the analysis of the spiking experiments discussed later, it is empirically determined that 70% is the optimal percentile for signal retrieval. We also tried to use the average of horizontal distances from several pct, but without significant improvement. Therefore the expression level Ek, related to gene concentration for gene k, is defined as<br>
Ek = max(I|Sk = pct - I|B = pct, 0).<br>
Instead of using the standard deviation, we take confidence intervals directly from the distribution curve Sk, with the background subtracted. E.g., 80% confidence intervals can be represented by a lower bound (Ekl) at 10 percentile and an upper bound (Eku) at 90 percentile of the distribution, i.e.,<br>
Ekl0 (0.1) = max (I|Sk = 0.1 - I|B = pct, 0)<br>
and<br>
Eku0 (0.9) = max (I|Sk = 0.9 - I|B = pct, 0).<br>
As one might expect, increment of the probe set size helps narrowing down the confidence intervals in a manner determined by the statistics of probe distribution. However, this piece of information is unknown and we took an ad hoc approach by modifying the boundaries formulae as the following:<br>
and<br>
<br>
MOID algorithm for normalization<br>
The most common way of understanding gene functions by expression profiling is to study the change of their expression levels in various disease states and cellular environments. The observed expression level is a product of complicated protocols, and is subjected to various factors from sample preparation to final image scanning. Therefore, it is a common practice to normalize expression data drawn from multiple profiles before making any reliable interpretation.<br>
The normalization procedure in MAS4 aims at normalizing the average intensities of probe sets (excluding the top and bottom 2%). To avoid possible signal contamination, MOID takes similar approach by normalizing the probe sets between 10 and 90 percentiles. However, it is noticed that an ideal normalization factor should be derived from only those genes that do not change their expression levels between the two experiments. <software>MAS</software> software provides the possibility to use a list of housekeeping genes for such purposes; however, it certainly requires careful downstream research to validate any such a list. Some common normalization criteria were summarized in a recent study by Zien et al. [12]. MOID uses a heuristic bootstrap method to identify an approximate list of unchanged genes between two data sets:<br>
Step 1: include all genes as the normalization gene list.<br>
Step 2: sort the Ek values (already background subtracted) for all the genes in the list; the interesting portion of expression values (between 10% and 90% in our case) is retrieved and average intensity is calculated for each experiment, respectively. The initial normalization factor nf is calculated by the ratio of the two average intensities.<br>
Step 3: normalize the data sets using nf obtained in the previous step; refine the gene list by further excluding those genes, whose intensity values changed by more than a certain fold, &gt;fmax or &lt; 1/fmax, between the two data sets.<br>
Step 4: repeat Step 2 and update nf to nf', until one of the following conditions is met:<br>
1) the maximum number of iterations, Itrmax, is reached;<br>
2) |nf/nf' - 1| ? max, where max is a small predefined threshold;<br>
3) size of the normalization gene list drops below a predefined threshold, Szmin.<br>
In practice, a single iteration is usually sufficient for most comparisons. User can adjust threshold parameters towards their preferred stringent levels. In this study we use fmax = 2.0, max = 0.05, Itrmax = 5, Szmin = 1000.<br>
<br>
MOID algorithm for comparison analysis<br>
Different probes within the same probe set generally respond very differently to mRNA sample fragments. We examined various probe properties such as melting temperature, nucleotide base composition, possible sequence motifs, and potential secondary structures in order to understand the cause of such diverse response properties. That study did not find any conclusive factor that explains observed probe signal distribution satisfactory. Although it seems rather difficult to pre-compute and predict probe responses, it is possible to derive some features afterwards for a probe based on a significant amount of expression data where the target gene of interest is present [8]. Instead of using a modeling approach to explain various signal contributions, MOID uses a new approach to avoid the affect of diversities of probe response factors in calculating expression fold changes.<br>
Let us assume any two probes in a probe set k of size nk always respond to their target gene concentration with factor r1 and r2, respectively. That is, if the gene is present at concentration c1 and c2 in two hybridizations, the two probes in average give signal intensities r1c1 and r2c1 in the first experiment, r1c2 and r2c2 in the second experiment. As in MAS4, fold change is calculated by the ratio between r1c2 + r2c2 and r1c1 + r2c1, where the result is essentially dominated by the probes with the largest response factors, and the statistics of the ratio becomes difficult to estimate. However, we observe that by taking the ratio of the two signals for each probe individually, one essentially is looking at nk independent measurements of c2/c1 for that probe set. This opens a possibility for obtaining a distribution of fold change values.<br>
If one assumes most probes have a rather stable intrinsic response factor, r, a fold change number can be calculated from each probe i in the set independently, which hopefully removes the affect of the unknown response factors ri. However, it seems the response factor of each probe has a non-negligible intrinsic spread as well; this may be further complicated by alternative splicing and tissue-specific cross hybridization issues. In addition, the nk ratio numbers calculated for a probe set are not normally distributed (it is well known that even the ratio of two normally distributed values is no longer normally distributed). The current background subtraction algorithm may not take fully into account response factor-unrelated signals, therefore could further increase the non-normal component. To overcome this difficulty, MOID uses a percentile, pctf, of the integral distribution formed by the nk fold change numbers, Fk(f), which satisfied the boundary conditions: F(0) = 0 and F(?) = 1. Empirically, 70% is used for pctf as determined by spiking experiments discussed later. The confidence intervals are also directly taken from relevant percentiles in the distribution corresponding to the lower bound and upper bound, respectively. The effect of the number nk is taken into account in the same heuristic manner as we did previously for absolute analysis. The final formulae are:<br>
<br>
MAS4 Algorithm for Absolute Analysis<br>
After a sample is hybridized to probes on a chip, the chip is scanned and fluorescent signals are collected and stored in an Affymetrix DAT file. For cell i, after filtering out boundary pixels, Ni numbers of pixels are used to calculate an average intensity value Ii and standard deviation ?i. Hereafter we index a probe set by k, which contains nk probe pairs. The calculated intensity values for thejth pair in the kth probe set are denoted as PMkj for PM and MMkj for MM. Background intensity, bg, is derived from the average intensity of the 2% darkest cells. Noise level, Q, is determined as<br>
where average is done over all background cells.<br>
MAS4 assumes that the cross hybridization component of a match is captured by its mismatch companion, therefore the difference, PMk - MMk, is used to derive expression levels. MAS4 also uses a "super-Olympic scoring" procedure to iteratively filter out outliers, defined as those probe pairs with difference values more than three times the standard deviation away from the mean. After filtering, the average difference value for a probe set, Dk, is used to represent the expression level of that particular gene.<br>
The significance of gene presence is determined by an empirical absolute call decision matrix coupled with proprietary MAS4 algorithms, parts of which are described in the <software>Microarray Suite</software> documentation. As the result, a probe set is marked as either "Present", "Absent", or "Marginal".<br>
MAS4 calculates average difference by<br>
Dk = &lt;PMkj - MMkj &gt;j,<br>
which is essentially<br>
Dk = &lt;PMkj &gt;j - &lt;MMkj &gt;j,<br>
where the average is taken over all the probe pairs surviving the super-Olympic filtering process mentioned above. Based on this observation, the one-one correspondences between PMkj and MMkj, typically with an average correlation coefficient around 0.8, are essentially lost during such averaging. Since MAS4 is successful in various applications despite the fact it averages out probe-specific mismatch signals, we hypothesize that contributions of the mismatch probes are mainly probe-nonspecific, if the pairwise correspondence between PMkj and MMkj are not enforced. This observation led to the idea of only using match probes in the MOID algorithm discussed above.<br>
<br>
MAS4 Algorithm for Normalization<br>
MAS4 introduces a normalization factor (a scaling factor in MAS4 serves the same purpose). The goal of normalization is to reduce false positives in the expression fold change calculation and ensure housekeeping genes are marked as unchanged. Since it is usually unknown a priori what genes sustain their expression level during the two experiments, different heuristic normalization schemes may be adopted and it is unclear at this point which particular approach is optimal. MAS4 normally derives the normalization factor by scaling average Dk for all genes with expression levels within the central 96% to a certain target intensity constant. This algorithm is based on the assumption that the total copy of mRNA per cell is a conserved number among experiments.<br>
<br>
MAS4 Algorithm for Comparison Analysis<br>
In comparison of two chips, we follow Affymetrix convention of calling the reference chip measurement as the base (b), the other one as the experiment (e). After the above normalization procedure, the fold change value for gene k, fk, is essentially calculated by dividing Dk (e) with Dk (b). MAS4 uses a filtering process to ensure that a common subset of "good" probe pairs between b and e are used to recalculate Dk. Very weak expression signals (sometimes negative) are rounded to the noise level mentioned before. The final formula for f is:<br>
fk = [Dk(e) + max (Q - Dk(b), 0)] / max (Dk(b), Q), if Dk(e) ? Dk(b),<br>
or<br>
fk = max (Dk(e), Q) / [Dk(b) + max (Q - Dk(e), 0)], otherwise.<br>
An empirical difference call decision matrix and a proprietary MAS4 algorithm, partly described in the <software>Microarray Suite</software> 4.0 documentation, determine the significance of the fold change. As a result, the probe set is marked as "Increase", "No Change", "Decrease", "Marginally Increase", or "Marginally Decrease". It is obvious that the fold change number is only meaningful if the probe set is clearly present at a level above the noise in both data sets.<br>
<br>
<br>
List of Abbreviations<br>
MOID - match-only integral distribution<br>
MAS4 - Affymetrix algorithms implemented in its <software>Microarray Suit</software> 4.0 software<br>
PM - perfect match<br>
DMM - mismatch<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC481056</b><br>
<software>Base-By-Base</software>: Single nucleotide-level analysis of whole viral genome alignments<br>
<br>
<br>
Background<br>
The recent advances in large-scale DNA sequencing technologies has significantly reduced the cost of this work, and it has become feasible to determine the sequence of multiple isolates of a number of viruses and examine smaller and smaller differences between them. The availability of this type of data permits novel hypotheses to be tested, but it requires new bioinformatics tools. For a number of years, our laboratory has been designing software specifically to manage and analyze large numbers of the bigger virus genomes such as those of poxviruses. Poxvirus genomes range in size from 150?380 kb and encode several hundred proteins [1]; there are now more than 30 completely sequenced poxvirus genomes available from <database>GenBank</database>. Coronaviruses provide another important example, with the largest genome (~30 kb) of all RNA viruses and 99 isolates of the SARS virus completely sequenced. This wealth of sequence data provides excellent opportunities to study new aspects of virus virulence and evolution. The <database>Poxvirus Orthologous Clusters</database> (<database>POCs</database>; [2]) <software>mySQL</software> database and query tools, now expanded to <database>Virus Orthologous Clusters</database> (<database>VOCs</database>), was constructed to organize all the genomes of one virus family into a single database and to group orthologous genes into families. A variety of tools are integrated into the database for rapid comparison of not only individual genes and proteins but also entire gene families if required. These tools include <software>BLAST</software> [3,4], <software>CLUSTALW</software> [5,6], <software>T-coffee</software> [7], hydrophobicity plots (using Kyte-Doolittle, Hopp-Woods and Parker-Guo-Hodges scales of hydrophobicity), DNA/protein dotplots and <software>NAP</software> [8] to align protein to DNA sequences. Gene maps of complete virus genomes are also generated automatically in <database>VOCs</database>. Continuing on the theme of working with gene families and complete genomes, the <software>Viral Genome Organizer</software> (<software>VGO</software>; [9]) was designed to display multiple whole genomes in a single working window with zooming capabilities and showing a variety of information such as ORFs, start/stop codons and AT%. <software>VGO</software> acts as a graphical portal to a variety of data that is stored in <database>VOCs</database> (amino acid composition, nucleotide composition, presence of orthologs in other genomes and pre-processed <software>BLAST</software> searches) or generated on-the-fly; for example clicking on gene X in one genome will cause the orthologous gene to be highlighted in a related genome. This software has the ability to easily and quickly answer a database query such as display genes present in variola virus that are absent from vaccinia virus. However, no software existed that could display all nucleotide differences between variola virus strain 1 and variola virus strain 2 and provide a summary of implications of these base changes; therefore the development of <software>Base-By-Base</software> (<software>BBB</software>) was initiated. This level of detail is required by current sequencing projects that aim to identify the genetic determinants responsible for phenotypes such as mortality rate/virulence, host tropism or tissue tropism displayed by different isolates of a single virus species. <software>BBB</software> allows the user to quickly display all nucleotide differences between genomes, and provides a detailed summary of the effect that each change produces in the relevant genes.<br>
A second problem associated with the alignment of closely related, large virus genomes, is that the available software such as <software>Dialign2</software> [10], used to produce acceptably fast, global alignments of complete genomes, invariably makes occasional small errors, often around the positioning of multiple small gaps. These small alignment errors pose a serious problem with fine analysis of genomes because the insertion of erroneous gaps results in an increase in the number of apparent, but not real, nucleotide substitutions. <software>BBB</software> provides a visual display to help the molecular biologist recognize these errors and quickly correct them using the built in sequence editor. Accurate large-scale alignments are essential for determining the evolutionary relationship of closely related viruses (such as isolates of a single strain), and in such alignments insertions and deletions themselves are very useful indicators of evolutionary lineage. Thus <software>BBB</software> will be very useful for phenotype/genotype analysis and for epidemiology.<br>
<br>
Implementation<br>
Design rationale<br>
<software>BBB</software> was coded in Java to simplify support for multiple platforms including Mac OS X, MS Windows and LINUX. A user initially accesses the application (client) from a web page using <software>Java Web Start</software>, which also automatically downloads the application from the host server computer whenever a new version is available. This approach greatly simplifies the distribution of updates and ensures users are taking advantage of the latest version of the software; it has worked very well for the distribution of <database>VOCs</database> and <software>VGO</software>.<br>
Although <software>BBB</software> was primarily designed to be used as an editor for large DNA sequences, the software also works with protein sequences and is used as the multiple alignment interface in <database>VOCs</database>. Both <fileFormat>FASTA</fileFormat> and <fileFormat>ClustalW</fileFormat> (.aln) formatted text files can be loaded into the program; multiple sequences may be placed in a single file and additional sequences can also be added at any time. Similarly, alignments may be exported in either of these formats. The native file format of <software>BBB</software> is, however, based on the Bioinformatics Sequence Markup Language (BSML) [11] standard that is itself an Extensible Markup Language (XML) dedicated to the needs of bioinformatics; it aims to provide an open language definition for distributing sequence data. The <fileFormat>BBB</fileFormat> file format stores the sequence alignment, gene features and other user-defined annotations for the sequences; because of the highly modular nature of <fileFormat>XML</fileFormat>, it is easy to incorporate new information in the alignment while maintaining compatibility with previous versions. <fileFormat>XML</fileFormat> is also highly interoperable since it has as its root a plain text file, allowing <fileFormat>BBB</fileFormat> and <fileFormat>BSML</fileFormat> files to be easily integrated with other software packages.<br>
Since <software>BBB</software> provides access to <software>ClustalW</software> [5,6] and <software>T-coffee</software> [7] software, specific regions or entire genome sequences can be selected and aligned within the program itself, but in normal operation it is expected that the user would import large sequences that had already been aligned by a more appropriate program such as <software>DIALIGN2</software> [10]. <software>ClustalW</software> and <software>T-coffee</software> are not distributed in the <software>BBB</software> jar file and are run instead on the remote <software>BBB</software> server. The user selects regions of the sequences to be re-aligned, and these are formatted by <software>BBB</software>, submitted to the server and subsequently received back in a new <software>BBB</software> window.<br>
The annotations for viral genomes in a <software>BBB</software> alignment are read into <software>BBB</software> from a <database>GenBank</database> file or from a <database>VOCs</database> database. Currently, poxviruses [12] and coronaviruses [13] are available in our <database>VOCs</database> databases although herpesviruses, baculoviruses and adenoviruses will be available in the near future [14]. The gene annotations are required by <software>BBB</software> to generate the tabular summary of the differences between genomes that details the effect of nucleotide changes on the genes and predicted proteins. These annotations are not required to produce a visual summary of sequence alignments, although we find it useful to have the genes displayed for one of the genomes in a large alignment.<br>
<br>
<br>
Results and Discussion<br>
Editing and display of sequence differences in real-time<br>
<software>BBB</software> is primarily intended to be a sequence analysis tool focused on genome sequence comparisons, but the program also provides similar functionality to other alignment viewers and editors such as <software>GeneDoc</software> [15] and <software>BioEdit</software> [16]. <software>BBB</software> gives users the ability to load, correct and save alignments. Alignments loaded into <software>BBB</software> are easily edited by dragging sequences across the screen to insert and remove gaps; multiple sequences may be selected and edited as one. One of many unique features of <software>BBB</software> is that the main window displays the DNA or protein sequences together with flags that show all the nucleotide or amino acid differences that are present between the sequences shown in the window. This information provides very important visual cues as to the position of major and minor differences between the sequences in an alignment and is updated in real-time as the user makes edits to the alignments (Figure 1). <software>BBB</software> provides a button to allow the user to skip through a sequence from one flagged nucleotide difference to the next, alleviating the need to scroll through an entire genome and detect the differences by eye. The sequence substitutions, insertions and deletions are flagged and color-coded in a single row between the pairs of sequences (Figure 1). The program preferences can be set so that the difference data represents differences between each adjacent pair of sequences or to a consensus sequence calculated by <software>BBB</software>. In pairwise comparison mode, the first sequence is compared to the second, second to the third and so on, making <software>BBB</software> especially useful for visualizing evolution of sequences. When comparing to the consensus, the sequence differences indicated on the screen are those differences between each sequence and the consensus sequence, this allows for a global view of the alignment and easily identifies conserved/variable regions in genomes which is useful information in drug design and vaccine development. The consensus sequence generated from an alignment can also be saved to a file (<fileFormat>FASTA</fileFormat> format) for analysis with other software.<br>
During development, it was discovered that it was important to have the flags that highlight nucleotide differences updated in real-time as the user manually modifies the alignment. These flags are created for the displayed area only, and are then updated and augmented when needed, such as when the user changes the alignment, by inserting or deleting gaps, or changes the view, by scrolling or setting the display area. This enables <software>BBB</software> to work with multiple large genome sequences and keep an acceptable refresh rate for displayed data. A Block Glue option facilitates the movement of long sequence blocks that are bigger than the editing window by permitting the dragging of complete blocks without introduction of new gaps within the alignment. Often alignment errors are obvious to an experienced molecular biologist and the alignment can be manually corrected, simply by dragging the appropriate nucleotides into new positions. However, if there are doubts about the <software>DIALIGN2</software> alignment or if there are a large number of sequences in the alignment, the user may choose to perform a local re-alignment of selected regions of the sequences using the <software>ClustalW</software> or <software>T-coffee</software> module of <software>BBB</software>. After the server completes an alignment, a new <software>BBB</software> window displays the new local alignment and the user is offered the option to import this new local alignment back into the original complete genome alignment. If required, forward and backward 3-frame translations of the DNA sequences can be displayed in the main sequence alignment window to help with alignment decisions. Methionines (green) and stop codons (red) are highlighted on the 3-frame translations, as are the genes that are associated with individual sequences through annotations in the <software>BBB</software> file. Figure 1 shows an example of a correction of a small mismatched region of two poxvirus genomes; breaking the single gap and moving the two resulting gaps reduces the number of mismatched nucleotides (not including those opposite a gap) from 10 to 3.<br>
<br>
Sequence filtering and display customization<br>
Since <software>Base-By-Base</software> has been designed for users to manipulate alignments of complete poxvirus genomes (150?300 kb), features to simplify and enhance the user-interface and to speed up the program's manipulation of the sequences have been incorporated where possible. However, some speed has been sacrificed by using Java, which was chosen to provide cross-platform functionality. <software>BBB</software> allows users to filter their view of the data in two ways, 1) complete genome sequences can be sorted and reordered in the main window or placed in the background, completely hidden from view to make analysis of dozens of sequences possible; 2) long genome alignments can be masked from the 5' and/or 3' ends to allow a user to focus on any particular region within the genome; this is especially useful for visually tracking the differences between individual orthologous genes in several different genomes. These user selected viewing options also apply to the functions for generating visual or tabular reports; only the genomes shown in the main window and the unmasked regions are evaluated by these report routines. From the preferences window, users can also toggle on/off the display of the sequence difference flags, sequence-numbering scales, user annotations, and there is a button in the main window to toggle on/off the display of 3-frame translation of DNA sequences (Figure 1). These features are especially useful for alignments of 30 sequences or more, where screen real estate precludes the display of all sequences or all features associated with the sequences.<br>
Since individual researchers frequently are interested in different genome features that may not be annotated in <database>GenBank</database> files, a tool was created in <software>BBB</software> that allows the user to add comments to different regions of one or more sequences in an alignment. These comments can be color coded, labeled with text and hidden or viewed in the main window as required; they are saved within the native .bbb alignment file. Again, since the aligned sequences are frequently very long, <software>BBB</software> provides a button to skip through the sequences from one comment to the next.<br>
To provide the user with additional information about the sequence alignments, <software>BBB</software> provides several different coloring styles for viewing the alignments in the main window. These include the default character-identity based scheme in which each nucleotide or amino acid is colored based on which nucleotide or amino acid it represents and a simple percent identity style which uses shades to indicate the frequency of each nucleotide or amino acid at each position in the alignment. Protein sequence alignments may also be viewed with similarity-matrix based (BLOSUM62 or PAM250) shading for which residues "similar" to the most frequently occurring amino acid are also colored. Lastly, a hydrophobicity coloring scheme shades amino acids based on the hydrophobicity score of each residue.<br>
<br>
Reporting sequence differences and effects of nucleotide changes<br>
Another of the unique and primary features of <software>BBB</software> is its ability to summarize the differences between two large closely related virus genomes; without such bioinformatics tools, this type of large-scale analysis is not feasible. Using the genome annotation information imported from a <database>GenBank</database> file, or from our own <database>VOCs</database> database, <software>BBB</software> is able to determine which pairs of genes have nucleotide differences and what, if any, effect these nucleotide substitutions have on the predicted proteins from each gene. Basic gene information is listed in a table, including start/stop positions, strand and length, followed by: 1) the number of nucleotide differences counted and categorized as substitutions, deletions and insertions; 2) calculation of the percent difference; 3) changes in promoter regions; 4) nucleotide substitutions categorized as silent, or listed with amino acid change (Figure 2). In addition, <software>BBB</software> displays the size of deletions and insertions to guide the user in comparison of percent differences since a single deletion event removing 12 nucleotides creates larger effect in the percent difference table than 6 single substitution events that could affect 6 different amino acids. The summary table, which can be very large for the poxviruses, can be sorted using the data in any column by simply clicking on any column header.<br>
The percent difference between genes is useful for spotting regions that may be mis-aligned by highlighting unusually large differences between particular genes. A region of 200 nucleotides upstream of each gene is also analyzed for differences; in poxviruses, promoters are small and almost all are within 200 nucleotides of the initiating ATG [17,18]. Thus, with very similar genomes it is very simple to determine which few genes have differences in the promoter regions and select these for further analysis. This type of analysis of promoter regions is very important when looking at different isolates of a single virus strain since modulation of gene expression may occur rapidly. Changes of one or two nucleotides in the small promoters of poxviruses can have a drastic effect on transcription rate and ultimately on protein expression [17,18]. This type of adaptation of a virus to a particular environment/host occurs at a much higher frequency than the acquisition of novel genes/promoters derived from the host or other viruses. These differences can easily be identified and viewed in <software>BBB</software>.<br>
Comparison of very closely related genomes is the primary purpose for which <software>BBB</software> was designed and there is usually a simple one-to-one correspondence of complete genes between such genomes. However, <software>BBB</software> also handles situations where fragmented genes may exist in one genome; it chooses counterpart genes in the other genome by determining the greatest portion of overlap between the gene on the first genome and the gene on the second genome. The tabular report includes raw information on the genes, such as the position and length of the genes (Figure 2).<br>
The Visual Overview Summary creates a pictorial representation of the differences (default) or similarities between each sequence in the main alignment window and the consensus sequence. This summary window is not a static picture, but rather it communicates with the alignment window (Figure 3). Examples of this communication activity are: 1) information showing the length of the sequence displayed in the alignment window is superimposed on the summary picture, and clicking in another area of the summary window causes that new region to be displayed in the main window; this feature also provides a very rapid way for navigating through large genomes; 2) when edits are made to the underlying sequences in the main sequence window, the consensus is automatically recalculated and the summary display is updated. Changes to the main window are polled approximately once per second, so that the system is not inundated with consensus refresh requests that can, in some cases, be quite time consuming. The default color scheme used in this window is identical to that of the main alignment display and nucleotide differences (or regions of mostly substitutions) are displayed as blue ticks or bars, and insertions and deletions are displayed in green and red, respectively. Similarity shading styles, scored with BLOSUM62 and PAM250 matrices, are available for analyzing protein sequences that might not be similar enough for the default parameters (Figure 3) and has proven to be very useful for the analysis of gene orthologs and protein families.<br>
As discussed above, <software>BBB</software> stores the sequence alignment, gene features and user annotation in its own XML format. It can, however, export the alignment text in <fileFormat>FASTA</fileFormat> and <fileFormat>ClustalW</fileFormat> (.aln) formats for convenient transfer to other programs. To capture a graphical view of the main window alignment, the user may choose to export either a full alignment or a particular sequence range to an image file in JPEG or PNG file formats. This permits the user to view and print a full alignment by wrapping the single row, at a user-specified width, which is normally displayed on the screen. All graphics, showing sequence differences, nucleotide translations and user-added annotations are preserved in the picture. For publication purposes, however, features such as user comments can be easily hidden from view by changing <software>BBB</software> preferences (Edit menu).<br>
Other methods of summarizing the information from a multiple sequence alignment in <software>BBB</software> are by using the Phylogeny tools or the Alignment Info tool. Trees are calculated from the alignment and drawn by routines from the <software>Phylogenetic Analysis Library</software> (<software>PAL</software>) [19]. The Alignment Info tool generates a tabular report of the percent identity between all the pairs of sequences in the alignment; this data can be exported as a tab-delimited file for convenient importing into a spreadsheet or table in a word processor. Since the percent identity is only calculated on the region of the alignment set in Display area, this provides a useful tool to calculate conservation in different regions of an alignment.<br>
<br>
Regular expression and fuzzy motif searches<br>
It is frequently necessary to search through viral genome sequences for restriction sites, primer sequences and other patterns. For most restriction sites a regular expression search suffices, however, to look for less well-defined motifs such as promoters or other regulatory sequences, allowance for a significant element of degeneracy is desirable. This is a tedious process using regular expressions; therefore, a fuzzy motif search capability was also implemented in <software>BBB</software> that allows a user-selected number of mismatches and ignores gaps in sequence alignments. The search results are returned in two formats (Figure 4); they are presented in table format in a new window together with information indicating the percent match to the query motif and are also indicated directly on the main sequence alignment window. To facilitate searches of long sequences or those that return a large number of matches, the search results table can be sorted by location of the match within the genome or by percent match (Figure 4). Users can also rapidly move from hit to hit along each of the sequences searched by simply clicking on the icon representing the search hit in the search results table. Fuzzy search expressions follow a simple grammar that is explained in the user documentation , and the regular expression searching capability is provided by the <software>Jakarta-ORO</software> text-processing package. <software>BBB</software> uses the Perl 5 regular expressions from the <package>ORO</package> package with case insensitive searching; this function searches both strands of the nucleotide sequence.<br>
<br>
Data connectivity<br>
<software>BBB</software> has been designed to accommodate large virus genomes and therefore works with sequences in the order of 300 kb. Initially, alignments are read into <software>BBB</software> from <fileFormat>FASTA</fileFormat> or <fileFormat>ClustalW</fileFormat> format alignment files which are then converted to the native <fileFormat>BBB</fileFormat> file format. By their <fileFormat>XML</fileFormat> nature, <fileFormat>BBB</fileFormat> files are simple text files that research labs can easily post on their websites, which are then accessible from inside the program. Users of the program have found it convenient to maintain a series of <fileFormat>BBB</fileFormat> alignment files, each containing a multiple alignment of one group of closely related virus genomes, which serve as founder alignments for users; for example, a large alignment of all SARS genomes is maintained at . These files may then be edited by users who can delete the genome sequences they are not interested in and save the new files to their local computer. If, by deleting a series of genomes from the multiple alignment, there are then some positions that have gaps in all remaining genomes, <software>BBB</software> asks the user if these empty columns should be removed from the alignment.<br>
Once a genome sequence alignment has been corrected, one of the most frequent requests is to view the visual summary of the alignment. Therefore an interface to generate and view these <fileFormat>BBB</fileFormat> summary files directly from a WWW page has been developed. Figure 5 shows a visual summary of 11 SARS genomes with an ORF map and substitutions, insertions and deletions color-coded. The WWW package allows the user to select an alignment file and view either the entire sequence alignment or a selected region of the genome sequences. If the <fileFormat>BBB</fileFormat> alignment file contains gene feature information, the position of genes will be superimposed on the alignment summary. Since the views are produced on-the-fly, the site maintainer only has to save the <fileFormat>BBB</fileFormat> file to the appropriate WWW directory and the users can be sure that they have access to the latest information. This feature also provides access to the summary display without the need to download <software>BBB</software> or learn how to run the software.<br>
<br>
<br>
Conclusions<br>
The goal of this project was to produce a tool to facilitate the comparison of closely related large viral genomes such as isolates of a single virus strain. To this end, a new software package called <software>Base-By-Base</software> has been developed; it uses a graphical interface to highlight differences between genomes and includes a multiple alignment editor so that the user can manually correct the errors made by programs making global alignments of complete genomes. When combined with our gene feature database, the <software>Viral Orthologous Clusters</software> system, or a <database>GenBank</database> file <software>BBB</software> is able to map gene features onto whole genome alignments, thereby giving users the ability to manipulate their alignment within the context of the annotated genes. Graphical summaries of multiple genome alignments are available from <software>BBB</software> in several formats. Furthermore, by using genome annotations, <software>BBB</software> is able to create tables that summarize all of the nucleotide differences between genomes and the implication of these changes on proteins encoded by the viral genes. Both coding and intergenic (e.g. promoter) sequences are analyzed. We believe that <software>BBB</software> will significantly enhance the analysis of a growing set of sequence data, namely the accumulation of multiple closely related virus genomes. Correlations between sequence and phenotype can be analyzed and hypotheses developed for testing. Conserved and variable regions can be viewed for phylogenetic relationships or vaccine or drug development. <software>BBB</software> is written in Java and has been tested on Linux, Mac OS X, and Windows. It is freely available for use under the terms of the GNU General Public License (GPL) at .<br>
<br>
Availability and requirements<br>
Project name: <software>Base-By-Base</software><br>
Project home page: <br>
Operating system(s): Platform independent<br>
Programming language: Java<br>
Other requirements: Java 1.4 or higher, this requires at least system 10.2.8 on the Macintosh.<br>
License: GNU General Public License<br>
Any restrictions to use by non-academics: Contact authors<br>
<br>
Authors' contributions<br>
RB and AJS were principle programmers of the <software>BBB</software> software. RLR, VT and CU contributed ideas for features and display requirements, and tested the program.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1131889</b><br>
Scoredist: A simple and robust protein sequence distance estimator<br>
<br>
<br>
Background<br>
Estimating divergence time of protein sequences is one of the fundamental problems in bioinformatics. Evolutionary distance estimates are used by many of the most commonly used phylogenetic tree reconstruction algorithms [1-3]. In current research, phylogenetic trees are used for many types of subsequent analysis, e.g. orthology inference [4-6]. Early models for sequence evolution focussed on nucleotides. They commonly employ Markov chains and assume independent evolution at every site. Each of the four nucleotides is identified by one state and the substitution probability is modelled as a state transition probability from one state to another. In the most straightforward approach, the same state transition probability is assigned to every substitution [7]. Subsequent models take account of more nucleotide specific properties, e.g. transitional and transversional substitutions as well as GC content (see [8] for an introduction). These more advanced approaches are bound to nucleotide sequences and cannot be directly used with protein sequences.<br>
Markov chain models for protein evolution differ from nucleotide approaches in their larger number of states and transitions for which parameters need to be estimated. The protein sequence Jukes-Cantor model assigns the same probability to each substitution and is hence a rather poor approximation. This method essentially takes the observed differences between two sequences and corrects this value to the estimated evolutionary distance using a logarithmic function. Other similar methods exist that also correct observed differences, e.g. Kimura's method [9]. Although they produce rather inaccurate distance estimates, correction-based distance estimators are popular because of their simplicity. More advanced protein evolution models estimate parameters from protein sequence alignments. Assuming the same substitutions for closely and distantly related sequences leads to the construction of the Dayhoff matrix series [10]. Following this approach, it suffices to collect data from alignments of closely related sequences to build an evolutionary model of amino acid substitution.<br>
Dayhoff and co-workers introduced the term Percent Accepted (point) Mutation (PAM), which denotes a commonly used measure for evolutionary distance between two aligned sequences (insertions and deletions are ignored). In other words, two sequences at a distance of 150 PAM are related to each other by 1.5 substitutions per position on average. As substitution is a stochastic process, some positions will experience multiple substitutions while others will experience none. It is also possible that secondary substitutions at one site will result in the original residue, making the evolutionary steps invisible. This is in essence the reason why estimating evolutionary distance is so hard ? multiple substitutions cannot be observed directly. An evolutionary distance of 250 PAM corresponds roughly to 80% observed differences. The term PAM is found in literature for both the matrix series given by Dayhoff et al. as well as for evolutionary distance unit. In this publication we refer to the matrices as Dayhoff matrices and reserve the term PAM for distance units.<br>
There are two major shortcomings connected with the derivation of the Dayhoff matrices. First, potential errors inherent in the experimental data will be magnified by extrapolation. Additionally, it is questionable whether substitution probabilities observed on closely related sequences can accurately reflect the evolution of more distantly related sequences. The efforts of researchers since the publication of the Dayhoff matrices have led to several other matrix series, sharing the idea of an underlying Markov chain. They differ in terms of the data they are built upon and account for the above-mentioned shortcomings in various ways [11-13].<br>
The approach behind the BLOSUM matrices [14] is different from Dayhoff's evolutionary model. Whereas the Markov model assumes that any transition probability matrix may be derived from another matrix in the same series, the BLOSUM matrices do not imply any evolutionary time. There is no direct mathematical relationship between matrices in the BLOSUM series. Sequences with identities above a given identity cutoff are clustered and used to derive score matrices. The BLOSUM matrices are known as a good general-purpose choice. Especially, BLOSUM62 is frequently chosen for the alignment of sequences.<br>
<br>
Results<br>
We here introduce Scoredist, a novel correction-based distance estimator for protein sequences. It applies a correction function to an observed reduction in normalised score, rather than to observed differences as other correction-based methods. This gives a better estimate of the divergence in the well-established PAM measure and allows the popular BLOSUM matrix series to be used. Other matrices could in principle be used, but the BLOSUM matrix has proved to be the most universal. Scoredist distance estimates are calculated directly by a simple equation and do not require cumbersome computational approximations, which is needed for e.g. Maximum Likelihood (ML) and Expected Distance (ED) estimates [15]. Additional calibration opens the possibility to make Scoredist tuned to other evolutionary models.<br>
In order to evaluate our novel protein distance estimator Scoredist against other estimators, we generated a large testset of artificial sequence alignments. Simulation is the only way to exactly know an alignment's evolutionary distance. The substitutions were made by <software>ROSE</software> [16] according to an evolutionary model that can be chosen arbitrarily. It is to be expected that a distance estimator based on a particular evolutionary model will perform optimally on a testset generated with the same model. We therefore generated testsets using four different matrix series: Dayhoff [10], MV [12], JTT [11], and WAG [13]. For each model, 2000 alignments were created for evolutionary distances between 1 and 200 PAM units, i.e. 10 alignments for each distance. The Scoredist, Maximum Likelihood, and Expected Distance estimators can all be tuned towards a particular evolutionary model. We therefore used three evolutionary models which were also used to generate the testsets for these distance estimators, and use a shorthand to refer to these as "method-model". For instance, Maximum Likelihood using the MV model is denoted ML-MV. The Jukes-Cantor and Kimura estimators can not be tuned to a specific model but were tested on all four datasets.<br>
Table 1 shows a compressed summary of the results. For each combination of distance estimator and dataset, the average root mean square deviation from the true distance was calculated for all 2000 alignments. The Expected Distance results were similar to ML, as the methods are akin in nature, but ML was generally more accurate and is much more widely used. As expected, low RMSD values as a sign of good distance estimates were generally obtained when using the same model for alignment creation and subsequent distance estimation. This is seen in the diagonal of low RMSD values from Dayhoff/Dayhoff to JTT/JTT. The only exception to this rule was observed when the testset was generated with Dayhoff. Here, ML-JTT was slightly better than ML-Dayhoff. This result was also verified for distances up to 250 and 300 PAM (data not shown). Comparing Scoredist and ML accuracies when training and testing using the same model resulted in a tie. Scoredist was better for MV, ML was better for JTT, and they were equally accurate for Dayhoff. When comparing accuracies for different training and testing models, however, Scoredist dominates. Here, Scoredist performed better than ML in five of the six cases. For the MV testset the difference was very big. The only case where ML was better than Scoredist was again when running ML-JTT on the Dayhoff testset, which for unclear reasons produced very accurate distance estimates.<br>
The Jukes-Cantor and Kimura correction methods are generally less accurate than Scoredist and ML estimators. In some cases they reached higher accuracy than Scoredist and ML trained on the "wrong" model. For instance, on the Dayhoff testset Kimura was better than Scoredist-MV and ML-MV, and on the MV testset Jukes-Cantor was better than Scoredist and ML trained on Dayhoff or JTT. However, Jukes-Cantor and Kimura never came near the Scoredist and ML accuracy when trained on the "right" model. In a real situation, it is of course not known which evolutionary model is most appropriate. Therefore, taking the average RMSD values for each training model reveals the generality and robustness of the method on different testsets. The average accuracy of Scoredist is consistently better than for ML, and Jukes-Cantor and Kimura are even further behind.<br>
Figure 1 two shows a more detailed picture of the different distance estimators. The average of 10 estimates from 10 independent simulations at each evolutionary distance is plotted for data generated with the Dayhoff matrices. The variance among the 10 estimates is not shown for clarity; they are however reflected by the RMSD values in Table 1 which may give a slightly different picture. For instance, it is possible that the average deviation is close to zero if the individual estimates have large positive and negative deviations that cancel each other out. Therefore, the RMSD values should be trusted more than the deviation plots when in doubt. Figure 1A shows the dependence on evolutionary model for Scoredist and ML. Testing on the Dayhoff testset, Scoredist-Dayhoff and ML-Dayhoff stayed reasonable accurate in the entire range (below 5% error). In contrast, Scoredist-MV and ML-MV deviated considerably from the true distance. It is however clear that ML is more affected by switching model than Scoredist is. In Figure 1B the testset was generated with the MV model. Again, the corresponding deviation was observed for "wrong model" estimators. Here it is even more pronounced that ML is more dependent on the model, and generalizes poorly. Scoredist was less affected by the change of model ? Scoredist-Dayhoff was considerably more accurate on the MV testset than ML-Dayhoff. As expected, when Scoredist and ML had been trained on MV data, the accuracy is very good for both estimators. In conclusion, we observed that although the Scoredist method is very simple compared to the ML method, it is approximately equally accurate when testing and training using the same evolutionary model. However, when testing on a different model, Scoredist is considerably more accurate.<br>
<br>
Implementation<br>
The Scoredist estimator was implemented in <software>Belvu</software>, which is a general-purpose multiple alignment viewer that allows basic alignment editing. <software>Belvu</software> can calculate and display phylogenetic trees. The tree reconstruction can be based on Scoredist or other common correction-based distance estimators available within <software>Belvu</software>.<br>
Multiple alignments can be coloured in <software>Belvu</software> according to conservation using average BLOSUM62 score in the column, or by residue-specific colours. User-specified cutoffs can be employed to fine-tune the display. <software>Belvu</software> has a range of functions for sorting, colouring, marking up, and printing alignments. In Figure 2, the alignment is coloured according to conservation, and sorted according to the tree. The effect of distance correction with Scoredist is illustrated.<br>
<software>Belvu</software> can also be utilised for batch mode operations on the multiple alignment, or for producing distance matrices or phylogenetic trees without graphical output. It is available for the most common UNIX operating systems and can be obtained from [20]. A Windows version exists but is less frequently maintained. See [17] for instructions, and [18] for information on the Stockholm format, which is used by the <database>Pfam</database> project.<br>
<br>
Discussion<br>
Our analysis was based on four different evolutionary models ? Dayhoff, MV, JTT and WAG. We chose these because they represent the spectrum of models well. The only tuning done in the Scoredist method is the estimation of the calibration factor c. This factor can be seen as a scaling factor for the logarithm base in equation (5) that needs to be set empirically.<br>
The difference between Scoredist and ML becomes particularly apparent in the MV dataset. There are several hypotheses for this behaviour. The Dayhoff matrices were constructed with the limited data available at the time. Given the substantial increase of research output in this field particularly during the last decade, it is not surprising that the M?ller-Vingron model (published in 2000) reports substantially other results than the Dayhoff (1978) and JTT (1992) matrices. Additionally, the calibration factor c can also be interpreted as measure for the similarity of the respective models. Following this argument, JTT and Dayhoff are more akin given a ?c ? 0.05. The MV model is more distant to both JTT (?c ? 0.11) and Dayhoff (?c ? 0.16).<br>
The Expected Distance estimator generally overestimates distances. For instance, among Dayhoff-calibrated estimators on the MV testset, Expected Distance is more than 10 PAM RMSD units (over 50%) poorer than the best method Scoredist. Similar values are observed for JTT calibrated estimators. Generally, MV-trained estimators are prone to underestimate evolutionary distances (Figure 1A). In combination with the ED higher distance estimation, this rather fortuitously leads to good results for ED ? MV. However, the scope of this research was to identify a robust method that performs well on various data sources. An estimator which is highly sensitive to the data source or possible incorrect calibration is of less value. The best single estimator was JTT-calibrated Scoredist. If the method per se is measured by averaging over all calibrations and testsets, Scoredist receives 15.39, ED 16.89, and ML 17.14 PAM RMSD units. This highlights Scoredist as the most robust estimator, with the distance between Scoredist and ED (?Scoredist, ED ? 1.50) being 6 fold the difference between ED and ML (?ED, ML ? 0.25).<br>
We here only present Scoredist results using BLOSUM62 for calculating the score ? between two sequences. In principle one could use some other score matrix, but we found that this had little effect on the results. Since the goal was to make a general-purpose method, BLOSUM62 was an obvious choice. The key to Scoredist is the usage of scores rather than identities, and the choice of somewhat arbitrary parameters is not of primary concern. At present, gaps in the alignments are not included in the Scoredist calculation. Traditionally, gaps have been difficult to embody in evolutionary models. In the models used here, they are at best crudely modelled by treating every gap equally. An inherent problem is that the probabilities for insertions and deletions (indels) are not necessarily synchronized with the substitution probabilities. Some protein families are more prone to indels than others, hence it is hard to make a generalizable model that suits all protein types. We have experimented with affine gap penalties in the Scoredist method (this is an option in the implementation), but this resulted in decreased accuracy. We therefore do not recommend using gaps to estimate protein distances.<br>
<br>
Conclusion<br>
We have developed the score matrix based distance estimator Scoredist for aligned protein sequences. Its main advantages are computational simplicity and high robustness. Most other distance estimators produce good results for certain evolutionary models but perform poorly on others. The Maximum Likelihood and Expected Distance were found to overfit their estimates to the evolutionary model so much that the results on testsets generated with other models suffered heavily. The correction-based methods Jukes-Cantor and Kimura also favoured a particular evolutionary model, but were not competitively accurate on any testset. It seems that Scoredist achieved the best compromise between accuracy and generalization power.<br>
<br>
Methods<br>
For the estimation of divergence time, let s1 and s2 be two aligned protein sequences (gaps are ignored) of identical length l. A similarity score ? is defined as<br>
<br>
where S is a log-odds score matrix. Log-odds score matrices are constructed such that substitutions by the same or a similar amino acid receive a positive score, whereas substitutions to dissimilar amino acids are attributed a negative score. The expected value for this kind of matrix is negative. This ensures that the comparison of unrelated sequences returns a negative score. For two random sequences of length l the expected score ?r(l) = ?0 * l, where is the expected value of the score matrix. As we strive to measure scores above the scores for the null model of sequence independence, the score ?(s1, s2) is deducted by the expected score ?r, giving the normalised score ?N<br>
?N = ?(s1, s2) - ?r (l). ??? (2)<br>
For two random sequences of length l the expected score ?r (l) = ?0 * l, where ?0 is the expected value of the score matrix.<br>
The expected score ?r for unrelated sequences can be regarded as lower limit. The upper limit of the score between s1 and any other sequences is given by ?(s1, s1). For two different sequences, the upper limit of the score ?U is, for the sake of symmetry, assumed to be<br>
<br>
and normalised<br>
?UN = ?U (s1, s2) - ?r (l). ??? (4)<br>
Any sound score ?N is situated within the interval [0, ?UN]. The validity of the upper boundary follows from the score's definition. The lower boundary might, however, get violated if two sequences receive a score ?(s1, s2) &lt;?r (l). As the model assumes independent evolution already for ?r (l), a score below ?r does not contain any additional information. A lower score is therefore set to ?(s1, s2) = ?r (l). We model the raw distance as a modified Poisson process<br>
<br>
As seen in Figure 3, dr is linearly related to the true distance, deviating only by a constant factor. The Scoredist evolutionary distance estimate of two sequences is given as the product of the raw distance and a calibration factor<br>
ds = c * dr. ??? (6)<br>
Evolutionary distances of 250?300 PAM units are commonly considered as the maximum for reasonable distance estimation and, therefore, the Scoredist estimate ds is restricted to the interval [0, 300] PAM.<br>
Calibration factors can be determined for various evolutionary models. We used the <software>ROSE</software> program [16] to simulate evolution with three different matrix series and generated 2000 sample sequence alignments for distances up to 200 PAM units. The calibration factor c was calculated by least squares fitting on this data, using the BLOSUM62 score matrix for calculating the score ? in the estimator (Table 2, Figure 3). The simulated evolution started with a random sequence of 200 residues. For each integer distance within the interval [1, 200] PAM, we produced 10 alignments, yielding 2000 alignments per dataset. The default gap parameters of <software>ROSE</software> V1.3 were applied. Each dataset was generated with the transition probability matrix and the stationary frequencies of the respective evolutionary model.<br>
Calculation of Maximum Likelihood (ML) and Expected Distances (ED): ML distances were estimated by applying the Newton-Raphson method to the derivative of the likelihood of the evolutionary distance given an alignment. To calculate ED, the same likelihood function was numerically integrated, to get its "center of gravity" [15]. Both methods are implemented in the program <software>lapd</software> (L. Arvestad, unpublished), which uses Perl and Octave. The Jukes-Cantor and Kimura distance estimators were run as implemented in <software>Belvu</software>. The popular <software>PROTDIST</software> program from the <software>PHYLIP</software> package [19] calculates only ML-Dayhoff and Kimura distances. We therefore chose to use <software>lapd</software> in order to assess Scoredist by a broader range of distance estimators.<br>
<br>
Authors' contributions<br>
ES had the initial idea and implemented the method. VH carried out the evaluation and wrote the first manuscript draft. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1190153</b><br>
Theme discovery from gene lists for identification and viewing of multiple functional groups<br>
<br>
<br>
Background<br>
Recent developments in biosciences have created a dramatic change from the analysis of a few genes to large gene lists. These lists are usually selected at the genomic level by criteria such as activity in a stress treatment [1], importance to cell survival in a specific growth condition [2], or as a result of clustering genes by expression profiles [3]. As current high throughput methods produce a vast amount of data as gene lists, the subsequent analysis tends to be a bottleneck due the size of the data set and the high probability of false positive genes among the lists.<br>
One solution to analyse a gene list is to draw information either from the existing literature or from the databases representing whole genome [4,5] or proteome annotations [6,7], and then using these to guide the analysis. Most of these databases simplify the analysis by classifying genes to the biological categories or classes that present their function, localization, or partnership in some protein complex. A further step is to estimate the statistical significance of associations between the classes and genes of the obtained list. Several applications have been recently reported for such analysis [8,9]. Most of these applications compare the frequency of gene classes in the user supplied gene list, obtained by various criteria, to the remaining genes that did not fulfill the criteria. The latter often includes the rest of the genes from the whole genome. The usual outcome from these methods is a sorted list of biological classes considered important. These methods have been beneficial to data analysis by guiding the process towards the most important features in the gene list [10-13]. In addition, the observation of multiple genes from the same functional class increases confidence in results obtained from high throughput methods.<br>
While these methods are useful, several weaknesses are associated with this approach. A gene list can have a heterogeneous structure with multiple dissimilar gene groups such as stress response, a specific metabolic pathway, and protein degradation. The basic statistics used by the previously mentioned methods are often insufficient to reveal this kind of heterogeneity from the associated functional classes. Rather, they have a tendency to be biased toward the gene sub-group associated with the most over-represented functional classes within the analyzed list of genes. This overwhelms many important, but less over-represented, classes that are associated with the rest of the genes in the list. Therefore, it could be hypothesized that there exists other interesting biological functions among the genes that are not members of the best scoring classes. As such, the existing methods do not address this question and thus there is a need for an approach that would concentrate on the possible heterogeneity in the gene list. In the current work, we propose the clustering of a gene list for finding gene groups that differ in functional class annotations.<br>
<br>
Results<br>
Principle of the method<br>
Our method takes, as input, the user given gene list chosen by some selection criteria. The selected list is referred to as a sample gene list, and the gene list that did not meet the criteria is referred to as a reference gene list. The aim is then clustering the sample gene list for finding gene groups with different functional class annotations. The clustering is solely based on the gene associations with functional classes obtained from <database>Gene Ontology</database> (<database>GO</database>) database [14], and the measurements like gene expression level or sequence similarity are not used. As a clustering method, we use Non-negative Matrix Factorization (NMF) [15] to create a k-means like partition. The well known weakness with this type of clustering approach is the requirement to select the number of clusters and the initialization for the algorithm. We circumvent this weakness by using a non-nested hierarchical clustering scheme, which allows parallel visualization of several different clustering results. Here, a gene list is repeatedly divided into a growing number of clusters by clustering from random starting initializations. The different clustering results are presented in consecutive levels ordered with the number of clusters, with the first level presenting the gene list without any clustering. Strongly correlating clusters between the consecutive levels are connected by edges forming a non-nested hierarchy (see figures 1, 2, 3). The output graph highlights the clusters that stay similar through the different clustering levels despite the varying number of divisions and different random starting initializations. The resulting visualization can be used either for obtaining suitable grouping for a gene list, or identifying individual clusters that are of interest.<br>
In the non-nested clustering hierarchy, the cluster contents are described with the most representative functional classes. For this, a combination of three different measures was used to show over-represented classes within each cluster. The measures are positive/negative signed ten based logarithmic transforms [10] of p-values calculated with Fisher's test [16,17], which compares class frequencies between two sets of genes. The first measure, "Original log(p)" (denoted by O.log(p)), makes a comparison between the whole user given sample and reference gene lists. It reports class over-representation that was observed before any clustering. Because of the wide usage of this measure reported in the literature [10,11,18], it is suitable for method comparison. As a comparison, the second measure, "Sample log(p)" (denoted by S.log(p)), concentrates fully on clustered sample gene list by comparing a single cluster against the other genes in the sample list. It highlights the classes that contributed most to the formation of the cluster. The third measure, "Complete log(p)" (denoted by C.log(p)), compares a single cluster against the other genes of the sample gene list and reference. It takes into account both the contribution to the formation of a cluster and the over-representation in the sample list before clustering, and thus we use it for reporting the contents of a cluster. C.log(p) is partly dependent on the preceding clustering, and thus can report some classes that are not over-represented in the whole user given sample gene list, which we are aiming to analyze. Therefore, such hits are filtered by excluding the classes with weak O.log(p) from the report. Similarly, classes that have not contributed to the formation of the analyzed cluster are removed by discarding the classes that do not show even slight over-representation with S.log(p). As a result of filtering, the remaining classes are over-represented in both the analyzed cluster and in the original sample list. In this description, only O.log(p) gives statistically analyzable results because C.log(p) and S.log(p) are both based on the same data with the preceding clustering. Nevertheless, the latter two are suitable for highlighting the classes that are over-represented within the clusters. A more detailed description of the non-nested clustering scheme is given in the Methods section.<br>
<br>
Software implementation<br>
In order to make the method applicable for others, we have developed an end-user program called <software>GENERATOR</software> (<software>GENElist Research Aimed Theme-discovery executOR</software>) for the Windows 2000/XP environments. It takes, as input, the sample and reference lists of genes that can be comprised of gene names or identifiers supported by <database>GO</database> database. The list of available species and allowable naming systems are described more in <software>GENERATOR</software> user manual [21] and in <database>GO</database> web site [22]. Alternatively, <software>GENERATOR</software> can be used to analyze existing binary data matrices like in-house created functional gene classifications or other similar binary data analysis problems consisting of sample and reference groups. The first outcome from the program is a non-nested hierarchical clustering tree, which shows the discovered gene sub-groups from the user given sample gene list. The content of each cluster is described by the two most over-represented classes. A more detailed analysis is also possible for each cluster by viewing the sorted list of over-represented classes or by viewing the clustered genes. The program can create multiple cluster trees, produce statistical evaluations for clustering divisions and single clusters, and provide flexibility in changing the parameters for clustering execution and visualizations. Results can be saved as graph figures and tab-delimited files describing different gene groups or class contents within them. These functions are further described in the program manual. <software>GENERATOR</software> will be updated twice in a year including the <database>GO</database> database within it and is freely available [21].<br>
<br>
Analysis with <software>GENERATOR</software><br>
Gene list from yeast under H2O2 stress<br>
We have analyzed the data obtained from growing yeast deletion strains during oxidative stress [2]. Yeast deletion strains have deletions in genes not needed in normal growth conditions (non-essential genes). The research aims to find new genes and functionalities that are important for the cells to survive and grow in the presence of oxidative stress. We limit the analysis to the gene list obtained from hydrogen peroxide stress (H2O2 stress). This was used as a sample list for <software>GENERATOR</software> and it included 117 genes of which 109 were recognized by the <database>GO</database> database. The remaining 4589 non-essential yeast genes were used as a reference list of which 4115 were recognized by the database. The use of a whole genome as a reference here might cause some error in the results as it is natural to assume that different functional groups have different proportions of non-essential genes. The principal observation when analyzing the results as one group in the original article is the clear association with mitochondrion [2].<br>
Clustering was done with 2 to 6 groups. In the first step the obtained clusters were analyzed against the other clusters using S.log(p) values to determine which functional classes contributed most to the formation of each cluster. The obtained graphical view is shown in figure 1A. The figure shows a cluster of ribosome genes that forms the clearest separate group (marked with I) and remains although the number of clusters changes from 2 to 6. The strong link between the different clustering results (thick lines showing correlations higher than 0.9) highlights this. Similarly, a cluster of genes with RNA associated function (marked with II) is clearly separated and is shown on several levels. Also, a small cluster of 'mitochondrial inner membrane' genes (marked with III), a cluster of genes with unknown function (marked with V), and a cluster associated with 'transcription regulation' and 'nucleus' (marked with IV) can be seen. All of these five clusters stay similar over many levels of the visualization despite the changing number of clusters and different random starting points. The whole cluster tree step was also replicated four times, each showing similar results. These replications are detailed below.<br>
The previous information obtained by S.log(p) explains the clustering, but it does little to help understand the original sample list. This is due to the exclusion of the reference list from the analysis. For example, the previous results do not provide emphasis on mitochondrial functions although it is the most significant theme when analyzing the data as one group (see table 2). Figure 1A also presents 'molecular function unknown' class, although it is under-represented in the original sample list. Therefore, the second step of the analysis is to take the reference gene list into account. Here, classes are sorted with C.log(p) values and O.log(p) and S.log(p) are used as cut-offs to remove non-relevant information. The rationale of using the cut-offs and the purpose of the different values is discussed more in the Methods (Description of the cluster contents). This is also the default view of <software>GENERATOR</software>. The resulting graph is presented in figure 1B. Now the obtained view is different showing 'mitochondrial ribosome' cluster (I, previous ribosome cluster), 'tRNA ligase' cluster (II, previous RNA associated cluster), 'mitochondrial inner membrane' cluster (III) and 'transcription regulation' cluster (IV, previous transcription and nucleus cluster) and a 'mitochondrial genome maintenance' cluster (V, previous cluster of unknown genes). The clusters are the same as the ones shown in the figure 1A but now each one of the clusters shows the functional classes, over-represented in the original sample list, that are associated with the clustered genes. The over-represented classes for clustering with 5 clusters from figure 1 are shown in table 1. In order to see how robust the results are, the non-nested hierarchical clustering was replicated four times. The replications are in figure 2 and show that similar clusters can be obtained with each.<br>
Analysis of the results in figures 1 and 2 (result summary shown in tables 1 and 5 [see Additional file 5]) shows that within the group of genes that first seem homogeneous, there are sub-groups differently associated with the mitochondrial functionality. The strongest feature in the obtained results is the group of mitochondrial ribosome clusters that stays similar whether clustering from 2 to 6 clusters. Analysis of this cluster actually reveals that there are two genes (YNR036C and YPL183W-A) that are reported as hypothetical mitochondrial ribosome proteins. The fact that the mitochondrial ribosome proteins are strongly over-represented in the dataset support the notion that they are mitochondrial ribosomal proteins.<br>
One small group, not mentioned in the original analysis [2], is the group of tRNA ligases (cluster II). Although this group only includes 6 members, its O.log(p) was 6.64 making the over-representation significant. A more detailed analysis reveals that the genes in question are mitochondrion associated tRNA ligases and one of them is a hypothetical mitochondrial tRNA ligase. Again its importance for the growth of yeast cells in oxidative stress further confirms its association with mitochondrial function. The rank of these ligase associated categories starts at 23 in the sorted class list for the original sample gene list (see table 2) and therefore this group can go easily unnoticed if the sample list is analyzed without clustering. The rest of the cluster II (in fig. 1, when using five clusters) includes proteins that link to RNA processing and to translation, for example, NAM1, two mitochondrial elongation factors, and YDR194C.<br>
Cluster V shows 'mitochondrion organization' and 'genome maintenance' (5 and 7 genes, with O.log(p) 4.06 and 5.97) but the analysis of the cluster content shows no clear common theme. Instead, most of the genes have no known function, and therefore this cluster does not seem to contribute to the analysis. Indeed the unknown function was associated to this cluster in figure 1A. A separate cluster of unknown genes is an expected behavior for our method as these genes have highly different <database>GO</database> classification profiles from the known genes. We have also observed it regularly with other datasets. Still, this cluster was able to highlight the small group of genes associated with mitochondrion genome maintenance.<br>
Cluster IV shows nucleus-associated functionalities ('transcription regulator activity', 'regulator complex', 'general RNA polymerase II transcription factor activity'). When the actual cluster content is analyzed, the cluster includes: RNA polymerase II holoenzymes, transcription factors, and transcription regulators. This cluster of genes was unexpected and seems to show a link from nucleus driven functionalities to mitochondrial functionalities. Clusters II and IV show nicely mitochondrion linked functions elsewhere in the cell, but at the same time these groups are harder to detect when analyzing the data as one group (see tables 2 and 7 [see Additional file 7]). In summary, <software>GENERATOR</software> has shown that within the mitochondrion associated gene list, the main members are mitochondrion ribosomal proteins, mitochondrion membrane genes, tRNA ligases, unknown genes, and genes associated with transcription regulation.<br>
<br>
Gene list from drug treated yeast<br>
Another dataset that was analyzed includes the gene expression differences in yeast during itraconanzole treatment, a drug known to affect sterol biosynthesis and normal growth [20]. Both up and down regulated genes were used for the analysis. These contained 255 genes of which 248 were recognized by the <software>GENERATOR</software> <database>GO</database> database. The remaining 6102 non-regulated yeast genes constituted the reference list of which 5369 were recognized by our database. When the obtained gene list is analyzed normally with the sorted class list, the most significant feature observed is the massive over-representation of the 'aminoacid biosynthesis' and related functional classes (table 4).<br>
Similar to the previous analysis, two steps were used and the classes that contributed most to the clustering were monitored first. The results show the 'carboxylic acid biosynthesis' associated cluster (marked with I) a cluster associated with 'cellular process' class (III); a 'macromolecule biosynthesis' associated cluster (II); and a cluster associated with unknown functionality (IV). With a larger number of clusters, 'nucleobase metabolism' and 'transcription' associated cluster (V) can be seen.<br>
When the clustering view is changed to show the over-representation reported by C.log(p) (figure 3B), the previous clusters obtain different annotations (result summary shown in tables 3 and 6 [see Additional file 6]). This analysis step was again repeated four times to see how similar the results remained (figure 6 [see Additional file 1]). Cluster I, that showed in fig. 3A carboxylic acid biosynthesis, is now associated with amino acid and carboxylic acid biosynthesis. It forms the most stable cluster and it is seen regularly on several clustering levels also in the replications. Cluster II (macromolecule biosynthesis) is now associated with steroid biosynthesis. Genes in the cluster represent sterol biosynthesis associated functions and other macromolecule biosynthesis functions (for example synthesis of phospholipids). Steroid synthesis is a known target of the drug and that it is now nicely separated from other functionalities that are likely more secondary responses to the drug. Third, a regularly seen cluster is one enriching the plasma membrane and cell wall associated functionalities (III). The genes in this cluster show many membrane associated functions, like transporting activities. Unexpectedly, another cluster, associated with cell wall (cluster IV) can be regularly observed. A detailed analysis of these clusters still reveals that they are different. Cluster III is associated strongly with 'plasma membrane' and 'cell wall'. The other cell wall cluster (cluster IV) is more connected to unknown cellular component than to cell wall and the connection to cell wall is also very weak. Even a slight raise of the cut-off for S.log(p) would filter this link. A more detailed analysis of the cluster IV reveals that 55 out of the 65 genes in the cluster have biological process unknown. Moreover, molecular function is unknown for 58 of these genes. Therefore this cluster does not contribute to the analysis of the gene list. Cluster V does not seem as stable as the earlier clusters. Still, it is observed in most of the replications (figure 6 [see Additional file 1]). It groups together genes associated with nucleobase metabolism and transcription. Detailed analysis shows transcription factors associated with regulation of transcription from the Pol II promoter. Among these genes, some of them are reported to be important for drug resistance (YLR266C, YCR106W) and to stress response (YFL031W, YMR037C) and to two associated with copper uptake (YGL166W, YMR021C). In summary, we observed with <software>GENERATOR</software> an amino acid biosynthesis associated group, steroid and lipid biosynthesis associated group, a group of unknown genes, and genes associated to membrane and transport.<br>
<br>
<br>
Comparison with competing methods<br>
Sorted class list<br>
<software>GENERATOR</software> was also compared to existing methods. One of the simplest ways of analyzing a gene list is to take it as one single group, analyze how over-represented different classes are, and to report the results as a sorted list. Sorting is based on the p-values calculated for the observed over-representation in order to show the best results at the top of the list. This method does not take into consideration the heterogeneity in the list, but otherwise it is similar to analysis done with each of the <software>GENERATOR</software> clusters. Actually, the first level of the <software>GENERATOR</software> cluster tree graph does this analysis. Therefore we compared <software>GENERATOR</software> clustering to the sorted class list using the results from the first level. We changed the default settings so that the number of reported functional classes was not limited.<br>
The comparison used the two previously analyzed data sets. The results from sorted class list were compared to <software>GENERATOR</software> clustering summaries shown in tables 1 and 3. When the number of classes was limited only by the p-value, an immediately observed drawback of the sorted list method was the amount of information (number of classes) obtained. For the H2O2 dataset, we obtained 75 classes and for itraconanzole 76 with -log(p-value) &gt; 2 (55 and 43 with -log(p-value) &gt; 3). The resulting sorted lists are shown in tables 7 [see Additional file 7] and 8 [see Additional file 8]. This can be corrected by raising the cut-off for the included genes. This is also reasonable as we have not used here any correction for increased risk of false positives due to multiple testing. Strong filtering with p-values or limiting the number of reported classes leaves the most over-represented functional classes. In the example datasets, the most over-represented functional classes were all associated with the same gene group. With H2O2, the first 18 functional classes were associated with mitochondrial ribosome proteins (see table 2). With itraconanzole, the first 19 classes (except classes 9 and 17) show functions associated with amino acid biosynthesis (see table 4).<br>
When the <software>GENERATOR</software> results were compared to a sorted class list, many classes were omitted from the results. With default settings, <software>GENERATOR</software> shows at maximum ten classes for each cluster in the output text file. This filters out the repetitive occurrences of functional classes associated with the same gene group. In the H2O2 dataset, classes like macromolecule biosynthesis, protein metabolism, and large ribosomal subunit were excluded in this way. This seems acceptable as many similar classes are shown in the results by cluster I. The omitted classes can be still viewed with the sorted list available for each cluster. Another group of classes that are not reported by <software>GENERATOR</software> with H2O2 were very broad classes, such as intracellular, cell, or physiological process. These contribute very little information to the analysis. Similar observations were also seen with the itraconanzole dataset, where many amino acid biosynthesis associated classes were excluded from <software>GENERATOR</software> clustering results. As an exception, itraconanzole showed some broad classes in the results (plasma membrane, cell wall).<br>
<br>
Direct acyclic graph<br>
Another way to analyze the obtained gene list is to map the over-represented functional classes into a tree like structure that is behind the <database>GO</database> classes and visualize the results as a graph structure. The benefit to the sorted list presentation is that the hierarchical structures are now visible, highlighting the over-represented functional classes occurring repetitively in the same part of the <database>GO</database> graph. Also, if there are different branches showing over-represented functional classes in the <database>GO</database> structure, they are clearly separated. The major drawback is the large size of the obtained visualization. The graph obtained from <software>AMIGO</software> server [23] using the whole list of over-represented classes from H2O2 dataset was simply too large for analysis (figure 7 [see Additional file 2]). Instead we selected a graphical output from <software>GO term finder</software> at <database>Saccharomyces Genome Database</database> [8] for comparison. The <software>GO term finder</software> adds color coding to show which of the classes showed strongest over-representation. It also tries to make the obtained graph smaller by discarding some branches. As the graph for each ontology is obtained separately, we combined the obtained three graphs to the same picture for a better view. We used <software>GENERATOR</software> clustering summaries shown in tables 1 and 3 for comparison.<br>
In order to compare the obtained <database>GO</database> graphs with the <software>GENERATOR</software> results, we flagged each class that was reported significant if it was included in the <software>GENERATOR</software> result table (figures 8 [see Additional file 3] and 9 [see Additional file 4]). We first observed, in the comparison, that the graphs obtained from <database>SGD</database> <software>GO term finder</software> are still large for analysis. Also, the important features are scattered over three graphs, in comparison to the single table from <software>GENERATOR</software>. It was observed that some classes in the H2O2 data were not shown in the <database>SGD</database> <database>GO</database> graph even though their log-p-value results were highly significant (tables 1 and 3, classes marked as 'missing'). Some of these classes were: aerobic respiration (O.log(p) 7.3), cellular respiration (7.03), and mitochondrial genome maintenance (5.97). This might be an artifact caused by the limited size of the <database>GO</database> graph. <database>SGD</database> graph, on the other hand, showed classes that were not reported by <software>GENERATOR</software>. These classes were the same classes discussed when comparing <software>GENERATOR</software> with the sorted lists. Some of the differences between the results might be explained by the usage of binomial test for calculating significance of the functional classes in <software>GO term finder</software>. It should be noted that the Fisher's exact test used by <software>GENERATOR</software> is a more correct method [8], although we observed similar p-values with both methods. Also the whole genome is always used as a population by <software>GO term finder</software>, which might also cause bias in the results with some datasets (see analysis of H2O2 dataset above).<br>
<br>
Comparison to <software>GOToolBox</software><br>
During the preparation of this manuscript, we also observed another method that performs similar <database>GO</database> clustering. GO-Proxy in <software>GOToolBox</software> [19], takes the user given sample gene list, creates the <database>GO</database> classifications for each gene and clusters the obtained matrix by using czekanowski-dice distance and hierarchical clustering. The reported clusters (called classes) are selected from the different levels of tree with two parameters, defined by the user. One parameter defines how similar genes have to be inside the cluster and the other defines the minimum size for the cluster. The principal difference between the methods is that <software>GENERATOR</software> (with default parameters) reports only the <database>GO</database>-classes that display over-representation in both the original sample gene list and in the obtained cluster, whereas <software>GOToolBox</software> concentrates its analysis to the obtained cluster. Also, <software>GENERATOR</software> gives an overview of the clustered data with visualization.<br>
In the analysis for H2O2 and itraconanzole datasets, <software>GOToolBox</software>, with default parameters, created more and smaller clusters when compared to <software>GENERATOR</software> (tables 9 [see Additional file 9] and 10 [see Additional file 10] for results with each ontology). The cluster number is probably larger because the same clusters with minor changes are selected from different levels of the hierarchical clustering tree which causes repetition in the results. The small clusters in <software>GOToolBox</software> results tend to give a scattered view of the data but could be also useful when analyzing details from the obtained gene list. However, by setting a larger minimum cluster size they can be filtered. With larger clusters <software>GOToolBox</software> reported nonspecific functional classes like cellular process, cell, or metabolism in addition to the same <database>GO</database>-classes that were previously reported by <software>GENERATOR</software> (mitochondrial ribosome classes, tRNA classes etc.). With the default settings, <software>GOToolBox</software> found also some small clusters that were not reported by <software>GENERATOR</software> (clusters associated with 'abiotic stress', 'RNA metabolism' etc.). These clusters were quite small and the most associated functional classes did not show any over-representation in the original sample list (see table 7 [see Additional file 7]) as <software>GOToolBox</software> does not filter the results with O.log(p). <software>GENERATOR</software> could be also run with a larger maximum cluster number in order to obtain similar smaller clusters.<br>
<br>
<br>
<br>
Discussion<br>
We have presented a method that groups a user provided gene list into functionally dissimilar gene clusters. The grouping is done with varying numbers of clusters, which are used to create a tree-like graphic visualization. Despite the emphasis on clustering, our method also analyzes the gene list as a single entity (result with one cluster). The obtained graph presents the main output of the method showing the most important simultaneous gene groups that occur in the data in a single figure. The graph can be created multiple times to see how stable it remains when different random initializations are used for clustering. Our results from clustering replications show that the most visible gene groups remain, thus increasing confidence in the method.<br>
There are two alternative methods previously used to obtain an overview of the over-represented functional categories. Methods like EASE analyze the gene list as one entity and output the functional categories as a sorted list according to the significance of the over-representation. Other methods, like <database>SGD</database> <software>GO term finder</software>, give the over-represented functional categories as a directed tree-like graph by using the hierarchical structure of <database>GO</database>. Graph methods create a much more complex representation with the danger of overwhelming the user with unimportant details. The sorted list gives an impression of a homogenous gene group. As an example, we showed the results from <database>SGD</database> <software>GO term finder</software>, <software>AMIGO</software> visualization, and the sorted list of functional classes for the gene list as one entity. These methods do not group the gene list before analyzing it. A positive unexpected observation was that results from the other methods seemed more informative after we marked them with the corresponding <software>GENERATOR</software> clusters. For example classes in a sorted list can be marked according to which cluster they belong to (see tables 2 and 4). Marking the corresponding clusters enables the opportunity to combine <software>GENERATOR</software> clustering results and results from other methods.<br>
We also compared the <software>GENERATOR</software> results to another gene clustering tool, <software>GOToolBox</software>. The principal difference in methods is that <software>GENERATOR</software> provides the cluster description by using filtering procedure which discards the <database>GO</database>-classes with no over-representation in the original sample gene list and with weak association to the genes of the cluster. <software>GENERATOR</software> includes also visualization for viewing the optional clustering results. Despite the differences we were able to obtain also similar <database>GO</database>-classes with both methods when analyzing the H2O2 and itraconanzole datasets.<br>
Since partitive clustering has an inherent weakness in the initialization, we present a novel solution. Instead of selecting a single clustering number, we monitor the results with a range of clustering numbers. As a result, we obtain correlations between the clusters that highlight those features that can be obtained even though the cluster number would change. The replication of the whole cluster tree visualization was done in order to further highlight those features that are conserved. It should be noted that these ideas could also be used with other clustering applications. Similar work was done by Heger and Holm [24] by replicating NMF many times and looking for the conserved features in the obtained matrix factorizations and by Brunet et al [25] where optimal cluster number was selected by replicating NMF clustering many times.<br>
We analyzed the obtained clusters by concentrating on those functional categories that were over-represented in the cluster when compared to the rest of the gene list and also in the original list of genes when compared to a reference list of genes. If the over-representation in the cluster only would be monitored, the obtained cluster would be well explained, but the drawback would be that the obtained categories could at worst be such that they were under-represented in the original gene list and therefore produce erroneous conclusions. If the over-representation in the original list would be only monitored, the clustering would not be informative to the analysis. The current way of combining these two over-representations highlights those features that are common between the original list and the obtained cluster. As the data is grouped to separate clusters, each of them will represent different features from the list of over-represented functional classes for the original gene list. The reporting method therefore separates those functional categories from the original gene list that are not associated to the same genes and groups together those functional categories from the original gene list that are connected to the same genes. A good example of genes that were associated to the same function were the members of the same protein complex that were often seen as a separate cluster.<br>
The selection of the reported functional categories requires the definition of the cut-off for the significant over-representation. Here the threshold was purposely selected to be liberal (p-value &lt; 0.01, O.log(p) &gt; 2.0). This is known to be too weak a threshold when the analysis includes multiple testing as it increases the possibility of the false positives. Therefore the emphasis was placed in the later analysis on those functional categories that showed clearly stronger over-representation than what the cut-off was and the p-values larger than 0.001 were monitored with caution. Similarly we also discarded classes with S.log(p) &lt; 1 from our analysis. The P-value borders could be selected more precisely by doing repetitive testing with a similar sized sample list with randomly selected members (permutation analysis). The evaluation of the results using runs with randomized samples from the analyzed data is one of the planned additions to the <software>GENERATOR</software> software.<br>
The associated software uses a reference list to calculate over-representation for the original cluster. Although the whole genome for the organism could be used, the reference list will ensure that the biases towards some functional groups in the test situation do not affect the analysis.<br>
The method demonstrates that a drugs primary target can be identified within a separate group among different regulated genes and different cellular functions. Work shown here was done with yeast allowing the use of detailed annotation of the yeast genome. Still, we have also obtained encouraging results from human cell line and C. elegans gene expression datasets (manuscript in preparation). As more information is being gathered from the gene functions, this method should be able to perform even better. Nonetheless, accuracy in the used gene annotations is the weak link for our method. This should not necessarily be a hindrance, as the randomly classified genes should distribute randomly also among the observations. Another limitation is the recognition of the analyzed genes. Gene identifiers can be problematic when working with different naming systems that originate from various databases or high throughput methods, like gene chips. These are also the problems faced by other methods.<br>
The presented software includes the possibility of using it also with binary matrices. The reference group can be given as a binary matrix or as a vector that represents a number of members of each category and also the size of the reference group. This should enable the analysis of other similar binary data sets, like SNP datasets, word occurrences in abstract texts etc. These are being currently tested as future applications.<br>
<br>
Conclusion<br>
We have presented an analysis method and associated software, <software>GENERATOR</software>, for analysis of large gene lists. Our aim has been to fulfill the need for an analysis tool to separate and identify functional gene groups from gene lists that would otherwise be difficult to find. The method should be useful especially as larger and more complex gene lists are produced due to the increased use of high throughput genomic methods.<br>
<br>
Methods<br>
Data representation<br>
The associations between genes and functional classes in the sample and reference gene lists must be represented as a binary matrix to enable the analysis (see figure 4, steps A and a). As functional classes, we use annotations from the April 2004 delivery of <database>Gene Ontology</database> (<database>GO</database>) database [14]. <database>GO</database> includes three principal sub-hierarchies, representing biological processes, cellular components and molecular functions for a gene. We combine the information from all these three hierarchies in the clustering process.<br>
The gene and functional class associations are transformed into binary matrix where rows represent genes and columns represent classes. Association between gene and class is denoted by one and lack of association with zero in a matrix cell. In addition to directly associated classes, a gene is also denoted to associate with its ancestors in the hierarchical <database>GO</database> structure to assure maximal information for analysis. The obtained matrix for sample gene list is inputted for the clustering process whereas the matrix for the reference list is summed into an occurrence vector (figure 4, step b) which is used later for analyzing the over-represented classes within the obtained clusters.<br>
<br>
Clustering technique for binary data<br>
A binary matrix is used as data when clustering the user given sample gene list into a fixed number of groups (see figure 4, step B). Many traditional clustering methods obtain weak results with such data due to its non-continuous nature (see for example [26,27]) and the small proportion of non-zero entries (sparse matrix). Therefore we have selected a clustering procedure based on Non-negative Matrix Factorization (NMF, [15]) that has shown good performance with binary data in the 'topic finding' literature ([15,28]).<br>
NMF aims to reduce the dimensions of multivariate data by factorization X ? WH where X represents the binary matrix obtained from the associations of n genes and m classes in the user given sample gene list. Given the fixed number for r, two matrices W (size n ? r) and H (size r ? m) are produced as a result, representing the input data X in compressed form of r factors. The first of the matrices describes the loadings of the genes on r factors and is further used in clustering. In the clustering process, the genes are deposited into clusters by using a winner-takes-all approach that finds the factor with the highest loading for each gene from matrix W. The relation between the highest loading and sum of all loadings is used to measure the fitness of a gene in a cluster. In the visualization (see next chapter) the fitness is used to present genes in a sorted order for each cluster. More detailed descriptions concerning clustering binary data with NMF are given in [15,28]. We use the NMF algorithm presented in [29] which minimizes the least squares error (LSE) between the input data and resulting factorization.<br>
<br>
Non-nested hierarchical clustering scheme<br>
The core of the proposed method is a non-nested hierarchical clustering tree, which is shown in figure 4, step C. There the user given sample gene list is repeatedly clustered into r number of groups, where r grows gradually from two into a user given number. Each partitive clustering is executed from a random starting initialization using NMF, producing an independent division level to the visualization. The levels are placed consecutively in the growing order of r starting from r = 1, which represents the sample gene list without any clustering. In the visualization, each level is shown with a bar of constant size that is split into r sections. Each section represents a single cluster, the size of which is indicated by the width of the section. Correlations between each cluster in level r and all clusters of previous level r-1 are calculated by comparing cluster memberships of genes with a correlation measure between two binary classifications presented in [10]. The strongest correlation for each cluster is denoted by a line between the corresponding clusters. The width of the line indicates the magnitude of the correlation. The lines between the first and second levels present only the proportions of the genes, as the binary correlation with the first level can not be defined. Together the edges and sections form a non-nested hierarchical tree that visualizes the underlying heterogeneity in the gene and class association data.<br>
<br>
Description of cluster contents<br>
We have developed a procedure for describing the contents of gene clusters (figure 4, steps c and D) resulting from the non-nested hierarchical clustering scheme introduced above. There, a combination of three measures is applied to find informative classes by studying their over-representation in the sample gene list with and without clustering. By definition, the over-representation means a greater frequency of classes in the collected set of genes than in the rest of the population. A robust way to test this is the calculation of p-values from a hypergeometric distribution with Fisher's test [16,17], that we apply. Classes with low p-values are highly over- or under-represented in the gene set and thus interesting. Nevertheless, the significant p-values are small numbers that are difficult to handle and visualize. They neither distinguish the over- and under-represented classes. Thus, we use signed logarithmic transform of p-value introduced before [10] which has negative or positive sign depending on the under- or over-representation and suitable scale for visualization.<br>
In our method, we study the over-representation for multiple purposes. We calculate the p-values for each biological class (description in figure 5):<br>
A) From original sample gene list without any clustering using user given reference gene list as a rest of the population. This is denoted by O.log(p).<br>
B) From each individual cluster using other clusters of sample gene list and reference gene list as a rest of the population. This is denoted by C.log(p).<br>
C) From each individual cluster using other clusters of the sample gene list as a rest of the population and excluding the user given reference gene list. This is denoted by S.log(p).<br>
In the default view of <software>GENERATOR</software>, these measures are used to show the over-represented functional classes in the clustering tree. In each cluster description, the basic over-representation measure C.log(p) is used to sort the classes. As C.log(p) is dependent on the clustering, it ranks high in some classes that are over-represented when measured from the cluster, but not over-represented when measured from the sample gene list without clustering. This is caused by the clustering process, when for example a tight group of genes is associated with the classes that are under-represented in the non clustered list. Since we aim at interpreting the whole list, such classes would be misleading and have to be removed. Therefore we filter them by using O.log(p), which is fully independent on the clustering. Another problem is that C.log(p) can rank high in some classes that have not contributed to formation of the analyzed cluster. These classes are under-represented in the cluster when comparing only to the rest of the sample gene list. Still they are so strongly over-represented in the whole sample gene list that C.log(p) shows over-representation. As these classes are uninformative for the analysis of the cluster, we filter them by using S.log(p). By default, the classes with O.log(p) &lt; 2.0 or S.log(p) &lt; 0.0 are filtered, although we encourage also the use of stricter cut-offs like 3 and 1, which we have found to work better especially with S.log(p). In the non-nested tree visualization, the two best classes from this filtered list are shown to describe cluster contents, and the longer list is available through the user interface (see Software Implementation in Results). The cluster is coloured according to the C.log(p) value of its most over-represented class with the strongest red for largest over-representation.<br>
<br>
Analysis protocol<br>
We study two clustering views for each data set in our analysis. In addition to previously discussed default view, which shows the over-represented classes in the clustering tree, we first study the cluster formation. For that, we sort the classes by S.log(p) within the clusters, which excludes the reference gene list and fully concentrates on clustered data. The outcomes from this setting are shown in fig. 1A and fig. 3A in Results. Similarly, the outcomes from the default view are shown in fig. 1(B) and fig. 3B. In our manual analysis (results shown in tables), we further filtered the results from default view by emphasizing the classes with O.log(p) &gt; 3.0 and S.log(p) &gt; 1.0. In addition to two different clustering views, we also study the stability of our clustering scheme with both datasets in figures 2 and 6 [see Additional file 1]. This helps us to detect random and non-random outcomes in similar way as with single clustering levels explained above.<br>
<br>
<br>
List of abbreviations<br>
<software>GENERATOR</software> <software>GENElist Research Aimed Theme-discovery executOR</software><br>
NMF Non-negative Matrix Factorization<br>
<database>GO</database> <database>Gene Ontology</database><br>
<database>SGD</database> <database>Saccharomyces Genome Database</database><br>
<database>MIPS</database> <database>Munich Information center for Protein Sequences</database><br>
DAG Direct Acyclic Graph<br>
LSE Least Squares Error<br>
log(p) Signed 10 based Logarithmic Transform of p-value<br>
O.log(p) Original log(p)<br>
C.log(p) Complete log(p)<br>
S.log(p) Sample log(p)<br>
<br>
Authors' contributions<br>
The original idea of the approach was introduced by PT. Design of the method and the associated software was contributed equally by both PT and PP. PP implemented and tested the software and executed the analysis for biological datasets. GW reviewed the results and provided advice and guidance on improving the analysis and performing comparison to other methods. The obtained results were interpreted by PT.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1261160</b><br>
Measuring similarities between transcription factor binding sites<br>
<br>
<br>
Background<br>
In order to dissect the complex machinery of transcriptional control computational tools are widely used [1]. Candidate binding sites of known transcription factors are located by consensus sequence search or binding scores calculated from position weight matrices (PWMs) [2]. These matrices are derived from position frequency matrices (PFMs) obtained by aligning binding sites for a given transcription factor. PFMs contain the observed nucleotide frequencies at each position of the alignment. A popular collection of eukaryotic PFMs is given by the <database>Transfac</database> database [3]. Furthermore, an open-access database, <database>Jaspar</database> [4], has been compiled recently.<br>
On-line tools are available to calculate high-scoring binding sites on the basis of these matrix collections [5-7]. For a given transcription factor these programs predict many binding sites (on average every 1000 bp) implying a high excess of false positives [1]. The situation is even worse if hundreds of different binding profiles are studied in parallel leading to multiple testing issues. Often these predictions overlap as a result of similarities of transcription factor binding profiles.<br>
First steps to overcome the flood of false positive signals are accurate predictions of promoter regions and enhancers [8-10]. Phylogenetic footprinting [11-13], correlation with gene expression data [14,15] or analysis of cooperative binding of multiple transcription factors [16] allow to reduce the amount of false positives by at least an order of magnitude. Another helpful strategy is the a priori reduction of the number of matrices to be considered. However, a user-defined preselection of a few matrices is highly subjective and might hide novel interactions of several transcription factors. Therefore, in this paper we combine two objective criteria to measure similarities of transcription factor binding site profiles. These measures allow to construct groups of similar profiles. Representative matrices of the groups may be chosen and constitute a reduced and unbiased list of independent profiles for searching binding sites.<br>
Similarities in the collections of matrices may arise from several sources:<br>
1. Identical transcription factors are represented by different matrices. This appears, e.g., due to the distinct nomenclature in <database>Transfac</database> and <database>Jaspar</database> (for example the TATA-binding protein is referred as TATA in <database>Transfac</database> and as TBP in <database>Jaspar</database>) or due to the availability of matrices obtained with different methods (see for example <database>Transfac</database> matrices SRF_01 and SRF_Q6) or stringency criteria (see for example AP1_Q2 and AP1_Q6).<br>
2. Factors within one family are represented by similar matrices due to the conserved structure of DNA-binding domains [17]. For example, both ATF and CREB matrices belong to the same bZIP family and recognise the TGACGT consensus sequence.<br>
3. There might be so far undetected similarities of different transcription factor binding sites. Such similarities can point to a possible cross-talk between different regulatory pathways (see our discussion of E-box binding sites below).<br>
4. It might be difficult to distinguish matrices for which only a few binding sites are known.<br>
In order to identify similar matrices we combine two similarity measures. The first one is based on the ?2 distance of position frequencies of PFMs. The other utilizes scores from the corresponding position weight matrices (PWMs) ? we calculate for a given pair of binding profiles the scores along a test DNA sequence and take the corresponding Pearson correlation coefficient as a similarity measure. Although related similarity measures have been already studied individually [15,17-21], our combined approach applied to the <database>Transfac</database> matrices reveals that the two selected measures capture different properties of the matrices and therefore the measures complement each other. Moreover, since for many matrices only a few experimentally verified binding sites are available we take into account these small sample sizes in both measures. The application of the measures is illustrated by mapping CLOCK-BMAL1 binding sites of circadian clock genes to the Myc-Max family.<br>
<br>
Implementation<br>
Databases<br>
A commonly used database of experimentally verified transcription factor binding sites is <database>Transfac</database> [3]. The release from May 2004 provides 694 position frequency matrices (PFMs) covering vertebrates, plants, insects and fungi. Recently, a publicly available <database>Jaspar</database> database [4] was compiled with 108 PFMs associated mainly to vertebrates. For our large-scale statistical analysis we discarded all matrices with inconsistencies, for example matrices, where the number of sites aligned to construct the matrix (sample size) could not be determined. Furthermore, we excluded rather poor matrices with a length below 6 bases or a sample size below 5. After these consistency checks and filtering steps we arrived at 637 different matrices for <database>Transfac</database> and 103 matrices for <database>Jaspar</database>. All the matrices can be characterized by their length, the sample size, and the information content [22] (Tab. 1).<br>
<br>
?2 distance D between position frequency matrices<br>
For each possible overlap (of at least 6 bases) of two PFMs we count the number of corresponding columns which are statistically independent. This task can be addressed by the homogeneity test using the ?2 measure with 3 degrees of freedom. The application of PFMs for the characterization of binding sites implies that the nucleotide positions are regarded as independent. Even though statistical dependencies between positions are known [23-25] the assumption of independent positions is a rather good approximation [1,26]. In the following we denote by fb,i and gb,i the entries of the overlapping parts of the two frequency matrices to be compared. The index i refers to the base position along the matrices and b enumerates the four nucleotides A, C, G and T. The ?2 distance at the position i is then given by:<br>
?2=?b=A,C,G,T(Ng,ifb,i?Nf,igb,i)2Nf,iNg,i(fb,i+gb,i)<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqaHhpWydaahaaWcbeqaaiabikdaYaaakiabg2da9maaqafabaWaaSaaaeaacqGGOaakcqWGobGtdaWgaaWcbaGaem4zaCMaeiilaWIaemyAaKgabeaakiabdAgaMnaaBaaaleaacqWGIbGycqGGSaalcqWGPbqAaeqaaOGaeyOeI0IaemOta40aaSbaaSqaaiabdAgaMjabcYcaSiabdMgaPbqabaGccqWGNbWzdaWgaaWcbaGaemOyaiMaeiilaWIaemyAaKgabeaakiabcMcaPmaaCaaaleqabaGaeGOmaidaaaGcbaGaemOta40aaSbaaSqaaiabdAgaMjabcYcaSiabdMgaPbqabaGccqWGobGtdaWgaaWcbaGaem4zaCMaeiilaWIaemyAaKgabeaakiabcIcaOiabdAgaMnaaBaaaleaacqWGIbGycqGGSaalcqWGPbqAaeqaaOGaey4kaSIaem4zaC2aaSbaaSqaaiabdkgaIjabcYcaSiabdMgaPbqabaGccqGGPaqkaaaaleaacqWGIbGycqGH9aqpcqqGbbqqcqGGSaalcqqGdbWqcqGGSaalcqqGhbWrcqGGSaalcqqGubavaeqaniabggHiLdaaaa@6A6E@<br>
where Nf,i = ?bfb,i and Ng,i = ?bgb,i are the sample sizes of the matrices columns at position i. If ?2 exceeds the threshold of ?th2<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqaHhpWydaqhaaWcbaGaeeiDaqNaeeiAaGgabaGaeGOmaidaaaaa@3248@ (p = 0.05) = 7.81 the null hypothesis that the base counts in both columns are from the same distribution is rejected with a p-value of 0.05. In order to simplify the analysis we simply count the number of significantly different positions. The example in Fig. 1 shows that for an appropriate alignment (with shift = 3) of the two matrices all ?2-values are below the ?th2<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqaHhpWydaqhaaWcbaGaeeiDaqNaeeiAaGgabaGaeGOmaidaaaaa@3248@ threshold and hence no column appears to be different. Although the counts in some columns look quite different the limited sample size allows no statistically significant discrimination.<br>
Obviously, the number of significantly different columns depends on the relative position of both matrices. In our algorithm we study all possible alignments with a minimum overlap of 6 bases and containing at least 75% of the information content of each matrix. We calculate the minimal number of different positions among these alignments. We call this number D and interpret it as the distance between the compared matrices. Fig. 1 illustrates that for a correct alignment of the ATF and CREB a distance D = 0 is obtained whereas other alignments lead to statistically significant different columns.<br>
An advantage of the distance measure we use in comparison to earlier studies [15,17,19,20] is the emphasis on the limited sample size of many matrices. Only few binding sites, such as those recognized by the Sp1 factor, are characterized by hundreds of experimentally verified sites. The more common sample size is around 15?20 (see Tab. 1) and, thus, it is much more difficult to distinguish matrices. The ?2 measure leading to the distance D takes into account the limited sample size in a statistically well defined manner. The proposed measure could be generalized by allowing gaps, using the sum of scores or by taking the number of possible shifts into account. Since we studied in this paper only rather strong similarities our simple discrete threshold D ? 1 was sufficient.<br>
<br>
Correlation C between position frequency matrices scores<br>
The information on experimentally verified binding sites stored in PFMs can be exploited to predict novel sites. For this purpose position weight matrices (PWMs) can be constructed from the counts fb,i in the following manner [1,27]. First, the probability pb,i of a base b at a given position i is given by:<br>
pb,i=fb,i+sbNi+?b?=A,C,G,Tsb?<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGWbaCdaWgaaWcbaGaemOyaiMaeiilaWIaemyAaKgabeaakiabg2da9maalaaabaGaemOzay2aaSbaaSqaaiabdkgaIjabcYcaSiabdMgaPbqabaGccqGHRaWkcqWGZbWCdaWgaaWcbaGaemOyaigabeaaaOqaaiabd6eaonaaBaaaleaacqWGPbqAaeqaaOGaey4kaSYaaabeaeaacqWGZbWCdaWgaaWcbaGafmOyaiMbauaaaeqaaaqaaiqbdkgaIzaafaGaeyypa0JaeeyqaeKaeiilaWIaee4qamKaeiilaWIaee4raCKaeiilaWIaeeivaqfabeqdcqGHris5aaaaaaa@4D8D@<br>
where Ni = ?b' fb',i denotes the sample size at the position i leading to the relative frequency fb,iNi<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaadaWcaaqaaiabdAgaMnaaBaaaleaacqWGIbGycqGGSaalcqWGPbqAaeqaaaGcbaGaemOta40aaSbaaSqaaiabdMgaPbqabaaaaaaa@347B@. This estimator is modified using pseudo-counts sb. As suggested earlier [28] we choose sb = Ni4<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaadaWcaaqaamaakaaabaGaemOta40aaSbaaSqaaiabdMgaPbqabaaabeaaaOqaaiabisda0aaaaaa@3078@, i.e. the pseudo-count is proportional to the standard deviation of the counted frequencies. Such a choice of relatively large pseudo-counts has a pronounced effect on PWMs with a small sample size. Due to the pseudo-counts the estimated probabilities are strictly positive even if zeros appear in the PFM. From the estimated probabilities pb,i we obtain the weights wb,i as follows:<br>
wb,i=log?2pb,irb,<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG3bWDdaWgaaWcbaGaemOyaiMaeiilaWIaemyAaKgabeaakiabg2da9iGbcYgaSjabc+gaVjabcEgaNnaaBaaaleaacqaIYaGmaeqaaOWaaSaaaeaacqWGWbaCdaWgaaWcbaGaemOyaiMaeiilaWIaemyAaKgabeaaaOqaaiabdkhaYnaaBaaaleaacqWGIbGyaeqaaaaakiabcYcaSaaa@4134@<br>
where rb refers to the a priori probability to find a base b in the DNA sequence. Consequently, the weights wb,i represent log-likelihood ratios to find a base b at a position i. Finally, the score Sk around the position k of a test DNA sequence is a sum of the weights corresponding to bases observed in the DNA sequence at the subsequent positions starting from the position k. The sum Sk is computed for each position k of the matrix along the DNA sequence. High positive scores Sk indicate locations in the test DNA sequence with strong binding affinities whereas zero or negative scores are found elsewhere (Fig. 2).<br>
This widely used technique of score calculation leads immediately to the second similarity measure (similar in spirit to the method used in [18], but modified to take into account the sample sizes of compared matrices). For two given matrices f and g we can directly obtain the corresponding scores Skf<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWudaqhaaWcbaGaem4AaSgabaGaemOzaygaaaaa@30BC@ and Skf<br>
 MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWudaqhaaWcbaGaem4AaSgabaGaemOzaygaaaaa@30BC@ along all positions k in a given test DNA sequence. If the weight matrices are highly similar we expect positive peaks at nearly the same positions, i.e. a prediction of nearly the same set of binding sites. In order to quantify the similarity of both matrices we calculate the Pearson correlation coefficient along a test sequence. Here we also consider all possible relative shifts between two PWMs (with a minimum overlap of 6 bases) and then take the maximum correlation coefficient as the similarity measure C of the two matrices. We have found, that the correlation coefficients do not depend strongly on the value of the pseudo-counts and reflect mainly the relevant rare peaks.<br>
In this paper we take as the test DNA sequence a random sequence with equidistributed bases. For specific applications it might be appropriate to use other test sequences such as upstream regions of the genes of interest.<br>
<br>
Sensitivity and specificity<br>
Sensitivity and specificity of different methods for measuring similarities of profiles recognized by transcription factors were assessed as follows: since large sets of experimentally verified similar matrix pairs are not available, artificial sets were prepared. A representative initial matrix (either ATF or CREB) was resampled to construct a set of matrices. On average we probed the initial matrix 18 times (which corresponds to the median sample size of Transfac matrices). In order to study varying sample sizes for each generated matrix the number of samples was randomly chosen out of the range from 13 to 21. All the matrices generated this way should be classified as similar to each other. A set with matrices dissimilar to each other was prepared by random shuffling of the contents of the initial matrix. The nucleotide counts at each position were randomly reordered as well as the order of the positions. Additionally, we take into account different lengths of the matrices. Both sets were extended with random columns and the number of added columns was chosen randomly from zero to half of the length of the initial matrix. In the analysis, sensitivity was defined as the fraction of resampled matrices which were correctly identified as similar matrices. Specificity was defined as the fraction of random matrices which were identified as dissimilar. Six methods quantifying similarity of profiles were compared. The D (chi2th) and C (corr) functions were calculated as introduced above. Another score was defined as a sum of ?2 obtained for each compared columns (chi2sum). Three other methods (introduced in [15,17,20]) calculate a total sum over all compared columns of: Euclidian distance (ned), column-column correlation (ccc) and scalar product of columns (sp).<br>
<br>
<br>
Results and discussion<br>
In this paper two similarity measures of matrices are studied. The first quantifies for a given pair of matrices the number of significantly different columns D. The other represents the correlation C of binding sites scores along a DNA sequence for each of the given matrices.<br>
Comparison of both similarity measures<br>
For the <database>Transfac</database> library we analyze whether the pairs of matrices with small distances D and high correlation coefficients C coincide, i.e. for what matrices the two measures give consistent results. Fig. 3 shows histograms of correlation coefficients C for matrices with distances D = 0, 1, 2. It turns out that there are many pairs of matrices with D = 0 and large values of C (see the right peak in the upper panel of Fig. 3). For such matrices the differences between their columns are negligible and predicted binding sites are essentially identical.<br>
There are, however, also many pairs of matrices with D = 0 and relatively small correlation coefficients C (see the left peak in the upper panel of Fig. 3). These pairs refer mainly to matrices with a low information content and/or small sample size. In such cases the differences between columns are not statistically significant (many Ns in both consensus sequences) but their scores along a test DNA sequence correlate only weakly. For example, matrices V$STAT4_01 and V$MEF2_01 (see <database>Transfac</database>) are characterised by sample sizes N = 6, N = 5 respectively and have a distance D = 0 but a correlation C = 0.20.<br>
There are also cases with a high correlation coefficient but with a distance D &gt; 2. Such a situation appears for large matrices for which only a part is informative. For example matrices V$GR_01 and V$PR_01 (see <database>Transfac</database>) have a length of 27, but only six positions constitute the core sequence (TGTTCT). Among the others positions three are significantly different, leading to a distance D = 3 but these differences affect the correlation C only weakly (C = 0.92).<br>
Several alternative measures have been proposed. We assessed the sensitivity and the specificity of these measures, as described in methods. The results of the comparison are presented in the supplemental Fig. 4. Both the our correlation measure and the column-to-column similarity give (for an appropriate threshold) a high specificity and sensitivity. However, in some cases, as illustrated above, adding a second criteria is useful to discard pairs involving large matrices for which only a part is informative. The D measure defined here can be used for this purpose. Both introduced measures quantify different properties and complement each other. Although alternative choices of measures might have been done, the advantage of using the correlation C is its implicit normalisation (the results do not depend much on the length and the sample size of the matrices) and the advantage of the distance D is its easy interpretation (number of different columns). Therefore, in the following, we focus on the most similar matrices based on the distance D and correlation C measures.<br>
<br>
Clusters of similar matrices<br>
Here we study the matrices of both <database>Jaspar</database> and <database>Transfac</database> databases. We consider pairs of matrices for which D ? 1 and C ? 0.8 as highly similar. These stringent thresholds were chosen to identify the most obvious similarities and they imply that the matrices are almost indistinguishable from a statistical point of view and that their scores along DNA sequences are strongly correlated. We verified that for all these pairs of matrices both similarity measures select the same relative shift of the corresponding matrices.<br>
Fig. 4 shows an overview of all such matrices. Even though details of these clusters are only readable in the supplementary material (Fig. 1) the graph reveals interesting properties: The connecting lines visualizing high similarity join <database>Jaspar</database> matrices (ellipses) with <database>Transfac</database> matrices (boxes) in many cases. Consequently, our technique allows an automatic "alignment" of these collections of matrices. This is not a trivial task since the naming conventions used in the databases is different, and thus finding matrices corresponding to each other requires expert knowledge. We find that 84 matrices from <database>Jaspar</database> have counterparts in <database>Transfac</database> with D ? 1 and C ? 0.8. Another 16 matrices have somewhat smaller similarities D ? 3 and C ? 0.6. Only the <database>Jaspar</database> matrices P_HMG-1, P_HMG-IY and V_Ghlf, have no obvious "partners" in <database>Transfac</database>. A complete list of <database>Transfac</database>-<database>Jaspar</database> matrix pairs with high similarities is provided in the supplementary material (Tab. 1). Lists for other thresholds or other sets of matrices can be calculated through our web interface [29].<br>
In addition to the edges between <database>Transfac</database> and <database>Jaspar</database> matrices there are many clusters containing multiple <database>Transfac</database> or <database>Jaspar</database> matrices. These clusters reflect pronounced similarities in the matrix collections. There are for example, matrices of the same transcription factor with different degrees of stringency (see for instance AP1 matrices). Moreover, different transcription factors of certain families have almost identical binding motifs (see for example Myc-Max, USF and ARNT). A complete list of all clusters is provided in the supplementary material (Tab. S2). An interesting collection of structural classes of transcription factors has been compiled recently by Sandelin and Wasserman [17]. Consistent with their results we find also clusters of the ETS family (see cluster 2 in Tab. S2, also enlarged in Fig. 5b), bHLH transcription factors (cluster 15), and REL family (cluster 5).<br>
In Fig. 5 we present enlargements of two selected clusters representing the GATA (panel a) and ETS (panel b) transcription factors family. The high similarity of these matrices cannot be directly noticed by inspection of names or consensus sequences. Furthermore, subgroups might be detected using our statistical approach. For example, the GATA cluster reveals that the <database>Jaspar</database> matrix has particularly high similarity to the <database>Transfac</database> entries GATA1_02, GATA3_01 and GATA6_01, but less similarities to other members of the GATA class. The clusters visualized in Fig. 4 and Fig. 5 can be exploited to reduce the number of matrices. Highly similar matrices match a DNA sequence either both or not at all. Therefore, one could construct "consensus matrices" as in [17] or one might select representative matrices in each cluster. In this way the number of overlapping predictions in the search for transcription factor binding sites can be decreased [17].<br>
<br>
Mapping of novel matrices to databases<br>
A careful inspection of the clusters found automatically by our similarity analysis might reveal unexpected similarities pointing to possible cross-talks of different signaling cascades on the level of transcriptional regulation. As an example we discuss the regulation of circadian clock genes and cell cycle control [30,31]. In both processes bHLH transcription factors bind as dimers to E-boxes. The corresponding Myc-Max cluster appeared already in Fig. 4 (the largest cluster). In the mammalian circadian clock the CLOCK-BMAL1 dimer regulates clock genes such as Per1, Per2, Per3, Cry1 and Cry2. We found no matrix in <database>Transfac</database> or <database>Jaspar</database> describing explicitly the binding sites of CLOCK-BMAL1. Consequently, we constructed such matrices ourselves in two different ways. On one hand we collected 9 experimentally verified binding sites from 7 different clock genes [32-36]. On the other hand, we took from a SELEX experiment 10 sequences with high affinities to the CLOCK-BMAL1 dimer [37].<br>
Both matrices are visualized in Fig. 6a. Details of the matrix construction are given in the supplementary material (Tab. S3). Both matrices contain the E-box consensus motif CACGTG but differ in the flanking regions.<br>
Fig. 6b shows that these novel matrices have highly similar counterparts in <database>Transfac</database> (NMYC, MYC, USF). Consequently, cross-talk of the circadian clock with cell cycle regulation and tumor genesis can be expected at the level of transcriptional control. Indeed, the success of chronotherapies and recent detailed studies on cross-talk underline the dependence of circadian rhythms with tumor growth [38]. Also in the process of liver regeneration a pronounced effect of the circadian clock on cell cycle control has been found [39]. This example illustrates that a careful SELEX experiment combined with a mapping of the resulting matrix to known matrices can reveal possible functions of the corresponding transcription factor.<br>
<br>
<br>
Conclusion<br>
Understanding gene regulation in higher eukaryotes is still challenging and current computational algorithms suffer from a large amount of false positive predictions [1,40]. In particular, mutually dependent position frequency matrices in databases such as <database>Transfac</database> or <database>Jaspar</database> lead to predictions of binding sites which overlap, what may be misinterpreted as a cluster of binding sites. Consequently, a careful pre-selection of matrices is essential. On one hand, expert knowledge can be used to select a subset of candidate matrices for the analysis of upstream regions. Such a selection is, however, subjective and novel combinations of transcription factor binding sites might be missed. On the other hand, for large scale computational studies, it is useful to have an automatic tool to detect similar matrices. Therefore, we introduce in this paper a method combining two independent similarity measures to compare position frequency matrices. This approach can be used to quantify similar matrices, to map the entries of different databases, and to cluster matrices.<br>
The first similarity measure used in our approach is based on a ?2 test. In contrast to earlier approaches based on normalized frequencies [15,17,20] we take into account the small sample size of many matrices. We count the number of significantly different matrix columns which defines the distance D. In this paper we focus on highly similar matrices with D ? 1. In forthcoming studies the ?2 measure might be taken directly to calculate distances of matrices in more detail.<br>
The second measure is related to the primary application of position weight matrices ? the prediction of binding sites in uncharacterized DNA sequences. We calculate for two matrices of interest the scores along a test DNA sequence and derive the Pearson correlation coefficient C of these vectors. Thus large values of C indicate that both matrices predict essentially the same binding sites. In this paper we take a 10000 bp long random sequence with equiprobable and independent bases as the test DNA sequence. However, the measure can be easily adapted also to other test sequences such as sets of promoter regions.<br>
Our combined similarity measure was first used to map the <database>Jaspar</database> matrices to the <database>Transfac</database> database automatically. Then, requiring rather strong similarity (D ? 1, C ? 0.8) we identified similar matrices present in these databases and constructed clusters of almost indistinguishable matrices. By choosing only one representative matrix for each cluster it is possible to construct smaller sets of matrices as input of binding site prediction algorithms. Consequently, this approach decreases the number of overlapping binding site predictions. Moreover, such a reduced set constitutes a better input for methods predicting close occurrences of different binding sites (e.g. [16]). In order to eliminate false signals further, approaches such as phylogenetic footprinting [1,12,13], transcriptional profiling [14], ChIP on chip experiments [41,42] or modeling cis-regulatory modules need to be combined with a preselection of independent matrices. Our combined technique can be used to predict cross-talk on the level of transcriptional control. As an illustration we discuss the cluster of E-box binding bHLH transcription factors. Since circadian clock genes are regulated by a binding site quite similar to the Myc-Max motif, a strong interdependence of circadian regulation and cell cycle control is expected and is indeed known empirically for decades in connection with chronotherapies or liver regeneration.<br>
Finally we use the similarity measures to assign newly derived matrices to known factors. To illustrate this application, we map an E-box matrix obtained from SELEX experiments with the CLOCK-BMAL1 dimer to the Myc-Max cluster. Thus the possible function of poorly characterized transcription factors can be predicted using affinity measurements combined with a comparison of the resulting matrix to database matrices.<br>
<br>
Availability<br>
The method is available through a web interface at .<br>
<br>
Authors' contributions<br>
SK, DG and HH designed the study. SK and DG were involved in programming and SK set up the web interface. SK, DG and HH interpreted the results and drafted the manuscript. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1382255</b><br>
Analysis and visualization of chromosomal abnormalities in SNP data with <software>SNPscan</software><br>
<br>
<br>
Background<br>
A single nucleotide polymorphism (SNP) is a variation in a DNA sequence that occurs in an appreciable portion of the population. High density SNP microarrays provide information on the copy number of individual SNPs, based on measurement of the intensity of hybridized genomic DNA fragments to the microarray. These microarrays also provide genotype information on the state of heterozygosity or homozygosity at each SNP position. SNP arrays facilitate studies of chromosomal copy number change, large-scale linkage analysis, and whole genome association studies.<br>
A variety of platforms for SNP-based genotyping are commercially available [1]. Most rely upon amplification of genomic DNA using the polymerase chain reaction (PCR) to reduce the complexity of the genome. One commonly used SNP microarray from Affymetrix consist of probesets corresponding to 11,555 SNPs at unique genomic loci (the Mapping 10 K array), with an average heterozygosity of about 37% and a median physical distance between SNPs of 105 kb [2]. Another Affymetrix platform contains 116,204 SNPs (the Mapping 100 K Set, consisting of the 50 K Xba and the 50 K Hind arrays that each contain ~58,000 SNPs) assigned to known loci. The Affymetrix <software>GeneChip DNA Analysis Software</software> 2.0 (<software>GDAS</software>) algorithm implements a dynamic model-based algorithm to genotype each SNP [2]. Each SNP genotype is assessed using 40 oligonucleotide probes organized into ten quartets consisting of probes representing both alleles. Four possible states or genotype models are assigned to each probe (A, B, AB, or NoCall), and a genotype call is assigned based on the confidence metric derived from the hybridization intensities to the probes [3-5]. Thus, this approach focuses on the conventional assumption that most SNPs are biallelic (having only two of the four nucleotides present at a given locus).<br>
In addition to genotyping, a second category of information provided by SNP arrays is an estimation of the copy number of chromosomal DNA based on the hybridization intensity signals of genomic DNA samples. SNP arrays have been used to assess copy number changes such as aneuploidy, deletions, and cancer-associated amplifications [6,7]. The Affymetrix <software>Copy Number Analysis Tool</software> (<software>CNAT</software>) [2] displays SNP copy number values and corresponding probability (p) values across one chromosome, as well as genotype scores and p values as regions of LOH. Several recent reports describe the visualization of SNP array data using this tool or similar plots of intensity ratios and/or log10 p values (y-axis) versus chromosomal position (x-axis) [6-8]. These studies have validated the use of SNP arrays to measure chromosomal copy number changes.<br>
A basic requirement of large-scale SNP assays is data visualization, which facilitates data analysis, statistical interpretation, and the identification of patterns of biological interest. We developed <software>SNPscan</software> as a tool for the visualization and analysis of high throughput SNP array data. Its main features are the ability to display the combined information of copy number and genotype, as well as associated p values, in a single plot for a dataset; the ability to simultaneously plot multiple data sets at user specified size; world wide web accessibility; paired (tumor vs. normal) ratio comparisons; and the ability to generate a file for visualization of SNP data on the <software>genome browser</software> developed by the University of California, Santa Cruz (UCSC) [9].<br>
An additional feature of <software>SNPscan</software> is its ability to identify regions of uniparental disomy (UPD). UPD occurs when both copies of a chromosome are inherited from one parent [10]. Two identical copies of the same chromosome may be inherited from one parent (uniparental isodisomy), or two distinct copies of a particular chromosome may be inherited from the same parent (uniparental heterodisomy). UPD may occur across an entire chromosome or segmentally, following somatic events such as mitotic recombination. <software>SNPscan</software> is useful to identify uniparental isodisomy on the basis of a region of homozygosity with a normal copy number. Uniparental isodisomy can be discerned through analysis of an individual genomic DNA sample. Given the data from parental genotypes, uniparental heterodisomy can also be identified.<br>
<br>
Results<br>
Implementation of <software>SNPscan</software><br>
<software>SNPscan</software> was written in the Perl programming language [11], permitting uploads of the user's data, setting of the user's selections, and execution of functions implemented in the R programming language [12]. The <software>R</software> codes execute <software>SNPscan</software>'s algorithm and generate graphics in several formats, to be sent back to the user via the <software>SNPscan</software> website [13] written in HTML and running under an <software>Apache</software> Web server.<br>
Data input<br>
The input data for <software>SNPscan</software> is a text file generated by Affymetrix <software>CNAT</software>v2.1 software, via its export tool (with the first line containing the header "Copy Number Analysis Tool" deleted; this is necessary to allow <software>R</software> to interpret the data matrix). The platform may be the Affymetrix 10 K, 50 K Hind, 50 K Xba, or 100 K (both Hind and Xba) Mapping Arrays. (While <software>SNPscan</software> was designed for use with data from Affymetrix SNP arrays, <software>SNPscan</software> can be used with data from any platform, given the appropriate input format.) The text file has rows corresponding to SNPs. For 10 K arrays, the column headers are (1) row numbers, (2) Probe Set (i.e. SNP identifiers), (3) Chromosome, (4) Physical position, (5) ID_Call (e.g. for the identifier TC02_3387, the column header is TC02_3387_Call), (6) ID_SPA_CN (single point analysis of the copy number estimate), (7) ID_SPA_pVal (single point analysis of the significance of the copy number variation), (8) ID_CPA_pVal (continuous point analysis of the significance value; a significance value for the regional average), (9) ID_LOH (-log10 of the probability of continuous genomic region of homozygous calls). For each additional case analyzed by <software>CNAT</software>, columns (5) through (9) are repeated with values for that dataset. The SNPscan algorithm accepts a text file containing all this information, but does not analyze the ID_CPA_pVal data (note that ID_SPA_pVal information is used by <software>SNPscan</software>). For 50 K or 100 K arrays, columns (1) through (7) are as described above, followed by (8) ID_GSA_CN, (9) ID_GSA_pVal, and (10) ID_LOH. The SNPscan algorithm uses all these column values.<br>
The columns that are used by <software>SNPscan</software> thus consist of SNP identifiers, chromosomal assignment and physical map position, and five additional columns for each array sample, as follows. (1) The call has four states: AA, BB, AB, or NoCall (i.e. no call, or null). These calls are determined in <software>GDAS</software> software using a dynamic model mapping algorithm, based on SNP intensity data [4,14]. (2) The copy number has values typically ranging from ~1 to 3, with a typical mean value of ~2.1 corresponding to disomic, autosomal loci and a mean of ~1.2 for hemizygous male X chromosome SNPs. (3) The negative log10 p value for copy number changes is a measure of the statistical significance of copy number changes. For example, a value of 4 corresponds to p &lt; 10-4 for rejecting the null hypothesis that no copy number changes have occurred at a given locus. The p values in <software>GDAS</software> are calculated by comparison of experimentally derived copy number values to values from 110 normal individuals. (4) An estimate of the statistical significance of copy number changes, accounting for the local regional changes in copy number. (5) A confidence metric of the genotype call, selected in <software>GDAS</software> by choosing the smallest p value from a Wilcoxon signed rank test to evaluate the four possible call models.<br>
A typical <software>SNPscan</software> analysis, consisting of data on ~58,000 SNPs on ten individuals, is approximately 18 megabyte in size for file upload. We have chosen a default restriction of 40 megabytes to the file size for upload. (The availability of source code at the <software>SNPscan</software> website enables users to install the program locally and increase the file upload size.) There are three upload web pages, one for each of the <software>SNPscan</software> tools (described below); a screenshot of one upload page is shown in Fig. 1. Sample data files are provided at the <software>SNPscan</software> website to help users become accustomed to the tools [13]. <software>SNPscan</software> currently supports analyses using NCBI Builds 34 and 35.<br>
<br>
Data output: SNPscanPlot<br>
There are three main outputs of <software>SNPscan</software>: a web page called SNPscanPlot (Fig. 2), a wiggle track (WIG) file that is used to visualize SNP data in a human genome browser (Fig. 3), and a series of plots and tables that summarize SNP genotype and copy number data (Fig. 4). For each SNP array dataset analyzed with the SNPscanPlot, the output includes the following features (Fig. 2A,B). The panels arranged vertically correspond to individual samples (e.g. subjects). Within each individual, the y-axis of the plot has three meanings, displayed with dots, bars, and connected lines of varying colors. The first meaning corresponds to copy number, represented by dots, with a user-defined scale (the default y-axis range for copy number values is 0?8). The second meaning is the grey vertical bar corresponding to LOH (confidence metric of the genotype call). The third meaning of the y-axis is the absolute value of the log10 p value of the single-point copy number (connecting yellow line). The points, in addition to representing the copy number, are also color-coded according to the possible calls: red (heterozygous call, AB), blue (homozygous call, AA or BB), and green (NoCall).<br>
This combination of allelic calls with copy number is a unique feature of <software>SNPscan</software>. The sequence of plotting (LOH, p value, homozygous calls, heterozygous calls, NoCalls) is also uniquely arranged. Combining these two designs, <software>SNPscan</software> can be used to display and thus discover a variety of chromosomal anomalies. As an example, apparent homozygosity in the SNP output (e.g. an AA or BB call) can be due to true homozygosity, or to a hemizygous deletion (e.g. an A or B call) that is interpreted as homozygous by the <software>GDAS</software> software. Interpretation of <software>SNPscan</software> plot outputs can resolve this issue for stretches of homozygosity by simultaneously displaying the copy number (and its associated p value) along with the genotype. For authentic homozygosity, the copy number is not changed, while for hemizygous deletions, the copy number is reduced and there is a significant -log10 p value.<br>
Fig. 2B shows a typical higher-resolution graphical output for a female case (upper two panels) and a male case (bottom panel) on chromosomes 22 and X. The single (hemizygous) male X chromosome has a large grey region (highly significant LOH p values), many blue dots (AA and BB calls) few heterozygous (red) points, and an apparent copy number loss (typically in the range 0.8?1.8) relative to chromosome 22 and relative to the female X chromosomes. For each chromosome, a short bar indicates the position of the centromere.<br>
Each data set (track) in SNPscanPlot includes four statistics in the labeling area at the right of the plot. These are (clockwise from the upper-left corner, for whichever autosomes are selected): the call rate (i.e., 1 ? NoCalls / all calls), the mean, the standard deviation, and heterozygosity rate (AB calls over (AA+AB+BB calls) (e.g. Fig. 2A,B and 5A,B). The call rate information includes all selected chromosomes. The mean, the standard deviation, and the heterozygosity rate (labelled as "AB") include only selected autosomes, unless chromosome X is the only selected chromosome. These values can be used to assess the chromosome copy number in a given chromosome (discussed below).<br>
<br>
Graphical output file formats<br>
Each SNPscanPlot result can be provided in several formats. The server returns a portable network graphics (PNG) output at a 20% reduction in size, to better fit into the screen, for a quick overview (Fig. 2A). The user can click on the PNG figure to obtain a tagged image file format (TIFF) file displayed at the full size as specified by the user (Fig. 1). This allows a more detailed view, but is slower for download due to the larger file size. Links are also provided to obtain a portable document format (PDF) file, as well as a PostScript (PS) file, to permit extremely detailed viewing and printing (e.g. Fig. 2B). The file size for a 50 K SNP array data set for ten individual arrays is typically 800 kilobytes for PNG, 2.7 megabytes for TIFF, 40 megabytes for PDF, and 55 megabytes for PS.<br>
<br>
Data output: genome browser<br>
The <software>SNPscan</software> program allows input data files to be converted to the WIG file format. This allows display of continuous-valued data in a track format that is compatible with the <software>UCSC Genome Browser</software> [15]. An example is shown in Fig. 3. Four custom tracks, visualized by uploading the WIG file to the <software>UCSC Genome Browser</software> website [9], are shown for chromosome 2. Many dozens of additional tracks may be added to the browser, and the data may be explored from a scale of several bases to the entire length of a chromosome.<br>
<br>
Data output: plots and tables<br>
A third feature of <software>SNPscan</software> is a group of graphical and tabular summary statistics to describe the genotype and copy number calls from a SNP data set. Examples include a tabular listing of SNPs organized by the size of a group of homozygous calls (Fig. 4A), and a plot of the number of instances of blocks of homozygous SNPs as a function of the length of the group (Fig. 4B).<br>
<br>
<br>
Identifying microdeletions via <software>SNPscan</software><br>
We analyzed a series of cases to demonstrate and validate the use of <software>SNPscan</software>. These cases include microdeletions, UPD, and possible chromosomal mosaicisms (presence of two or more cell lines, or subsets of a cell line, showing different chromosome constitutions).<br>
Case 1: deletion on chromosome 7<br>
Hoover-Fong and colleagues described a female neonate with a unique phenotype of severe facial anomalies including anophthalmia, cryptophthalmos, bilateral cleft lip, and unilateral cleft palate [16]. Conventional karyotyping and fluorescence in situ hybridization (FISH) revealed a 7p15.1-21.1 deletion [16]. We isolated genomic DNA from a lymphoblastoid cell line (identifier L99-2287) and generated SNP data using both 10 K and 100 K mapping arrays. Using <software>SNPscan</software>, we visualized the 7p deletion region (Fig. 5A, top row). Note the presence of a reduced copy number, a preponderance of heterozygous calls (blue dots) with almost no heterozygous calls (red dots), and a high -log10 copy number p value (i.e. a significant p value). We estimated the size of the deletion by analyzing the <software>CNAT</software>-derived calculation of the <software>CNAT</software> copy number value, single point analysis copy number p value, and p value for LOH region based on genotype calls. Together, these parameters define the beginning and end positions of deleted regions. For this case, the size of the deletion was 6.7?9.2 megabases (Mb) based on 10 K, 50 K Xba and 50 K Hind SNP arrays (data not shown), further refining it from a reported estimate of 14 Mb [16]. In addition, we used genomic microarrays (array comparative genomic hybridization) in a dye-swap protocol to independently confirm the extent of the deletion on chromosome 7p (Fig. 6A). The genomic array result suggests an interstitial deletion of 7.6 Mb, consistent with the SNP data analysis.<br>
The clinical anomalies observed in this case result from a developmental insult between four and seven weeks of gestation. A description of the exact genes that are deleted in this syndrome is necessary to elucidate the etiology of the disorder. Generation of a WIG file and display of the deletion region on the <software>UCSC genome browser</software> provided a view of the genomic landscape including the affected genes (data not shown).<br>
<br>
Case 2: deletion on chromosome 3<br>
Cargile et al. identified a case with 3p deletion syndrome, with the karyotype 46,XY,del(3)(p26.2).ish del(3)(p25.3p26.2)(3ptel+)[17]. Clinical features, typical for deletion 3p syndrome [18,19], included ptosis, microcephaly, growth retardation, and developmental delay. FISH studies narrowed the deleted region to 4.4 Mb, the smallest ever identified for this syndrome. We obtained genomic DNA from a lymphblastoid cell line (L99-2297), generated 10 K and 100 K SNP array data, and identified the 3p deletion region. A region of blue (homozygous calls) and green (NoCall) dots, with a copy number below 1, is evident in the whole-genome view (Fig. 5A, second row) and at higher magnification (Fig. 5B, top). LOH of SNPs indicated a deleted region of 2.9 to 4.5 Mb. Analysis of genomic microarrays suggested an interstitial loss of 3p from RP11-21J23 to RP11-91K4 (1.3 Mb), consistent with SNP data (Fig. 6B). This patient lacks many major clinical features that are typically present in 3p deletion syndrome, such as cardiovascular defects, renal anomalies, triangular face and rocker bottom feet. Thus it is of interest to define the genes that are deleted in this case, as well as the ones that are spared.<br>
<br>
Case 3: deletion on chromosome 2<br>
An interstitial deletion of chromosome 2q32-34 associated with carbamoyl phosphate synthetase I (CPS I) deficiency, a urea cycle defect, was reported by Loscalzo and colleagues [20]. We obtained a skin fibroblast cell line (TC02-3387), purified genomic DNA, and performed 50 K Xba SNP mapping arrays and array comparative genomic hybridization (aCGH; genomic microarrays). In this case a deletion region of approximately 32.2 Mb was refined to 26.1 Mb using SNP arrays (Fig. 5A, third row; Fig. 5B, lower panel), and to 23.3 Mb by genomic arrays (Fig. 6C). The region of deletion is characterized by a large grey area (indicating a significant LOH p value), by many blue dots (homozygous calls) and green dots (NoCalls). The few red dots (11 heterozygous calls out of 558 SNPs in this region) are consistent with the error rate on this individual's X chromosome (10 heterozygous calls out of 1,184 SNPs). The copy number of the deletion region is 1.41, comparable to the X chromosome copy number (1.35) and less than the nondeleted portion of chromosome 2 (mean value 2.22).<br>
<br>
<br>
Identifying uniparental isodisomy via <software>SNPscan</software><br>
Uniparental isodisomy can be detected in the analysis of SNP array data by the presence of regions of homozygosity (i.e. LOH) in the absence of copy number changes. We applied <software>SNPscan</software> to the analysis of a previously identified patient (case 4, below) and apparently normal individuals from the CEPH collection (cases 5 and 6). Altug-Teber et al. [21] recently used Affymetrix 10 K mapping arrays to analyze complete or segmental UPD in six families with diagnoses of Prader-Willi syndrome (matUPD15), Angelman syndrome (patUPD15), Silver-Russell syndrome (matUPD7), Beckwith-Wiedemann syndrome (patUPD11p), pseudohypoparathyroidism (patUPD20q) and a chromosomal rearrangement (patUPD2p, matUPD2q). We analyzed these data in <software>SNPscan</software> and observed uniparental isodisomy consistent with the findings of Altug-Teber et al. (data not shown).<br>
Case 4: 14q uniparental isodisomy in a patient with a translocation<br>
Antonarakis and colleagues described a nine-year old female with a de novo Robertsonian translocation t(13;14), short stature, developmental delay, and other symptoms [22]. Genotyping of polymorphic markers indicated maternal UPD for chromosome 14, including isodisomy for proximal markers and heterodisomy for distal markers. Additionally, there was mosaic trisomy 14 detected in 5% of blood lymphocytes. We obtained a lymphoblastoid cell line (identifier X_1054), purified genomic DNA and performed SNP array analysis. Uniparental isodisomy was evident by a large region of homozygous calls (blue dots), high LOH (grey background), and significant p values (yellow lines)(Fig. 5A, bottom row). We calculated that the copy number for chromosome 14 was 2.19 (3.2% higher than the copy number for other autosomes in this case); however, we did not independently confirm the presence of trisomy in these lymphoblasts, and in general this limited degree of mosaic trisomy does not permit visual identification of subtle copy number changes.<br>
<br>
Case 5: uniparental isodisomy in an apparently normal female<br>
We tested the functionality of <software>SNPscan</software> by visualizing SNP data from 90 individuals (30 trios of individuals and parents) assayed by Affymetrix on its 100 K SNP mapping set. The data are publicly available (downloaded on May 11, 2005)[23]. These samples are CEPH trios from the <database>International HapMap Project</database> [24] with no known chromosomal disorders. A group of ten CEPH SNP data sets visualized with <software>SNPscan</software> is shown in Fig. 7A. In case NA12874, a male with at least six grandchildren, we observed uniparental isodisomy across the entire long arm of chromosome 1 (Fig. 2A, bottom panel; Figs. 7A and 7B, bottom panels). For all apparently normal CEPH cases we studied for which we observed chromosomal abnormalities, there are no known instances of unreported kinship [24].<br>
To confirm that case NA12874 had an extended region of homozygosity, we amplified genomic DNA from that case and five randomly selected CEPH controls. We generated and sequenced eight PCR products, four from chromosome 1p (12 SNPs that were from a presumably unaffected region of the chromosome) and four from 1q (11 SNPs that formed part of the presumed UPD region)(Table 1). The SNP calls were consistent with the sequenced DNA except for one SNP (SNP_A-1692270), in which all DNA sequences were homozygous while all SNP array calls were heterozygous. That discrepancy is likely explained because the 25 nucleotides spanning that SNP are expected to hybridize to two distinct positions on chromosome 1, based on <software>BLAST</software> searching.<br>
For case NA12874, 11 of 11 SNPs on chromosome 1q were homozygous, while only 7 of 10 SNP positions on 1p were homozygous (Table 1). By comparison to five controls, this result is not likely to have occurred by chance (chi squared test, p &lt; 0.01). For the SNP sequences on the 1p arm, the results for case NA12874 included three positions of heterozygosity, and did not differ from five control cases (p &lt; 0.74). Thus, DNA sequencing of genomic DNA confirmed the homozygosity on 1q for N12874, consistent with an interpretation of uniparental isodisomy as visualized by <software>SNPscan</software>.<br>
The copy numbers and call rates typically vary between experiments. For the ten apparently normal individuals in Fig. 7A, the mean autosomal copy number values ranged from 2.08 to 2.11 (standard deviation from 0.43 to 0.56), with call rates from 98.1% to 99.4%. For the cases in Fig. 5A, the mean autosomal copy number values ranged from 2.12 to 2.29 (standard deviation from 0.62 to 1.25), with call rates from 90.5% to 95.8%. <software>SNPscan</software> is useful for identification of abnormalities with datasets of varying call rates.<br>
<br>
Case 6: Partial 2q uniparental isodisomy<br>
Inspection of <software>SNPscan</software> visualization of the 30 CEPH trios revealed a possible case of partial uniparental isodisomy on chromosome 2q in case NA07056, a 65-year old female with at least six grandchildren (Fig. 2A,B middle panels). <software>SNPscan</software> showed a small segment in chromosome 2q of NA07056 as homozygous. Using the table and plot tool, we determined that region includes a stretch of 168 homozyogous SNPs in Hind 50 K array, and 170 SNPs in the Xba 50 K array. The length of this region was 7.98 Mb.<br>
<br>
<br>
Identifying mosaic chromosomal deletions via <software>SNPscan</software><br>
Mosaic chromosomal gains and losses complicate SNP analysis because somatic tissues may express variable chromosomal changes between cell types or within a population of cells. <software>SNPscan</software> can detect some cases of mosaicism as well as related problems such as mislabelled samples.<br>
Case 7: 2q Deletion with mosaicism found in an apparently normal control<br>
By using <software>SNPscan</software> to analyze the 100K SNP data from apparently normal CEPH controls, we identified a previously unreported microdeletion on chromosome 2q24 (7.8 MB in size) of a healthy Caucasian female (Coriell repository identifier NA07055)(Fig. 2A, top panel; Fig. 7A, second panel; Fig. 7B, top panel). There was a region of homozygosity, with only three AB calls among a span of 158 consecutive SNPs from the Hind chip, and 21 AB calls among 189 consecutive SNP calls from the Xba chip. For this putative deletion region, the copy number was 1.66. (For all other autosomes, the mean copy number was 2.11.) There was an increased proportion of no calls in this region (15 no calls per 189 SNPs for the Xba array or 7.9% relative to 1.0% no calls across all autosomes), also consistent with the occurrence of a hemizygous deletion. We used FISH to confirm the presence of a deletion. Using two BAC probes (RP11-350H9 and RP11-1105K12), 16 of 23 metaphase cells (70%) displayed a deletion in 2q24, while the other 7 metaphase cells displayed both copies. An example of each case is shown in Fig. 8. These results suggest that the individual had a mosaic deletion in this region. This is consistent with the information displayed by the <software>UCSC Genome Browser</software> (Fig. 3), where the CN (copy number) track had lower values, the p_value track indicated the statistical significance of the copy number change, but the LOH track showed many fragmented bars of light color (instead of a large dark block shown in typical deletion segments) due to the mosaic nature of this cell line. In an independent study, researchers at the Coriell Cell Repositories performed aCGH using Spectral Genomics microarrays on this cell line (GM07055)(Dr. Patrick Bender, personal communication). A deletion of 8.7 Mb was observed, consistent with our findings.<br>
<br>
Case 8: identifying loss of the X chromosome with mosaicism<br>
Analysis of case NA10854 with <software>SNPscan</software> indicated a loss of one copy of the X chromosome, with mosaicism (Fig. 9). NA10854 was a 42-year old Caucasian female with 8 children. Of the 90 CEPH individuals, the mean copy numbers for all 44 male X chromosomes was 1.205 for Hind and 1.279 for Xba 50 K arrays, whereas the mean copy number for all 46 female X chromosomes was 2.018 for Hind and 2.093 for Xba. For case NA10854, the copy numbers were 1.429 for Hind and 1.552 for Xba. This finding would be expected if 70% of the cells from this individual had only one copy of chromosome X. Subsequent karyotyping by the Coriell Cell Repositories indicated that an X chromosome was missing from 68% of the cells, with a karyotype 45,X34/46,XX16 (Dr. Jay Leonard, personal communication).<br>
Loss of a copy of the X chromosome can occur in females in vivo as a function of age, and in cell cultures in vitro as a function of population doublings. Further examination of <software>SNPscan</software> data revealed three other females (NA07348, NA10859, NA12145) with low chromosome X copy numbers, as listed in Table 2. These likely also represent females having mosaic loss of the X chromosome as revealed by <software>SNPscan</software>.<br>
<br>
Case 9: identifying mislabelled SNP data<br>
When displaying the 100 K trio data downloaded from Affymetrix's website, <software>SNPscan</software> revealed several anomalous results for NA11839 (father of NA10854) and NA11840 (mother of NA10854), as shown in Figure 9. For the NA11839 (male) Hind 50 K data, there were unexpectedly 263 heterozygous calls and a copy number of 2.031 for the X chromosome, typical of a female SNP profile. For the NA11840 (female) Hind data set there were zero AB calls and an X chromosome copy number of 1.222, typical of a male SNP profile. The Xba 50 K array data appeared appropriate for each gender. We concluded that the labels on the Hind SNP data sets were likely erroneous. After swapping them (Fig. 9, bottom two panels) the apparent error was corrected. The AB calls and averaged copy number of chromosome X, listed in Table 3, also confirmed these labelling errors.<br>
<br>
<br>
Paired data comparison via <software>SNPscan</software><br>
In some applications of SNP technology, paired samples (e.g. cancerous versus normal tissue) from one individual are analyzed. If the paired data option is selected, <software>SNPscan</software> generates a third plot for every two SNP data set inputs, in the input #1 vs. input #2 order. The third plot provides a series of comparisons between the samples.<br>
Case 10: paired samples<br>
We analyzed data from paired lung cancer and unaffected tissue (whole blood) from the same individual (Fig. 10). When the paired data comparison option is selected in <software>SNPscan</software>, the user must provide an even number of SNP data sets in the input file. <software>SNPscan</software> allows its users to choose plotting colors for eight types of information that are displayed to visualize differences between a paired set of samples. The eight categories are (1) retention of homozygosity (e.g. an AA genotype matched to AA in the second sample, or BB? BB); (2) retention of heterozygosity (e.g. AB? AB); (3) genotype change from homozygous to heterozygous (AA? AB, BB? AB); (4) LOH genotype change (AB? AA, AB? BB); (5) NoCall in the first sample but not in the second; (6) NoCall in the second sample but not in the first; (7) NoCall in both; and (8) diploid switch (AA? BB, BB? AA)(this is not expected to represent a biological phenomenon). Fig. 10 (third panel, dotted oval) highlights a narrow region of copy number gain (genomic amplification) in a tumor sample. Genotype changes from heterozygous to homozygous are highlighted in red, showing a cluster of such changes in the amplification region. This might have occurred due to the expansion of one allele resulting in apparent homozygosity.<br>
<br>
<br>
Comparison to existing tools<br>
<software>SNPscan</software> can be compared to other algorithms and programs that perform the analysis and visualization of SNP microarray data. Various tools have been created to analyze SNP data, including the discovery and analysis of SNPs as well as predictions of functional aspects of SNPs (reviewed in [25]). A variety of reports describe SNP copy number and genotype (LOH) data in adjacent plots [6-8,26,27], highlighting the usefulness of SNP arrays in generating both types of information. A relatively limited number of tools provide data visualization features partially overlapping those of <software>SNPscan</software>, as follows.<br>
Affymetrix's <software>Copy Number Analysis Tool</software> v2.1<br>
<software>SNPscan</software> analyzes SNP data that have been processed by <software>CNAT</software>v2.1, providing a variety of features for further analysis and visualization. <software>CNAT</software>v2.1 provides a visualization tool showing neighborhood smoothed copy number, an associated p value, and LOH information on three separate tracks, for one chromosome of one individual at a time [2]. It does not provide the capability to efficiently scan for chromosomal anomalies in datasets from a large number of individuals. In contrast, <software>SNPscan</software> allows data from multiple individuals to be analyzed simultaneously, limited only by the size of the upload buffer. (Users with dozens or hundreds of SNP data sets can set up a local copy of <software>SNPscan</software> and increase the upload capacity.) Relative to <software>CNAT</software>v2.1, <software>SNPscan</software> offers unique features including visualization of regions of uniparental isodisomy, tabular and graphic summaries of genotype and copy number data, and conversion of SNP data to the WIG format. <software>CNAT</software>v2.1 allows conversion of SNP files to an integrated genome browser format, but does not permit direct upload to the <software>UCSC genome browser</software>, and only one metric (LOH, copy number, and associated p values) may be exported at a time.<br>
<br>
<software>dChipSNP</software><br>
<software>dChipSNP</software> permits concurrent analysis of LOH and copy number changes in paired samples [28,29]. The three main functions of <software>dChipSNP</software> [30] include statistical inference to identify LOH regions, copy number analysis, and linkage analysis. The <software>Copy Number Analysis tool</software> of <software>dChipSNP</software> offers a raw, or an inferred, copy number track using multiple normal samples for reference [26]. The algorithm includes a hidden Markov model to make inferences about LOH, with four states in the model comparing normal and tumor samples (non-informative, no call, loss, and retention). <software>dChipSNP</software> is a highly useful program, but requires paired samples for its LOH analyses, making it particularly appropriate for comparisons of tumor DNA and matched controls. While <software>SNPscan</software> is platform-independent, <software>dChipSNP</software> use is restricted to the Windows operating system.<br>
<br>
<software>Copy Number Analyzer for Affymetrix GeneChip Mapping</software> 100 K arrays (<software>CNAG</software>)<br>
Nannya et al. introduced an algorithm for analysis of paired samples using data from Affymetrix GeneChip Mapping 100 K arrays [31]. The CNAG algorithm provides an improved signal-to-noise ratio for the detection of copy number changes through the use of a hidden Markov model. Its features include corrections for the length and GC content of PCR products used for array hybridization, and optimized selection of the reference samples. The output includes a chromosome ideogram (x-axis) versus a series of tracks (y-axis) containing copy number ratios, copy number inference from a hidden Markov model, and heterozygous SNP calls. <software>CNAG</software> thus is distinguished from <software>SNPscan</software> which displays combined copy number and genotype information. <software>CNAG</software> is available by download as a set of algorithms written in C++ for Microsoft Windows, but a web-based implementation is currently unavailable.<br>
<br>
<br>
<br>
Discussion<br>
A variety of technologies have been applied to the measurement of chromosomal abnormalities including conventional karyotyping (e.g. Giemsa staining of metaphase chromosomes), FISH, and conventional and array-based CGH [32,33]. SNP arrays represent a high-density, high-throughput technique with the capacity for measuring highly informative allelic information from complex genomic DNA samples [34]. For SNP array data analysis, a major requirement is accurate measurement of the genotype call and the copy number. Affymetrix <software>GDAS</software> software provides calls using a dynamic model algorithm. <software>GDAS</software> includes a calculation of the probability of a stretch of homozygosity occurring at random in the dataset, based on the probability of a homozygous call for each SNP relative to a reference set of 110 individuals [2]. The <software>GDAS</software> and <software>CNAT</software> software further provide estimates of the chromosome copy number. p values are generated by comparison of observed single-point and genome-smoothed estimates of the copy number to a reference set of 110 ethnically diverse individuals [8]. Genome-smoothed estimates account for the behavior of neighboring SNPs in analyzing copy number and associated p values.<br>
In addition to generating statistical estimates of genotype and copy number values in samples assayed on SNP arrays, it is also essential to visualize the data. We created <software>SNPscan</software> as a web-accessible tool to both analyze and visualize data sets that have been initially processed in <software>GDAS</software> and <software>CNAT</software>.<br>
<software>SNPscan</software> is useful for the following six reasons. (1) <software>SNPscan</software> provides a variety of useful summary statistics. These include the mean autosomal copy number and standard deviation as well as the call rate and percent heterozygosity for each SNP dataset. The <software>SNPscan</software> plots and tables also offer a variety of summary measures of LOH and copy number changes (e.g. Fig. 4). (2) It is extremely helpful to visualize data across multiple individuals (e.g. Fig. 7) to assess whether observed changes are specific to particular samples. In some applications, it is also helpful to analyze paired (e.g. normal and tumor) samples (Fig. 10). (3) A main feature of <software>SNPscan</software> plots is the integration of copy number and genotype data. This is crucial in interpreting the genetic mechanisms could account for observed SNP data, including deletions and duplications. Visualization using <software>SNPscan</software> indicated a series of chromosomal abnormalities. We validated several of these by FISH, genomic DNA sequencing, and aCGH. A number of these chromosomal anomalies, such as mosaic loss of a portion of chromosome 2 (Fig. 7), and mosaic loss of the X chromosome in four apparently normal females (Fig. 7 and Table 2), would have been difficult to detect without an appropriate visualization tool. While <software>SNPscan</software> is useful to identify a variety of chromosomal abnormalities, it is also important to consider possible genetic mechanisms such as de novo mutations, inherited mutations, inbreeding, etc. (4) In addition to being web-accessible, <software>SNPscan</software> offers both low-resolution (PNG, TIFF) and high-definition (PDF, PostScript) graphical output formats at user-defined sizes. (5) As a further feature of <software>SNPscan</software>, SNP datasets can be converted to the WIG format for visualization with the <software>UCSC Genome Browser</software> [15]. This provides tremendous flexibility and depth in further exploring the genomic landscape of potentially affected regions. (6) SNP arrays also represent a powerful tool for the detection of UPD and in particular uniparental isodisomy, which appears as a region of homozygosity without copy number change. <software>SNPscan</software> permitted detection of UPD in a previously characterized clinical case (Fig. 5) and in an apparently normal case (Fig. 7). There are three main disease consequences that may be caused by UPD, all of which could occur in a single patient: trisomy mosaicism, homozygosity of recessive autosomal mutations, and genomic imprinting [35-37]. While UPD has only been appreciated as a genetic phenomenon since 1980 [10], it is likely that the analysis of high-density SNP data will enable the discovery of many more cases.<br>
<br>
Conclusion<br>
<software>SNPscan</software> enables its users to convert high throughput, alphanumeric SNP data into plots of various sizes and resolutions for rapid visual identification of chromosomal anomalies. <software>SNPscan</software> uniquely correlates each SNP's copy number information with its allelic call information. The correlated data plotted across physical positions, aided with LOH and p-value information, generates distinct patterns for easy visual identification of a variety of abnormalities. <software>SNPscan</software> is freely available to the research community.<br>
<br>
Methods<br>
Acquisition of cell lines and DNA samples from patients and controls<br>
Genomic DNA and lymphoblastoid cell lines were obtained from apparently normal individuals (Coriell Cell Repositories, Camden, NJ)[38]. Identifiers for Coriell DNA samples begin with the letters NA, while cell lines begin with GM. Lymphoblastoid cell lines were obtained from patients with known chromosomal abnormalities from the Kennedy Krieger Institute. Samples were deidentified, and informed consent was obtained in all cases. The study was conducted with Institutional Review Board approval from the Johns Hopkins University.<br>
<br>
Acquisition of SNP data<br>
Genomic DNA samples (250 nanograms) were analyzed on Affymetrix, Inc. (Santa Clara, CA) SNP arrays [2]. The quantity and integrity of all DNA samples was assessed by electrophoresing an aliquot on an agarose gel (1%) and staining with ethidium bromide. SNP array data were generated at the Center for Inherited Disease Research during training on SNP array protocols (Johns Hopkins) or in the laboratory of Dr. David Sidransky (Johns Hopkins University). SNP microarrays consist of either the 10 K set or the GeneChip? Mapping 100 K Set, consisting of a 50 K XbaI chip and a 50 K HindIII chip. Initial data analysis was performed with <software>GeneChip DNA Analysis Software</software> (<software>GDAS</software>) software and the <software>Chromosome Copy Number tool</software> (Affymetrix, Inc.). Additional SNP data sets were acquired from publicly available sources.<br>
Affymetrix 100 K set data were analyzed with <software>GDAS</software> software. Quality control from Affymetrix includes Modified Partitioning Around Medoids (MPAM) Call Rate (MCR), which is applied to 10 K chips [39]. MPAM is a model-based algorithm to determine which allele is present at a particular SNP site in each sample. Models of the relative allele signal (RAS) value were developed based on the performance of over 110 presumably normal control samples. "Call zone" scores are used to partition scores into the categories AA, BB, AB, or NoCall. For the 100 K set Affymetrix SNP chips, a Dynamic Model Mapping (DMM) algorithm is applied. Probe cells are paired to provide a perfect match (PM) and mismatch (MM) 25 mer oligonucleotide. In all, for each SNP there are ten quartets, of which the optimal seven are used. Each quartet consists of a PM for allele A, mismatch for allele A, PM for allele B, and mismatch for allele B. The various quartets correspond to offsets of the mismatched nucleotide from the 13th (central) position of the 25 mer oligonucleotide. DMM is a likelihood model based algorithm that uses Wilcoxon's signed rank test to provide a genotype call for each SNP, as well as quality information for each call.<br>
<br>
Use of <software>SNPscan</software> website<br>
A typical use of the web-accessible version of <software>SNPscan</software> is as follows. From the home page of <software>SNPscan</software>, three main tools are available: <software>SNPscan</software> (to generate <software>SNPscan</software> Plots), <software>KKISNP</software> (to obtain summary tables and graphs), and <software>SNPscan Browser</software> (to create a wiggle file for upload to the <software>UCSC Genome Browser</software>). To use <software>SNPscan</software>, one follows the 12 steps described in detail on the website. Briefly, these steps are as follows. (1) Select a file to upload. This can be from a local machine, or from sample text files provided on the <software>SNPscan</software> website. (2) Enter the file name. (3)(4)(5) Set the height, width and width ratio of the output plots; default settings are provided for letter size paper. (6) Set the upper limit for the y-axis; the default setting of 8 is appropriate for most users. If left blank, the y-axis scales to the highest data values. (7) Specify the chromosomes. As a default, when this field is left blank, all chromosomes are shown. One can select a specific chromosome (e.g. 4) or set of chromosomes (e.g. 4, 5, 11), or group number (e.g. G for chromosomes 21 and 22). (8) Plot LOH; this is selected as a default. (9) Plot p value; this can be adjusted for 10 K or 50 K and 100 K analyses, or can be turned off. (10) Plot a comparison of paired data (e.g. paired normal versus cancer samples from individuals). For this feature, a third track is generated for each pair, highlighting their differences. There are eight optional color selections, with 11 colors in each palette, for features such as allelic gains and losses. (11) Select the NCBI Build (34 or 35). (12) Click the submit button. Depending on the size of the dataset and the network traffic, it may take several minutes for data to be returned. The output appears in the TIFF format, with options to select PNG, PS, and PDF formats.<br>
<br>
Acquisition of aCGH data<br>
Chromosomal microduplications and microdeletions were assessed using CGH arrays (Spectral Genomics, Inc., Houston, TX)[40]. The SpectralChip 2600? array consists of 2,600 BAC clones corresponding to all 24 human chromosomes. BACs are spaced at an average 1 Mb interval. Genomic DNA from each patient (test sample) and from a pool of normal individuals (reference sample) was fragmented by sonication (Misonix, Inc., Farmingdale, NY). 1 ?g of genomic DNA from each patient sample was labeled with Cy3-dCTP or Cy5-dCTP (Amersham Biosciences, Piscataway, NJ) and hybridized according to a Spectral Genomics protocol. To confirm adequate dye labeling efficiency, samples were electrophoresed on a 1% agarose gel. Repetitive DNA sequences were blocked by the addition of Cot-1 DNA. Hybridization was for 16 hours at 37?C followed by a series of washes (2X sodium chloride sodium citrate, 0.5% sodium dodecylsulfate, 22?C, 5 seconds; 2X SSC, 50% formamide, 50?C, 20 min; 2X SSC, Igepal 0.1%, 50?C, 20 min; 0.2X SSC, 50?C, 10 min, then rinsed twice in H2O). Slides were dried with a stream of nitrogen gas, then scanned with a GenePix 4000B (Axon Instruments, Sunnyvale, CA). Image analysis was performed using <software>GenePix Pro</software> software to generate a GenePix Results (.gpr) file. The fluorescence ratio was determined in order to identify regions of genomic DNA that deviate from a 1:1 ratio. The .gpr file was analyzed using <software>SpectralWare</software> software (Spectral Genomics).<br>
<br>
Fluorescence in situ hybridization<br>
FISH was performed on metaphase chromosomes by the method of Pinkel and Gray [41]. BAC clones were obtained from Roswell Park Cancer Institute (Buffalo, NY). 1 ?g of BAC DNA was labelled by nick translation with a kit (Vysis Inc., Downers Grove, IL) including incorporation of Spectrum Orange dUTP. The BAC DNA was separated from the reaction mixture, co-precipitated with highly repetitive DNA, denatured, pre-annealed, then hybridized in situ to metaphase cells.<br>
Lymphoblastoid cells were obtained from the Kennedy Krieger Institute (Baltimore, MD) or the Coriell Cell Repository (Camden, NJ). Cells were grown in RPMI medium, treated with colcemid (a mitotic spindle inhibitor), and metaphase spreads were incubated with chromosome-specific telomere probes (Vysis, Inc.) and labelled BAC clones. Cells were counterstained with 4',6-diamidino-2-phenylindole (DAPI) to visualize nuclei. Fluorescent signals were visualized using a Zeiss AxioSkop equipped with epifluorescence, appropriate filter sets, and a computer-assisted FISH capture system and software for producing high resolution images.<br>
<br>
Genomic DNA sequencing<br>
Genomic DNA samples from five apparently normal cases (NA07357, NA06985, NA07056, NA10855, NA12006) and one case suspected to have UPD (NA12874) were purchased from Coriell Cell Repositories. Genomic DNA (100 ng) was PCR amplified (Expand High Fidelity PCR System, Roche Applied Science, Indianapolis, IN) using standard conditions including 30 to 35 cycles using denaturation (94?C, 15 sec), annealing (60?C, 30 sec), and extension (72?C, 1 min). The eight sets of oligonucleotide primers were as follows (numbered 1p1 to 1p4 for chromosome 1p region, and 1q1 to 1q4 for chromosome 1q): 1p1, 5'-CTCTGTGCAAGGTGTGAGGA-3' and 5'-ATGGCCCAAGGTCACATAAA-3'; 1p2, 5'-TTGAAACACTTCACAAAAGATGTG-3' and 5'-GTGCTCCTGGGAGAACTCAG-3'; 1p3, 5'-CCCAGTGCCATTATTACACTCA-3' and 5'-ATAGGGGCTCTGCACCTTTC-3'; 1p4, 5'-TGTATTGTTGGATTTGGTTTGC-3' and 5'-AGTCCCAGATGGGTTCACTG-3'; 1q1, 5'-TGTCTTCCAAAACGCACTTG-3' and 5'-AGCCCATCACGTCATATTCC-3'; 1q2, 5'-GGGGGTATCAGAGGCAATTT-3' and 5'-AGTGAAGAGCTCCTGCCTTG-3'; 1q3, 5'-CATCCGTGAGAATGGAAACC-3' and 5'-ATGAGGTCCATGCAGGAAAA-3'; 1q4, 5'-CAGGCAGGCTTTGACTCTTC-3' and 5'-CCCTAGAAACAGCTCCCAAA-3'. PCR products were electrophoresed on a 1.5% agarose gel, purified (Gel Extraction Kit, Qiagen, Valencia, CA), and sequenced (Synthesis and Sequencing Facility, Johns Hopkins). Eight sets of primers were designed to span 2 or 3 SNPs (selected with a bias towards SNPs having heterozygous calls) in &lt;500 base pair regions of chromosome 1p (where UPD is not expected) and 1q.<br>
<br>
<software>SNPscan</software> availability and requirements<br>
Project name: <software>SNPscan</software><br>
Project home page: <br>
Operating system(s): Platform independent website<br>
Programming languages: Perl, HTML, R v2.1<br>
Other requirements: Data input is from <software>CNAT</software>v2.0 or v2.1 (Affymetrix).<br>
License: GNU GPL<br>
Any restrictions to use by non-academics: none. The source code is downloadable from the website.<br>
<br>
<br>
List of abbreviations<br>
CEPH, Centre d'?tude du Polymorphisme Humain; CGH, comparative genomic hybridization; <software>CNAT</software>, <software>Copy Number Analysis Tool</software>; DAPI, 4',6-diamidino-2-phenylindole; FISH, fluorescence in situ hybridization; <software>GDAS</software>, <software>GeneChip DNA Analysis Software</software>; LOH, loss of heterozygosity; Mb, megabase; PDF, portable document format; PNG, portable network graphics; PS, PostScript; SNP, single nucleotide polymorphism; TIFF, tagged image file format; UCSC, University of California, Santa Cruz; UPD, uniparental disomy; WIG, wiggle track.<br>
<br>
Authors' contributions<br>
JT designed the SNPscan algorithm, developed the <software>KKISNP</software> tools, and implemented the <software>SNPscan</software> website. He identified novel chromosomal anomalies in the apparently normal 90 individuals discussed in this paper via <software>SNPscan</software> and performed data analysis in R. YY performed the FISH studies and sequencing of genomic DNA, and contributed to writing the manuscript. GHT made intellectual contributions to the design and interpretation of the experiments and contributed to the writing of the manuscript. IR contributed to the data analysis and software development. JP conceived of the study and contributed to data analysis, development of the <software>SNPscan</software> website, and writing the manuscript. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2266781</b><br>
<database>PARPs database</database>: A LIMS systems for protein-protein interaction data mining or laboratory information management system<br>
<br>
<br>
Background<br>
Proteomics aims to identify, characterize and quantify all of the proteins expressed by a given living organism, tissue or cell line[1]. Typically, this approach subjects protein mixtures to proteolytic digestion prior to liquid chromatographic separation and MS/MS analysis of the resulting peptides [2]. Several database search engines, notably <software>Mascot</software>[3], <software>Sequest</software> [4], and <software>X!Tandem</software> [5] assign probable peptide sequences to MS/MS spectra and infer the identity of the proteins present in the sample analyzed. High-throughput proteomic experiments generate large data sets of protein identifications, which can only be properly validated and reported through adequate data processing [1,6,7]. Subsequent integration, sorting and comparison of these datasets pose significant challenges, especially when simultaneously analysing multiple experiments.<br>
One of the most effective approaches to elucidate the biological function of a protein is the identification of its interaction partners. We are only now beginning to appreciate the nature and complexity of networks of interacting proteins. The unravelling of any such network using traditional biochemical approaches remains a significant challenge. Recently, however the application of high-throughput technologies, such as large-scale yeast two-hybrid analysis and mass spectrometry coupled to immuno- or affinity-based capture has made possible the rapid generation of huge protein interaction datasets [8-10]. As consequence, researchers often face the dilemma of how to effectively utilize all available data. Investigators relying solely on a traditional approach to draw conclusions or set research priorities are likely to find themselves outpaced by peers who combine in silico biology and empirical methods. Thus for protein interaction studies, there is clearly a need to develop a systematic and stepwise in silico approach to predict potential interactors. This approach will most likely improve our understanding of how complex biological systems work.<br>
The need to develop a Laboratory Information Management System (LIMS) for researchers in the field of proteomics that would allow to track, archive and aid in a greater understanding of how biological systems work has been recognized. In 2002, Cho and co-workers developed an original LIMS for proteome research (<database>YPRC-PDB</database>)[11], constructed using a commercial database (RDB). They intended to establish <database>YPRC-RDB</database> as a proteome data warehouse. In 2003, Goh and co-workers [12] developed <software>SPINE2</software>, a LIMS for structural proteomics, constructed with <software>MySQL</software> and Perl, and also designed to work as a pipeline to public data resources. In 2004, Garwood and co-workers [13] developed <database>PEDRo</database>, The <database>Proteomic Experimental Data Repository</database>, constructed with a native <fileFormat>XML</fileFormat> database, Xindice with an ambitious <software>Apache</software> Software Foundation basis. The <fileFormat>XML</fileFormat>-based document format has been chosen for communication that the other formats. The native <fileFormat>XML</fileFormat> database has great potential, but many have critical limitations for proteomic research. On the other hand, commercially available LIMSes (Amersham Biosciences and Bio-Rad Laboratories Inc, etc.) have also been developed and released, but they are not exactly suitable for laboratories like ours because the generic solutions are first and foremost prohibitively expensive. These systems are usually re-packagings of applications developed for the pharmaceutical industry for drug discovery and development.<br>
The focus of our laboratory is the study of the action of poly(ADP-ribose) polymerases (PARPs) and their role in the cell. Poly(ADP-ribosylation) is a post-translational protein modification consisting of long chains of poly(ADP-ribose) (pADPr) synthesized by PARPs at the expenses of NAD+. Poly(ADP-ribose) chains are short-lived owing to the activity of the poly(ADP-ribose) glycohydrolase enzyme, which catabolizes the pADPr within minutes of its synthesis[14]. The PARP family may comprise as many as 17 members which share a common catalytic domain responsible for the synthesis of poly(ADP-ribose) [15-17]. The best characterized and abundant member of this family is PARP-1, a 113-kDa nuclear protein comprising a DNA-binding domain made of two zinc fingers that allow PARP-1 to be rapidly activated in response to DNA damage. Poly(ADP-ribose) crucially contributes to chromatin remodelling, DNA damage repair, regulation of transcription, and cell division [18-20]; and PARP-1 is an important actor in many key cellular processes, including BER, transcription, and apoptosis.<br>
We herein describe the architecture and major features of a web-based utility called "<database>PARPs database</database>" (<database>PARPs-DB</database>), which is designed to rationally organize the protein and peptide data generated by the LC-MS/MS analysis of tryptic digest of proteins that co-immunoprecipitate with PARPs proteins into reports meaningful to biological researchers. <database>PARPs-DB</database> offers a LIMS work environment to annotate and study protein-protein interactions and its easy-to-use relational data management system can rapidly supply information pertaining to the biological characteristics of a majority of proteins in a proteomic dataset. The major features of our LIMS are flexibility, compactness, and connectivity to public databases.<br>
Also presented is a list of previously unidentified PARP-1 interactors that were found via affinity co-immunoprecipitation, as well as the analysis of these new PARP-1 interactions generated through <database>PARPs-DB</database>.<br>
Given the advantages provided by an in silico approach that can predict or prioritize potential interactors, it seems reasonable to propose that <database>PARPs-DB</database> will become an essential tool for initially evaluating novel hypotheses and will offer improved rationale for the prioritization of potential interactors. In effect, by facilitating the processing of protein-protein interactions and the selection of the most promising interactors (to be submitted first to empirical measurements) <database>PARPs-DB</database> should lower the cost and shorten the time necessary to discover the most biologically significant interactions between PARPs and other proteins.<br>
We don't have infrastructure to access on-line but the <database>PARPs database</database> source codes and user documentation are available for the scientific community [21]. Tools from the <software>Sashimi</software> project that were used in <database>PARPs-DB</database> are also available [22]. Licensed programs in Sourceforge such as <software>Mascot</software>, <software>Sequest</software> or <software>Oracle</software> were not included in the <database>PARPs</database> repository. Dialects for <software>MySQL</software> and <software>PostgreSQL</software> servers were developed in alpha version and are available upon request. Further information on these scripts can be obtained from the corresponding author (see Additional file 1).<br>
<br>
Construction and Content<br>
Design of the <database>PARPs database</database> software<br>
<database>PARPs Database</database> consists of a core system of services that provide underlying system functionality. Modules, which provide most data handling and analytical support (such as LC-MS/MS data mining), plug into the core. This design means the platform is easily expandable: the architecture allows new analytical modules to be added and integrated without having to modify the core system. <database>PARPs database</database> was designed to be platform-independent and easy to maintain, and is implemented in Solaris Sun Operating system 10 (Sun Microsystems, Santa Clara, CA, USA). It requires access to an <software>Oracle</software> 10 g relational database (Oracle, Redwood Shores, CA, USA), with which it communicates through an abstraction layer that isolates the core system from subtle differences between <software>Oracle</software> database builds. The user interface supports the use of the <software>Apache</software> server[23] for external access via the Hyper Text Transfer Protocol (HTTP). It consists of a set of programs, written in the Practical Extraction and Report Language (Perl) and in PL/SQL, which generates the user interface in Hypertext Transfer Markup Language (HTML), using Cascading Style Sheets (CSS), eXtensible Markup Language (XML) Style sheet Language Transformation (XSLT), and the Scalable Vector Graphics language (SVG). A web browser is used as the user interface of the LIMS, because it is universally available on most client systems. <software>Internet Explorer</software> 6.0 or later, <software>Netscape</software> 7.1 or later, or <software>Firefox</software> version 1.03 or later should be installed in the client PC.<br>
Dialects for <software>PostgreSQL</software> and <software>MySQL</software> servers were implemented, and support for <software>Microsoft SQL</software> is under development.<br>
<br>
Database design<br>
Figure 1 outlines the database schema for the data pertaining to experimental protocols, data analysis and results (the full-scale schema is available on-line as Supplemental Figure 1). The database is defined in the Unified Modeling Language [24], which is a standard notation designed to improve the process of developing large software systems [25]. In this context, it allows us to describe experimental methods, results, and subsequent analyses in an implementation-independent manner. UML schemas (Figure 1) are referred to as class diagrams. They consist of boxes (classes), representing important entity types, connected by various types of lines and arrows signifying the relationship between them.<br>
The sample origin (shown in red) holds basic information, such as the specific biological material used, which subcellular fraction was studied, and the experimental conditions to which the organism was subjected. Sample origin has also two offsprings: 'organism' holds the name of the species/strain used and a list of the relevant gene/mutations carried; and 'tagging process' describes the labelling of the parts of a combined sample for differential expression studies, such as isotope-coded affinity tag (ICAT) mass spectrometry[26].<br>
The sample (shown in purple) simply holds an identification code (laboratory-specific), the production date, and the name of the responsible person.<br>
The next classes represent sample's processing step before moving on to run a mass spectrometry experiment. For example, running a two-dimensional gel with sample, then putting a spot from that gel through two-dimensional liquid chromatography. The class HPLC describes the equipment's origin, its dimensions, the stationary phase, the pore size in the beads, the total injection volume, and the flow rate. The class gel capture the description of the gel, the image analysis software used, and whatever images of the gel are available, referred to by URIs (Universal Resource Identifiers). There are also several parameters describing the gel itself (for example, percent acrylamide in the mix, the solubilisation buffer and stain used, a measure of the total protein on the gel, the in-gel digest).<br>
Mass spectrometry (shown in orange). Details about the makeup of the mass spectrometry machine is stored in seven classes. Source is an abstract class that will, in practice, be either MALDI or Electrospray, each of which has its own set of fields (voltages of various kinds; tip, solvent, and interface details for electrospray; laser wavelength and matrix type for MALDI runs). Instrument represents the mass analysing and fragmentation section of the mass spectrometer (for example, Quadrupole, Ion Trap, or Collision Cell, each with its own parameters).<br>
MS results analysis (shown in green). To perform a protein identification, a particular Peak list would be submitted to an identification tool, such as <software>Mascot</software>, <software>Sequest</software> and <software>X!Tandem</software>. The classes 'DBSearch Parameters' capture information about who processed data, when they did it, what program they used, what database was used, what errors were taken into account when searching, what potential modifications were allowed on proteins from the sample that generated the peak list.<br>
The protein tables (shown in blue) store identifiers (accession numbers) that point to external web-based information sources. Short text annotations such as <database>Gene Ontology</database> [27] descriptions, descriptions of functional or structural regions within the protein sequence, and information about associated diseases and biological pathways are also stored when available. While the identifiers serve as links to external databases and web pages, the annotations stored within <database>PARPs-DB</database> are human readable and easily searchable. <database>PARPs-DB</database> also supports input of local protein sequences and annotations, as well as pointers to local databases. A sequence or annotation marked as "defunct" will not automatically be deleted from the database, which means old <fileFormat>FASTA</fileFormat> files can be reanalysed with new annotations even if their records have been deleted or replaced by subsequent information in the primary source.<br>
The database was designed to contain a minimal amount of information but still sufficient data to allow effective Structured Query Language (SQL) queries. These queries enable ready access to any information stored in the database as well as in the <fileFormat>XML</fileFormat> files generated by the data analysis server. With the tables and <fileFormat>XML</fileFormat> files serving as the primary data storage objects, the relational dataset is relatively easy to build, maintain and query.<br>
<br>
LC-MS/MS data analytical module<br>
A key design element of <database>PARPs database</database> is the ability to generate analytic modules that plug into and use the core of <database>PARPs</database> system. Three pivotal LC-MS/MS tools integrated in <database>PARPs-DB</database> are the peptide-spectrum matching programs: <software>Sequest</software>, <software>X!Tandem</software> and <software>Mascot</software>. We have also included <software>PeptideProphet</software> to validate peptides assigned to MS/MS spectra [28] and <software>ProteinProphet</software> to infer the proteins[29] present in the sample from the list of observed peptides. These open-source tools are components of the <software>Trans Proteomic Pipeline</software> (<software>TPP</software>) from Seattle Institute for Systems Biology (ISB) [30]. To access MS/MS data, <fileFormat>RAW</fileFormat> files were converted to the open file format (<fileFormat>mzXML</fileFormat> or <fileFormat>mzData</fileFormat>) using <package>Readw</package>.exe from <software>Sashimi</software> for LTQ mass spectrometer for example or our own conversion software. <software>Sashimi</software> [22]is a project initiated at the ISB that aims at providing the scientific community with free and open-source software tools for the downstream analysis of mass spectrometric data. <software>Sashimi</software> is focused on the bioinformatics standards necessary to the set up of a generic proteomic pipeline using common output formats at each processing step. We have also integrated three executable: <package>Sequest2XML</package>, <package>Mascot2XML</package> and <package>Tandem2XML</package>, also from <software>Sashimi</software>, to convert search engine outputs (DAT, OUT and XML files) to pepXML[31].<br>
<br>
Links to public databases<br>
The underlying protein knowledge base used by <database>PARPs-DB</database> was extracted from multiple online resources, based on cross-references. Five human gene and protein data sources were integrated within <database>PARPs-DB</database> : protein databases maintained by <database>IPI</database> [32] and <database>UniProt</database> [33], and three NCBI databases: <database>Entrez Gene</database> [34], <database>RefSeq</database> [35], and <database>GenPept</database>. Three protein-protein interactions databases were also included in <database>PARPs-DB</database>'s knowledge base : the <database>Biomolecular Interaction Network Database</database> (<database>BIND</database>)[36], the <database>Database of Interacting Proteins</database> (<database>DIP</database>)[37] and <database>Human Protein Reference Database</database> (<database>HPRD</database>) [38]. For each identified protein stored in <database>PARPs-DB</database>, the data analysis server gathers the protein's function, sequence and post-translational modifications from the above sources and presents the extracted data along with the identified protein. Different strategies have been used to update our databases. For protein databases such as <database>Uniprot</database> and <database>IPI</database>, we use Perl scripts to download <fileFormat>Fasta</fileFormat> files from the <database>Uniprot</database> and EBI server. A report of the new release updates is produced. For protein-protein interaction databases such as <database>HPRD</database> and <database>BIND</database>, monthly updates are also performed through Perl scripts.<br>
<br>
Protein-Protein interaction viewer<br>
Finally, in order to visualize protein-protein interaction networks, we have developed a protein-protein interaction viewer, in Java language (Java JDK 1.4.2_05 and Netbeans 3.6). This viewer uses three libraries: Xerces Java Parser 2.6.2, Piccolo Java 1.1, and JDOM 1.0 this last library being used to manipulate and parse the <fileFormat>XML</fileFormat> files. The central organization of the protein-protein interaction viewer is a network graph with molecular species represented as nodes and intermolecular interactions represented as links, that is, edges between nodes. This application provides basic functionality for integrating data on the graph, a visual representation of the graph and integrated data. Data are integrated with the graph model using attributes. Graphical browsers allow the user to examine all attributes on the currently selected nodes and edges.<br>
One of the most fundamental tools for interpreting molecular interaction data is visualization of nodes and edge as two dimensional network. It utilizes a relaxation layout algorithm which attempts to prevent overlapping of nodes. This viewer is small, stable, multi-platform and simple to use. It can function as a stand-alone applet or be integrated into a web application.<br>
<br>
PARP-1 co-immunoprecipitation<br>
Cell culture<br>
Human cervical carcinoma HeLa cells obtained from ATCC (Manassas, VA, USA) were cultured in Dulbecco's modified Eagle's medium (DMEM) supplemented with 10% fetal bovine serum, 2 mM L-glutamine, 100 U/ml penicillin and 100 ?g/ml streptomycin in an humidified atmosphere of 5% CO2 at 37?C. All the above-mentioned reagents were purchased from Invitrogen (Burlington, ON, Canada).<br>
<br>
Immunoprecipitation of endogeneous PARP-1<br>
Cells grown in 150 mm culture dishes were washed with ice-cold phosphate-buffered saline (PBS). 400 ?l/dish of ice-cold lysis buffer (175 mM KPO4, pH 8.0, 150 mM NaCl, 1% NP-40, 1 mM DTT, 0.5 mM PMSF and Complete? protease-inhibitor cocktail (according to Roche diagnostics instructions)) was added to the cells. Cells were harvested using a cell scraper. Lysed cells coming from three dishes were pooled then gently mixed by inversion for 1 hour at 4?C and centrifuged 10 minutes at 6000 g at 4?C to remove insoluble cellular debris. The cellular extract was mixed with 180 ?l of magnetic beads coupled to protein G (Dynal, Invitrogen) and 8 ?l of monoclonal antibody to PARP-1(F1-23)or 8 ?l of normal mouse IgG as control and incubated during 2 hours at 4?C with rotation. The beads had been previously blocked during 1 hour with 1% BSA and washed with lysis buffer. At the end of the incubation period, the beads were washed 3 times with lysis buffer. 180 ?l of 2? Laemmli SDS sample buffer containing 5% (v/v) ?-mercaptoethanol was added to the beads and they were placed in a boiling bath for 5 minutes to elute the immunoprecipitated proteins.<br>
<br>
Protein separation and digestion<br>
Immunoprecipitated proteins were separated by SDS 8% PAGE. The gel was fixed for 30 min with 10% (v/v) methanol and 7% (v/v) acetic acid solution, then stained with SYPRO Ruby fluorescent protein stain (Bio-Rad, Hercules, CA, USA) according to the manufacturer's instruction. The entire protein profile of the immunoprecipitated proteins was sliced from the gel into 50 bands using a gel excision Lanepicker? (The Gel Company) and placed into a 96-well plate. In-gel protein digests were performed on a MassPrep? liquid handling station (Waters) using sequencing-grade modified trypsin (Promega) according to the manufacturer's instructions. Peptide extracts were evaporated to dryness using a SpeedVac? and resuspended in 10 ?l of 0.1% formic acid in water.<br>
<br>
LC-MS/MS<br>
Final extracts were analysed by LC-MS/MS using an LCQ-DECA XP mass spectrometer equipped with a nanospray ESI (electrospray ionization) source and a Surveyor autosampler and HPLC system (Thermo Electron). A 5 ?l volume of extract was first focused on a Peptide CapTrap? (Michrom Bioresources) and then loaded on a Biobasic C18 PicoFRIT?capillary column (PFC7515-BI-10; New Objective). Elution of peptides was performed using a linear acetonitrile gradient (0?60%) over 20 min at a flow rate of approximately 200 nl/min (buffer A: 0.1% formic acid in water; buffer B: 0.1% formic acid in acetonitrile). MS, including collision-induced dissociation, was performed in an automated fashion using the dynamic exclusion option.<br>
<br>
Protein identification<br>
Peptides were assigned MS/MS spectra by searching using <software>Sequest</software> (version 2.0 SR2), <software>Mascot</software> (version 2.1) and <software>X!Tandem</software> (2006.04.01.2) and the assignments were also validated with <software>Scaffold</software> software (Proteome Software Inc.; version Scaffold-01_03_02). MS/MS spectra were searched against the <database>IPI</database> human protein database (version 3.01)[32] to which the sequences of protein constructs, proteins of interest, and common contaminants were added. Searches were performed specifying complete (fixed) carbamidomethylation modification of cysteine (+57 Da) and oxidation of methionine (+16 Da) residues. The digestion enzyme parameter was set to trypsin. The proteins identified in this paper were obtained with a <software>Scaffold</software> probability cut-off of 80%.<br>
<br>
Western blots<br>
Total protein extracts and proteins eluted from the immunoprecipitations were separated by 8% SDS-PAGE and then transferred onto a 0.45 ?m pore-size PVDF membrane (Millipore, Bedford, MA, USA). After incubating 1 hour with the blocking solution (PBS with 0.1% (v/v) Tween-20 (PBS-T) containing 5% non-fat milk), the membrane was probed with primary antibodies to PARP1 (C2-10, mouse monoclonal 1:5000) or RFC1 (Replication factor C, 140 kDa subunit, rabbit polyclonal antibody 1:2500) (Bethyl Laboratories, Montgomery, TX, USA) overnight at room temperature with shaking. After washing with PBS-T, species-specific horseradish peroxidase-conjugated secondary antibody was added for 1 hour at room temperature. The signals were finally detected with the Western Lightning? Chemiluminescence reagent plus kit (Perkin Elmer, Boston, MA, USA).<br>
<br>
<br>
<br>
Utility and Discussion<br>
Protein interaction research workflow<br>
The workflow of the PARPs protein interaction research is illustrated in Figure 2. In our LIMS, the data processing is divided into sections corresponding to the four main steps: sample preparation, MS Data acquisition, protein identification, and <database>PARPs-DB</database>.<br>
The sample preparation (Figure 2A) section allows laboratories to track and organize biological experiments and view the workflow of those experiments.<br>
For the purpose of MS data acquisition (Figure 2B), different types of mass spectrometers, using different methods for ionization and mass determination, may be used. As the instruments from diverse suppliers use different formats to store instrument parameters and spectral data, <database>PARPs-DB</database> uses parsers to convert the data from the different mass spectrometers (LTQ and QSTAR) into mzXML/mzData. <database>PARPs-DB</database> is very flexible. Additional mass spectrometers and converters to XML files can be easily included. mzXML and mzData are designed to encompass all of the information required by the peptide-spectrum matching software such as <software>Sequest</software> and <software>Mascot</software>. Moreover these data representations, developed respectively by Seattle Institute for Systems Biology (ISB) and the European Bioinformatics Institute (EBI), provide an OS and architecture-independent standardized file format and remove the burden of having to support multiple native formats. The <software>Sashimi</software> project currently provides converters from native binary files to mzXML (example <package>ReAdW</package>, convert the RAW files generated by <software>Xcalibur</software>). Unfortunately, there is no program available to convert proprietary binary format to mzData. Therefore, the easiest way to publish a peak list data in mzData today is to convert mzXML into mzData using any XML parser.<br>
By converting all native binary data to mzXML/mzData and using these standards at the start of our analysis pipeline, the downstream software tools, specifically the database search module and raw spectral viewer, can be used in a uniform manner regardless of the instrument used to acquire the MS/MS spectra.<br>
In order to identify proteins from the tandem mass (MS/MS) spectra, the protein identification section (Figure 2C) is used to submit the spectra to three search engines, namely <software>Sequest</software>, <software>Mascot</software> and <software>X!Tandem</software>. <database>PARPs-DB</database> MS/MS analytical module stores, shares, analyses, mines and publishes tandem MS data. This module supports pepXML, which stores the results of peptide sequence assignments and subsequent peptide-level analyses in a XML files. After search results have been written or converted to pepXML, they can uniformly be subjected to peptide-level applications and viewed without regard to the algorithm used to assign peptides to MS/MS spectra. Users can examine individual LC-MS/MS runs and groups of runs using complex customizable analytical filters for peptides and proteins on the various search engine specific scores (XCorr for <software>Sequest</software>, log(e) for <software>X!Tandem</software>). These filters can be saved for later use. Finally, protein identifications are stored in protXML. The multiple possibility discordant sequence identification presented in each run is encompassed by protein <software>ProteinProphet</software> which all peptide evidence is combined. This data in XML format (developed by <software>Sashimi</software>) stores protein identifications inferred from input lists of peptides and their subsequent protein-level analyses. After protein identifications are converted to protXML, protein-level analyses such as protein quantification can proceed and results viewed without regard to the method used to infer protein identification. With the help of this standard, we have used a set of open source tools, <software>PeptideProphet</software> and <software>ProteinProphet</software>, which provide a standardized method of validating MS/MS data. For example, accurate probabilities provided by <software>PeptideProphet</software> and <software>ProteinProphet</software> serve as guides for the interpretation of peptide and protein identifications, respectively, and enable the prediction of false positive error rates that can be used as objective criteria for the comparison of data sets generated by different researchers. This module interacts with the protein annotation (described in the next section) module to display information rich annotations for putative protein identifications.<br>
This last section represents the <database>PARPs database</database> (Figure 2D). Following the execution of the data processing methods described above, the results are loaded automatically into <database>PARPs-DB</database> for viewing. The database system is interconnected with the protein annotation module (Figure 2D). This module manages protein sequence annotations to help investigators cope with any newly updated or revised information about proteins and their properties. Sequence annotations are automatically updated. However, updates to the system are stored incrementally so that any previous version of a database annotation can be retrieved at any time. Protein annotations interact closely with the protein identification section to allow users to view up-to-date descriptions of protein sequence that have been identified.<br>
A sequence or annotation marked as "defunct" will not automatically be deleted from the database, which means old <fileFormat>FASTA</fileFormat> files can be reanalyzed with new annotations even if their records have been deleted or replaced by subsequent information in the primary source. Specific databases such as <database>UniProt</database>, <database>IPI</database>, <database>RefSeq</database>, <database>BIND</database>, <database>HPRD</database>, <database>Gene Ontology</database> are downloaded in the <database>PARPs-DB</database>.<br>
<br>
Accessing and navigating experiments in <database>PARPs database</database><br>
To facilitate data analysis, a graphical user interface (GUI) was developed. The GUI guides the user through all steps of the experiment to enter information such as immunoprecipitation methods, gel images, mass spectra, search engine results, etc. (Figure 3), which ensure a complete documentation of the experimental setting. After all necessary data have been stored in the system, the user can select data sets for visualization.<br>
To access the LIMS server using a web-based client, the user must first login with an authorized username and a given password. <database>PARPs-DB</database> users are authenticated against a Lightweight Directory Access Protocol (LDAP) provider such as institution's name server. Experimental data and other materials are stored in projects and their sub-folders, much like a file system. Each project has one or more groups of users associated with it, and each group can have a distinct set of permissions (e.g., read only, read and write) to each of the project's folder. When users log in, the authorization system determines what data they have permission to view, edit, and/or delete and provides access accordingly.<br>
Inside the <database>PARPs-DB</database>, there are three main sections, "sample origin," "mass spectrometry" and "sample results" corresponding to different steps of a proteomic experiment. These sections allow users to store experimental parameters, results and annotations. Each section has a distinct set of permissions. For example, a molecular biologist cannot access the mass spectrometry section, and conversely, mass spectrometer users cannot access the molecular biology section. In each section, we have developed tools to help the user reduce the time needed to analyse data.<br>
First, the sample origin section enables the user to enter experimental parameters by selecting a number of options. Experimental information includes cell type and cellular conditions, method of gene transfer (when applicable) and gene sequence, and details of the immunoprecipitation method such as lysis buffer composition, antibodies, cell lysis. The user can print experimental details entered in the database (Figure 3A). For example, an image of a stained gel showing proteins immunoprecipitated in the described experiment may be loaded into the database.<br>
The second section of <database>PARPs-DB</database> is the mass spectrometry section. In this section, the user may define the parameters of a mass spectrometry experiment including: the plate number, the spot position, method files (Figure 3B) and parameters for search engines. A tabular file is generated to upload the list of samples into the mass spectrometer software. At the end of MS/MS analysis, the raw data is transformed in mzXML and mzData automatically in the background. Unfortunately, all converters run on windows environment because some Windows specific libraries are necessary. After each run, the binary data files are transformed locally in XML files on windows computer. After the conversion, each XML files are transferred with Secure File Transfert Protocol (SFTP) to UNIX server.<br>
The user accesses database searching through another section of the user interface in order to set specific search engine parameters such as the database to be searched and amino acid modifications. The data pipeline will submit the mzXML or mzData to the search algorithms and manage the specification of search parameters and <fileFormat>FASTA</fileFormat> files. Once analysed, the system offers graphical and tabular views of the experimental steps and their input and output. Users can monitor the progress of their searches via the web interface.<br>
The last section of <database>PARPs-DB</database> is the sample results section. Access to LC-MS/MS results is available in this section, which shows protein and peptide identifications in the list view of the <database>PARPs database</database>. The data may be sorted according to certain experimental protocols (e.g. digestion) or according to the identification probabilities (Figure 3C). Display columns include the <database>UniProt</database>[33] or <database>IPI</database>[32] or <database>RefSeq</database> [35] annotation, the number of uniquely identified peptides per protein, and the total number of identified peptides per protein. MS/MS search results can be evaluated using this module, which allows proteins and peptides to be sorted and filtered by various criteria. Each identified protein is linked to the protein annotation module (see below), through which it is automatically linked following parsing of the <fileFormat>FASTA</fileFormat> file, allowing access to a variety of up-to-date external sources (Figure 3D). The accession numbers of proteins identified from the <software>Sequest</software>, <software>X!Tandem</software>, or <software>Mascot</software> searches are matched with those from <database>IPI</database>, and specific information regarding the protein of interest is automatically retrieved and displayed within the database window. Additional information from the software-assisted identification of the protein is displayed in a portal view, including identified peptides. The purpose of this feature of the <database>PARPs database</database> is to automatically connect protein identifications to their function and other relevant biological information extracted from external databases. A statistics module within <database>PARPs-DB</database> provides basic information about each experiment in the form of charts (e.g. <database>gene ontology</database> annotations, the number of peptides per protein, proteins identified with a certain <software>ProteinProphet</software> probability, etc.). In addition, the database allows for the comparison of data from different experiments at protein and peptide levels. Users are able to query the database, add notes to specific identifications, and select and export lists of interesting proteins including annotations.<br>
Different tools are accessible throughout <database>PARPs database</database> navigation. These include <software>BLAST</software>, <software>CLUSTALW</software> and our protein-protein interaction viewer, a graphical tool that is linked to <database>PARPs-DB</database>. The viewer displays protein-protein interactions from the <database>PARPs-DB</database> (Figure 3E). The protein interaction network is shown as nodes (proteins) and edges (interactions). The interaction network can also be displayed with the annotations for the proteins in the nodes. Each node is linked to the protein annotation module. We displayed the confidence of each external protein-protein interaction using the thickness of the edge (default value 2). Redundant interactions independent reports in each external data source were assigned confidence values of 3. In addition, the colour (red: <database>Bind</database> database, blue: <database>HPRD</database> database, or green: <database>PARPs database</database>) of the edges can be selected to indicate the respective data sources.<br>
The user can scan all the deposited internal (our protein-protein interaction assays) and external protein-protein interactions (from publicly available data sources: <database>INTACT</database>, <database>BIND</database>, <database>HPRD</database>, <database>String</database>) in the database. Information about protein-protein interactions beyond the target protein is shown in the interaction network to visually characterize the protein network. Proteins of interest can be searched by either accession number or keywords. When users input the accession number of a protein, the protein interaction network is shown as nodes (proteins) and edges (interactions). The interaction network can also be displayed with the annotations for the proteins in the nodes. Each node is linked to the protein annotation. Data and results can be exported to other formats including <fileFormat>PSI-MI</fileFormat>, <fileFormat>Excel</fileFormat> and <fileFormat>DTA</fileFormat> (<software>Sequest</software> files) for additional analysis using other tools. This method was created to exchange data easily between different laboratories.<br>
<br>
Using proteomics standards in <database>PARPs database</database><br>
A major obstacle to uniform proteomic analysis has been the great heterogeneity of data formats at three distinct levels: different mass spectrometers output their raw spectral data in different proprietary formats, methods that assign peptides to MS/MS spectra output their results in a variety of formats, and different methods to infer protein identifications from lists of peptides output their results in different formats. The proteomics community has recognized this problem and is tackling it through the formation of working groups (Proteomics Standard Initiative; Institute for Systems biology) concerned with the development of standards for the capture and sharing of proteomics data. The <database>PARPs database</database> was developed in agreement with the HUPO-PSI (Human Proteome Organization-Proteomics Standard Initiative), which includes PSI-MI (Molecular Interactions), MS (Mass spectrometry) and GPS (General Proteomics Standards). The GPS development of standard ways to represent proteomics data and an agreed minimum required level of detail are both urgently required to facilitate the analysis, dissemination and exchange of proteomics data. Minimal Information about Proteomic Experiment (MIAPE) is a proposed standard format for proteomics covering 2-DE and MS. The <database>PARPs database</database> contains classes derived from <database>PEDRo</database>[13]. As mentioned earlier, the latest proteomic standards such as mzXML, mzData and PSI-MI have been incorporated into our pipeline. We expect the new format such as mzXML to facilitate the exchange and publication of MS-based proteomics data and that our <database>PARPs database</database> will provide a consistent platform for the development of new analytical tools.<br>
We have developed <database>PARPs-DB</database> as an in-house, flexible Laboratory Information Management System (LIMS) which integrates all aspects of the study of protein-protein interactions by mass spectrometry-based proteomics, from sample processing information to protein interactions visualization. <database>PARPs-DB</database> allows easy network access to public databases, manages the output of different search engines (<software>MASCOT</software>, <software>X!Tandem</software> and <software>Sequest</software>) and supports HUPO's Proteomic Standard Initiative formats. Although several LIMS have been made freely available during last few years, none of these existed when work on the <database>PARPs database</database> was initiated in 2002. Advantages of <database>PARPs-DB</database> over these free LIMS includes: integration of all data related to the mass spectrometry-based study of protein-protein interactions; multiple search engine support; and user-friendliness.<br>
It is indicative of the poor availability of appropriate commercial systems that development of in-house LIMS such as <database>PARPs-DB</database> started at around the same time in different laboratories across the world, in order to fill the urgent need to automate proteomic data storage and analysis. Commercial LIMS now available remain however prohibitively expensive for small MS facilities and more rigid than an in-house system. These systems often include rigorously defined user roles and access privileges, as well as extensive auditing of data file changes. Although essential to pharmaceutical companies filing drug applications which must comply to regulatory standards (e.g. 21 CFR part 11), these features can hamper academic research because files can not easily be modified, maintained, and updated. Additionally, integration of the output of a new mass spectrometer can be more difficult and costly than with an in-house, flexible system. Finally, available LIMS from mass spectrometer vendors present many limitations: there is notably no integration of the data generated by other vendors' instruments, nor of the data not directly to the mass spectrometer; for instance, they do not handle pre-MS sample processing information and protein interaction data.<br>
<br>
Constructing protein-protein interaction network for PARP-1<br>
To illustrate the use of <database>PARPs-DB</database> for discovery of protein-protein interactions, we describe here an experiment for PARP-1 co-immunoprecipitation with interacting proteins. This co-immunoprecipitation is part of experiments aiming at identifying PARP-1 interactors which will be published elsewhere (Ethier et al., manuscript in preparation). Construction of this interaction network involved three bioinformatics steps and the predicted interactions were then verified using standard biochemical techniques.<br>
Step1. Identification of PARP-1 interacting proteins from published experimental studies<br>
The first step in the generation of PARP-1 interaction model is an extensive search of the literature in order to collect published experimental data on PARP-1 interactions. The keyword used in the <database>PARPs-DB</database> search for literature was PARP-1. However, a functional network is not only limited to physical protein-protein interactions but also includes genetic and biochemical interactions (Figure 4).<br>
<br>
Step2. Establishment of the PARP-1 interaction network by analysis of public databases<br>
The interacting molecules are summarized in Figure 4. Four different comprehensive large-scale yeast protein interaction databases were included in <database>PARPs-DB</database>: <database>BIND</database>, <database>HPRD</database>, <database>INTACT</database>, and <database>STRING</database>. Search within these databases resulted in 52 PARP-1 interactors. <database>BIND</database>, <database>INTACT</database>, <database>HPRD</database> and <database>STRING</database> all have extensive collections of human protein-protein interactions, although the former three databases are primarily used to extract, but not predict, protein-protein interaction data from literature. The database <database>STRING</database> ('<database>Search Tool for the Retrieval of Interacting Genes/Proteins</database>') aims to collect, predict and unify most types of protein-protein associations, including direct and indirect associations. In order to cover organisms not yet addressed experimentally, <database>STRING</database> runs a set of prediction algorithms, and transfers known interactions from model organisms to other species based on predicted orthology of the respective proteins.<br>
One important point in the analysis of data from public protein-protein interaction databases is the quality of the results. Indeed, Deng et al compared the data from all the large-scale yeast interactions screens present in the public protein interaction databases. They developed a maximum likelihood estimation method to access the reliability of the interaction data, and found that the Uetzdata were more reliable than the Ito[8] data, and that the Gavin data were more reliable than the Ho data. Therefore, a cautious use of public databases is indicated. In addition, they suggested that the MS-based analysis of protein complexes performed better in function predictions than the two-hybrid data, thus validating the theory that each component of a complex can be assigned a function based on that of the whole complex. It is clear that yeast two-hybrid and MS-based techniques have both independently made significant impacts on our understanding of the interactome. However, each technique has specific drawbacks that limit the information provided if used alone.<br>
<br>
Step3. Selection of Protein-Protein Interaction by <database>Gene Ontology Accession</database><br>
The next step was to group protein-protein interactions by molecular function via the <database>Gene Ontology</database> [27] controlled vocabulary included in <database>PARPs-DB</database>. Because intracellular events may be compartmentalized to unique intracellular location, to provide additional specificity for target selection we also included a spatial component to further refine the construction of the model. Therefore, we further prioritized our target selection by using two keywords (DNA replication and nucleus). We chose these two keywords to illustrate our approach but PARP-1 is involved in many other cellular processes. After filtering according to function, six out of the 52 initial proteins interactors remained in the networks: PCNA, topoisomerase I and II, DNA ligase I, DNA Pol ? and ?. An extensive literature search about these proteins in the context of replication and data mining helped to construct a human PARP-1-protein interaction map in <database>PARPs-DB</database>. We found 13 proteins and one complex that interact with PARP-1 in the complex machinery replication. Of the 13 interacting proteins, PCNA, topoisomerase I, DNA ligase I, DNA Pol ? and ? have been previously reported to interact with PARP-1. Topo2, MSH2, RFC1, RFC2, RFC3, RFC4, RFC5 have been reported to be related to DNA replication. This analysis raises the possibility that PARP-1 may regulate these complexes as a whole rather than regulate one or more the individual components. Of all the potential candidates, we propose that PARP-1 could interact with other proteins in RFC (RFC1 to RFC5) in the context of machinery replication (Figure 5).<br>
<br>
Demonstration of biochemical interactions between PARP-1 and RFC1<br>
Verifying the interactions from molecules identified in silico is vital to provide a confident interaction network useful for further study. With the exception of PCNA, which has been characterized, the prioritized candidates were next tested experimentally to confirm the predicted protein-protein interactions. This was carried out using co-immunoprecipitation of PARP-1 followed by mass spectrometry to identify the interacting proteins.<br>
<software>Scaffold</software> (version Scaffold-01_03_02) was used to group and validate MS/MS based peptide and protein identifications from <software>Sequest</software>, <software>Mascot</software> and <software>X!Tandem</software>. This software is based on the PeptideProphet algorithm which provides an empirical statistical model which estimates the accuracy of peptide identifications made by database search engines. For each tandem mass spectrum, <software>PeptideProphet</software> determines the probability that the spectrum is correctly assigned to a peptide. <software>Scaffold</software> system was used to group the assigned peptides according to corresponding protein and to compute probability of a correct protein assignment for each protein. Peptide identifications with <software>Scaffold</software> software were accepted if they could be established at greater than 80.0% probability as specified by the PeptideProphet algorithm. For the co-immunoprecipitation eluate, protein identifications were accepted if they could be established at greater than 95% probability and contained at least 2 identified peptides. Table 1 lists all the RFC proteins identified with a minimum probability of 95%. It should be noted that the maximal protein probability is limited to 95% in <software>Scaffold</software>. This was set to take into account the light risk that a peptide spectrum match (PSM) is incorrect even if the theoretical and experimental spectra are very similar (e.g: correct peptide absent from the search database).<br>
The co-immunoprecipitation assay with RFC1 antibody performed in HeLa cells suggests that Replication Factor C subunit (RFC1, 2, 3, 4, 5) (Table 1 and Table 2) formed a complex with endogenous PARP-1. The protein sequence coverage of PARP-1 in this study is 60%.<br>
RFC-2, 3, 4 and 5 were each identified with a minimum probability of 95% and by more than 4 peptides with a minimum probability of 95%. The confidence of the identification of RFC-2, 3, 4 and 5 is thus very high and, moreover, we have identified this RFC complex in several co-immunoprecipitates. The case of RFC-1 is different as it was identified by only two peptides of probabilities of 95% and identified in only one co-immunoprecipitate. For this reason, this potential interaction was confirmed by western blot analysis of complexes immunopurified with mouse monoclonal F1-23 antibody (Figure 6). RFC-1 was detected by western blot analysis with rabbit polyclonal RFC1 antibody. As expected, RFC-1 was pulled-down by PARP-1. Although substantially more work is required to determine whether RFC complex interacts directly with PARP-1 or via other proteins such as PCNA, DNA polymerase or through interaction with poly(ADP-ribose), the findings reported here suggest that the <database>PARPs-DB</database> may be useful for finding interacting proteins.<br>
<br>
<br>
<br>
Conclusion<br>
The work presented here has demonstrated how bioinformatics can supplement conventional biological investigation. The <database>PARPs-DB</database> enables storage, annotation and representation of data generated by molecular biology. Moreover this system has identified a previously unknown protein interaction of PARP-1. The <database>PARPs database</database> allows the effective description of proteomics experiments and analysis of protein-protein interactions.<br>
Because the <database>PARPs database</database> was developed to facilitate data sharing and exchange, it includes the latest standard format to allow sharing of experimental design and results with the scientific community. We have incorporated tools allowing the extraction of protein-protein interactions from the <database>HPRD</database>, <database>DIP</database> and <database>BIND</database> public databases, literature and other sources of information. Reports for peptide and protein analyses are output. These provide comparison reports from multiple or concatenated experiments, thereby significantly increasing the confidence in peptide and protein identifications.<br>
The biochemical data between PARP-1 and RFC complex confirmed the interaction reported earlier. However, substantially more work is required to delineate the specificity and the structural interaction with respect to the regulation of their cellular function between PARP-1 and RFC complex. It is anticipated that the building of such an integrated platform, which can be constantly up-graded, could provide a predictive understanding of a novel gene's function in its biological context. A key design element of <database>PARPs database</database> is the ability to add tools or module that plug into and use the core systems. The <database>PARPs-DB</database> will be expanded as needed in order to make the analyses more efficient.<br>
<br>
Availability and requirements<br>
Project name: <database>PARPs-DB</database><br>
Project home page: <br>
Operating system(s) : Unix, Linux, <software>Oracle</software> and <software>MySQL</software>;<br>
Programming Language: Perl, JAVA, SQL;<br>
Licence: GNU GPL;<br>
<database>PARPs-DB</database> is distributed under the GNU GPL licence and available from the website <br>
<br>
Abbreviations<br>
SQL: Standard Query Language; NCBI: National Center for Biotechnology Information; MS: Mass spectrometry; PARP: Poly(ADP-Ribose) Polymerase; ISB: Institute for Systems Biology; EBI: European Bioinformatics Institute; LDAP: Lightweight Directory Access Protocol; <database>IPI</database>: <database>International Protein Index</database>; <database>HPRD</database>: <database>Human Protein Reference Database</database>; <database>BIND</database>: <database>Biology Interaction Database</database>.<br>
<br>
Authors' contributions<br>
AD implemented, designed the database for the mass spectrometry. MR, CE and APC performed molecular biology approach and helped revise the manuscript. GGP participated and supervised the project. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2566583</b><br>
IgTM: An algorithm to predict transmembrane domains and topology in proteins<br>
<br>
<br>
Background<br>
Membrane proteins are involved in a variety of important biological functions [1,2] where they play the role of receptors or transporters. The number of transmembrane segments of a protein and some characteristics such as loop lengths can identify features of the proteins, as well as their role [3]. Therefore, it is very important to predict the location of transmembrane domains along the sequence, since these are the basic structural building blocks defining the protein topology. Several works have dealt with this prediction task from different approaches, mainly using Hidden Markov Models (HMM) [4-6], neural networks [7,8] or statistical analysis [9]. A rich literature is available on proteins prediction. For reviews on different methods for predicting transmembrane domains in proteins, we refer the reader to [10-12].<br>
This work addresses the problem of protein transmembrane domains prediction by making use of a Grammatical Inference (GI) based approach. GI is a particular case of Inductive Inference, an iterative process that takes into account a set of facts and tries to obtain a model consistent with the available data. In GI the model resulting from the induction process is a formal grammar (that generates a formal language) inferred from a set of sample strings, composed by a set M+ of strings belonging to a target formal language and, in some cases, another set M- of strings that do not belong to the language. The results of the inference process gives as the result a language (hypothesis) that, in essence, models all the common features of the strings. This grammatical approach is suitable for the task due to the sequential nature of the information. Some works apply formal languages methods to molecular biology [13]. Figure 1 depicts a general GI scheme. Several classifications of the GI algorithms can be made, for instance: when both sets are non-empty we remalgo[cont2]Algorithm refer to complete presentation algorithms; positive presentation algorithms are those that use an empty M- set; taking into account these algebraic properties of the obtained languages, it is possible to distinguish between characterisable and non-characterisable algorithms. It is difficult to identify what information is suitable to be considered into M-, therefore we will take into account only positive presentation in our approach. For more information, we refer the reader to [14-16].<br>
Usually, the model used in GI is a finite state abstract machine commonly named finite automata. HMMs are closely related to finite automata, and therefore our approach is also related to several works that succesfully tackle this task [4-6]. Nevertheless, it is to note that the topology of a HMM, number of states and their connection, is a priori fixed by an expert that takes profit from known information. Once the topology is fixed, the available data is used to set the probability of each transition of the HMM. As stated above, the input of a GI algorithm is a set of sequences, therefore no aid from an expert is needed, because both the topology of the automaton and the probability between states is automatically stablished by the algorithm.<br>
Generally speaking, HMMs provide a good solution when the topology of the HMM can be fairly set. In that case, the sequences provided are used just to set the transition probabilities among states. A GI approach tries to extract more information from the sequences and provides good prediction tools using only sequential information. The most important drawback of GI is the lack of enough data to infer proper models.<br>
GI has been used previously in various bioinformatics related tasks, such as gene-finding or prediction of coiled coil domains [17]. The good performance of those works leds us to apply GI algorithms to the prediction of other domains in proteins, such as transmembrane segments.<br>
Our work takes into account a set of protein sequences with known evidence of transmenbrane domains. Firstly, these sequences are processed in order to distinguish among inner, outer and transmembrane residues. This labelling allows to obtain an Even Linear structure (that considers a relationship among the symbols in a sequence, such that the first and the last symbols are related, the second and the last but one are also related and so on). It is possible to model this structure by using an Even Linear Language (ELL) that can be learned using GI techniques. The obtained language is then used to build a probabilistic transducer (an abstract machine that processes an input sequence and obtains another output sequence or transduction with an occurrence probability). The resulting transducer allows to process any unknown protein sequence to obtain a transduction. The transduction shows those detected transmembrane domains. The experimental results have been compared with <software>TMHMM</software> 2.0 [4], <software>Pred-TMR</software> [9], <software>Prodiv-TMHMM</software> [6], <software>HMMTOP</software> 2.0 [18,5], <software>PHOBIUS</software> [19-21], <software>TMpred</software> [22,23] and <software>MEMSAT3</software> [24].<br>
<br>
Results and discussion<br>
Introduction<br>
We consider the prediction of transmembrane domains as a transduction problem. That is, given an amino acid sequence, the output of our system is a sequence with the same length which distinguishes between those amino acids that are within a transmembrane domain and those that are not.<br>
The available data are transformed in a training set with even linear structure. An item of the data set is a string whose first half is made up by the symbol sequence of the protein and the second by the symbols of the expected output string in reverse order. In order to learn the ELL with this set, we considered as the main feature the segments of a given length k set as a input parameter. The class of ktss languages is a well-known subclass of the regular languages and it is characterized by the set of segments of length k that appear in the words of the language, therefore, we can take profit of previous learning results in order to address this task [25-27].<br>
The transducer is obtained using the structure of the inferred ELG (Even Linear Grammar). The general method is described in Algorithm 1. Please refer to section Notation and definition to details.<br>
<br>
Algorithm 1 Transmembrane Grammatical Inference approach<br>
Input:<br>
? A set P of amino acid sequences with known transmembrane domains.<br>
? A set L of domain labeled sequences. Each string x in P has its corresponding string lx in L.<br>
Output:<br>
? A transducer to locate transmembrane domains.<br>
Method:<br>
? Combine the sets P and L to obtain the training set M with strings xlxr<br>
? Apply to the strings in M the transformation function ?<br>
? Apply a GI algorithm for (a subclass of) regular languages<br>
? Undo the transformation ? to obtain the ELG from the regular language<br>
? Return the transducer obtained from the ELG<br>
End<br>
The returned transducer can be used to analyse problem sequences to obtain the corresponding transduction.<br>
<br>
Datasets<br>
Due to the fact that each approach to transmembrane prediction uses its own dataset, in order to test our approach six different datasets has been considered. The first one was a set of 160 membrane proteins used in [4], which we refer to as the <database>TMHMM</database> set. Experimental topology data is available for these proteins, most of them have been analysed with biochemical and genetic methods (these methods are not always reliable), and only a small number of membrane protein domains of this dataset have been determined at an atomic resolution. The dataset contains 108 multi-spanning and 52 single-spanning proteins. The original dataset was larger, but those proteins whith conflicting topologies for different experiments were not included.<br>
The second set used was <database>TMPDB</database> [28], whose latest version (Release 6.3) contains 302 transmembrane protein sequences (276 alpha-helical sequences, 17 beta-stranded sequences and 9 alpha-helical sequences with short pore-forming alpha-helices buried in the membrane). The topologies of these sequences are based on definite experimental evidences such as X-ray crystallography, NMR, gene fusion technique, substituted cysteine accessibility method, Asp(N)-linked glycosylation experiment and other biochemical methods. The third and fourth datasets are subsets of <database>TMPDB</database>, where homologous proteins have been removed: the third set, TMPDB-?-nR, contains 230 alpha-helix non redundant proteins; and the fourth set TMPDB-??-nR, has been obtained by adding 15 ?-barrel proteins to the third set.<br>
The fifth dataset used is the 101-Pred-TMR database, a set of 101 non-homologous proteins, extracted form <database>SwissProt</database> database, used in [9,29]. These proteins were selected from a set of 155 proteins, discarding those with more than 25% of similarity.<br>
The last dataset used was the <database>MPTOPO</database> dataset [30]. In its last version (August 2007) the set contains 185 proteins: 25 of them ?-barrels and the rest ?-helix transmembrane. All the segments have been experimentally validated. The 3D structure of 119 of these proteins has been determined using x-ray diffraction or NMR methods, therefore, these transmembrane segments are known precisely. The rest of transmembrane segments correspond to 41 helices that have been identified by experimental techniques such as gene fusion, proteolytic degradation, and amino acid deletion. The proteins whose topologies are based solely on hydropathy plots have not been included in the dataset.<br>
<br>
Codification<br>
Protein sequences can be considered as strings from a 20 symbols alphabet, where each symbol represents one of the amino acids. In order to reduce the alphabet size without loss of information, we considered an encoding based on some properties of the amino acids (originally proposed by Dayhoff). The Table 1 shows the correspondence of each amino acid for Dayhoff encoding. This encoding has been previously used in some GI papers [31-33].<br>
<br>
Performance measures<br>
Several measures are suitable to evaluate the results. Some of them, addressing gene-finding problems, are reviewed in [34]. This measures can also be applied to functional domain location tasks. Among all the proposed measures, Sensitivity and Specificity are probably the most used. Intuitively, Sensitivity (Sn) measures the probability of predicting a particular residue inside a domain. Specificity (Sp) measures the probability of predicted residues to be actually into a domain. Therefore, Sn and Sp can be computed as follows:<br>
Sn=TPTP+FNSp=TPTP+FP<br>
Where:<br>
True positives (TP): correctly localized amino acids into a TM domain.<br>
True Negatives (TN): correctly annotated amino acids out of a TM domain.<br>
False positives (FP): amino acids out of a TM domain annotated as belonging to a domain.<br>
False Negative (FN): amino acids into a TM domain not correctly localized (annotated as out of any domain).<br>
Note that neither Sn nor Sp, took individually, constitute an exhaustive measure. A single value that summarizes both measures into a better one is the Correlation Coefficient (CC), also referred to as Mathews Correlation Coefficient [35]. It can be computed as follows:<br>
CC=(TP?TN)?(FN?FP)(TP+FN)?(TN+FP)?(TP+FP)?(TN+FN)<br>
Unfortunately, although CC has some interesting statistical properties [34], it has also an undesirable drawback. It is not defined if any factor of the root is equal to zero. In the literature there exist some measures that overcome this inconvenient, in this work we will use the Approximate Correlation (AC) which is defined as follows:<br>
ACP=14[TPTP+FN+TPTP+FP+TNTN+FP+TNTN+FN]AC=(ACP?0.5)?2<br>
We have to note that we were not able to calculate CC for every sample of the testing set (independently the dataset considered). In those cases, the samples were not taken into account. The Approximate Correlation AC has a 100% coverage, including those samples for which it was not possible to calculate CC or Sp. This can explain the relevant difference between AC and CC observed in some experiments. In addition to this, we have used the common segment-based measure Segment overlap, (Sov?obs) defined by [36]:<br>
Sov?obs=1N?smin(E)?max(B)+1+?max(E)?min(B)+1len(s1)<br>
where N is the total number of residues observed within all the domains of the protein, s1 and s2 are two overlaped segments, E is {end(s1); end(s2)}, B is {beg(s1); beg(s2)} and ? is a parameter for the accepted (maximal) deviation. We used a value of ? = 3.<br>
We have also calculated the number of segments correctly predicted at three accuracy thresholds: 100%, 90% and 75%, that is, number of segments with the 100%, 90% or more, and 75% or more of their amino acids are correctly predicted. This measure is similar to Sensibility, but it is based on segments. Therefore it is necessary to calculate also the Sp measure in order to complement it. This measure allows to obtain a reliable evaluation for those segments that contain false negatives not only at the extremities of the segment. For example, this occurs when a viewed segment is recognized as more than one segment, and there are some false negatives between two of this predicted segments. Figure 2 shows how this measure is calculated.<br>
<br>
Experimentation<br>
Note that our approach needs some information to learn a model. In order to obtain probabilistic relevance in the test of our method, we followed a leaving one out scheme: each sample protein of the dataset is annotated using as training set all the other samples. The process is repeated until all sample proteins have been used as test sequences. We carried out various experiments, taking into account different annotations for the test sequences. Each experiment was carried out over the six databases <database>TMHMM</database>, <database>TMPDB</database>, TMPDB-?-nR, TMPDB-??-nR, 101pred-tmr and <database>MPTOPO</database>. Note that all these sets but <database>TMPDB</database> have non homologous sequences.<br>
We hereby provide a description of each experiment, all the experiments but the last consider a previous reduction using the Dayhoff code: The first one (exp1) considered a two-classes encoding, that is, residues inside and outside a transmembrane domain; the second experiment (exp2) added another class in order to consider the topology of the protein (inner and outer residues); the third experiment (exp3) also included a class to distinguish among transmembrane domains with previous inner and outer regions; the fourth (exp4) experiment took into account the previous encoding with a special labelling of the last five residues of each region preceding a transmembrane one; the fifth experiment (exp5) added special symbols to track the transition to a transmembrane and out to one; the last experiment (exp6) did not consider the Dayhoff encoding and used the annotation of the second experiment.<br>
Each of the experiments builds a different model for the language of the TM proteins, that highlights differents propierties of them, by searching different patterns among the amino acids, depending on whether they belong, for instance, to a TM zone or not (exp1), to an inner or outer zone (exp2 and exp6), to a TM domain with previous inner or outer regions (exp3), to the sequence of the last 5 residues that precede a TM segment (exp4), or to the set of amino acids that represent a transition from a TM zone to an inner or outer zone, or vice versa (exp5). Figure 3 shows the annotation and encoding of an example sequence for each different experimental configuration.<br>
Once encoded the sequences, and for each of the described encodings, a set of experiments were run to test the best learning parameter of the inference algorithm. The best accuracy was obtained in the experiment with the configuration of exp5 and exp6. The HMM-based methods we compared our system with, obtain a slightly better precision. The difference in results can be explained with the fact that GI algorithms need a greater quantity of data than the amount needed by Hidden Markov Models in order to achieve the same accuracy.<br>
The main advantage of our approach is that it learns the topology of the model from samples, without the need of the external knowledge, as in HMM-based methods, where states and edges are determined by an expert. In a GI method, the automata are built by the algorithm, which stablishes the topology, number of states, the transitions or edges between states and probabilities of transition. Tables 2, 3, 4, 5, 6 show the experimental results of the fifth and sixth experiments (those which returned the best results) with the six datasets.<br>
Although it may seem erroneous or non-sense to build a model to predict both ? and ? transmembrane domains, we would like to illustrate with this experiment the way a GI approach distinguishes from other approaches: if the dataset contains enough data (sequences in our case) from differents classes (?-helices and ?-barrels), the model obtained should be able recognize all the different patterns. Table 7 compares the results of the experiment carried out over TMPDB-?-nR and TMPDB-??-nR datasets. The results with TMPDB-??-nR are slightly worse, but it can be explained because the set of ?-barrel proteins contains only 15 sequences, and it is difficult to learn an accurate model from this set. In fact, when we train and test with only this set of ?-barrel proteins (which would be TMPDB-?-nR) the result are roughly worse: (results from exp5) 0.506 for Sp, 0.170 for AC and 0.318 for Sov3obs; and in exp6:0.541 for Sp, 0.270 for AC and 0.584 for Sov3obs.<br>
<br>
<br>
Conclusion<br>
This work addresses the problem of the localization of transmembrane segments within proteins by making use of Grammatical Inference (GI) algorithms. GI has been effectively used in some bioinformatic related tasks, such as gene-finding or prediction of coiled coil domains. IgTM exploits the features of proteins by using Even Linear Languages as the inferred class of languages. We tested different labellings for the input sequences, with the best accuracy achieved using a labelling that takes into account several changes in the sequence topology: from inside and outside the membrane to it and vice versa. We compared our method with other methods to predict transmembrane domains in proteins, obtaining slightly less accuracy with respect to them. This should be due to the fact that in GI the training phase need more data than the most common approach, based on Hidden Markov Models. In addition to this, many of the available prediction tools are closed, that is, there is no way to know exactly the training set used by the tools which we have compared igTM with, therefore it is possible that some of our six datasets included proteins used by these tools in the training phase (in this case, the tools we compare our algorithm with, would obtain better results). The same problem happens with online prediction tools, where the data considered to build the tools is not available. Then, since the other methods can have been trained on sequences that share homology with the test set (or even sequences included in the test set), the comparison could be not very reliable. However, the obtained results show that GI can be used effectively in bioinformatics related tasks. Furthermore, the main advantage of GI when applied to bioinformatics tasks is that an expert is not needed in order to give additional information (in this case the topology of transmembrane proteins). An online version of <software>IgTM</software> is publicly available at <br>
It remains as a future work to use this method together with another one (based on HMM or not). This could lead to improve the performance. At present we are testing other inference algorithms to learn the automata, the use of new codings to the sequences [37,38], and the consideration of new datasets (for instance the M?ller dataset [39]).<br>
<br>
Methods<br>
Introduction<br>
Our approach considers the concatenation of the protein symbols with the inverted annotation string, the whole considered as an ELL string. We subsequently apply a transformation to it, in order to obtain a string belonging to a regular language. The transformation is done by joining the first symbol of the first half with the last of the second one, the second symbol of the first half with the second-last symbol, and so on. Then, a GI process learns a language building a transducer that accepts the first part of each symbol (the one coming from the first half of the string) and returns the second part as output. The test phase consists in using Viterbi's algorithm to analyse the string. This algorithm returns the transduction that is most likely to be produced by the input string.<br>
<br>
Notation and definitions<br>
Let ? be an alphabet and ?* the set of words over the alphabet. A language is any subset of ?*, that is a set of words. For any word x over ?* let xi denote the i-th symbol of the sequence. Let |x| denote the length of the word and let xr denote the reverse of x. Let also ? denote the empty word. A grammar is denoted by G = (N, ?, P, S) where N and ? are the auxiliar and terminal alphabets, P is the set of productions and S ? N is the initial symbol or axiom. Intuitively, a grammar can be seen as a rewritting system that uses the set of productions to generate a set of words over ?*. The language generated by a grammar G is denoted by L(G).<br>
An Even Linear Grammar (ELG) is a context-free grammar [40] where the productions are of the forms:<br>
A?xBywhere?A,B?N,x,y????and?|x|=|y|A?xwhere?A?N,x???<br>
The class of Even linear Languages (ELL) is a subclass of the context free languages and includes properly the class of regular languages. Given an ELG, it is possible to obtain an equivalent one where the productions are of the form:<br>
A?aBbwhere?A,B?N,a,b??A?awhere?A?N,a???{?}<br>
The learning of ELL can be reduced to the inference of regular languages [41]. The general algorithm consists in transforming the training strings through a function ?: ?* ? [? ? ?]* ? [?]* defined as follows:<br>
?(?)=??(a)=[a]where?a???(axb)=[ab]?(x)where?a,b???and?x???<br>
Intuitively, this function relates the first and last symbols of the word, as well as the second and the last but one, and so on. Once applied the function ?, it is possible to use any regular language inference algorithm to learn a language over the alphabet [? ? ?]* ? [?]*, that is, the alphabet of paired symbols. The learned language can be processed to undo the transformation ? as follows:<br>
?A?[ab]B?P?add?the?production?A?aBb?to?the?ELG?A?[a]?P?add?the?production?A?a?to?the?ELG?A???P?and?all?these?productions?to?the?ELG<br>
Several inference algorithms are suitable to be applied, each obtaining a different solution. In fact, if the GI algorithm identifies a subclass of regular languages, then a subclass of ELL is obtained and applied with good performance.<br>
A finite state transducer is an abstract machine formally defined by a system ? = (Q, ?, ?, q0, QF, E) where: Q is a set of states, ? and ? are respectively the input and output alphabets, q0 is the initial state, QF ? Q is the set of final states and E ? (Q ? ?* ? ?* ? Q) is the set of transitions of the transducer. A transducer processes an input string (word of a language), and outputs another string. A successful path in a transducer is a sequence of transitions (q0, x1, y1, q1), (q1, x2, y2, q2), ..., (qn-1, xn, yn, qn) where qn ? QF and for 1 ? i ? n: qi ? Q, xi ? ?* and yi ? ?*. Note that a path can be denoted as (q0, x1x2 ... xn, y1y2 ... yn, qn) whenever the sequence of states are not of particular concern. A transduction is defined as a function t: ?* ? ?* where t(x) = y if and only if there exist a successful path (q0, x, y, qn). Figure 4 shows an example of transducer, and the transduction that an accepted sequence generates. We refer the interested reader to [42].<br>
<br>
Grammatical inference approach to transmembrane segments prediction<br>
We consider the transmembrane segments prediction problem as a transduction problem. That is, given an amino acid sequence, the output of our system is a sequence with the same length which distinguishes between those amino acids within transmembrane segment and those that are not. In our work, we took into account the special features of our problem to propose a method based on inference of ELL.<br>
First of all, we had to transform the available data to obtain a training set with even linear structure. This set was used to infer an ELL. The transducer is obtained using the structure of the inferred ELG. Given a ELG G = (N, ?, P, S) that does not contain productions of the form A ? a, a ? ?, it is possible to obtain a transducer ? = (N, ?, ?, S, QF, E) where:<br>
QF={A?N:(A??)?P}E={(A,a,b,B):(A?aBb)?P}<br>
Example 1 shows how this transformation work.<br>
Example 1 Given the ELG G = (N, ?, P, S) with the productions:<br>
S?aS0|bB1A?aA1|bS0B?aA1|bB1|?<br>
then, the transducer ? = (N, ?, ?, S, {B}, E) is obtained where:<br>
E={(S,a,0,S),(S,b,1,B),(A,a,1,A),(A,b,0,S),(B,a,1,A),(B,b,1,B)}<br>
The resulting transducer is shown in Figure 4.<br>
As we stated before, the learning problem for ELL can be reduced to the problem of learning regular languages. In our work, in order to learn the ELL, we use an algorithm to infer k-testable in the strict sense (k-TSS) languages [25-27]. The class of ktss languages is contained into the regular languages one; it is characterized by the set of segments of length k that appear in the words of the language.<br>
Our approach considered a set of protein sequences P with known transmembrane domains and another set L of strings over an alphabet of labels ? = {i, o, M}. For each sequence x in P, a labeled sequence lx is obtained. The labelling allows to distinguish the transmembrane segments from the non-transmembrane ones. That is, given the string x = x1x2 ... xn ? P and its corresponding labeled string lx, = l1l2 ... ln ? L, li = M whenever xi correspond to a transmembrane segment, li = i, when correspond to a inner segment, and li = o, when correspond to a outer segment.<br>
These sets were combined to obtain another set, named M, with the strings xlxr. Note that the strings in this set have an even linear structure and an even length. The set M was used to obtain a probabilistic transducer by ELL inference. The general method is summarized in Algorithm 1.<br>
The returned transducer can be used to analyse problem sequences to obtain the corresponding transduction. It is possible that the transducer may result to be non-deterministic and the test sequences may not belong to the language accepted by the transducer. Therefore, an error-correcting parser (for instance Viterbi's algorithm) is necessary to analyze the test sequences. We employed a standard configuration of Viterbi's algorithm used when a GI approach is applied to pattern recognition tasks (i.e. [33]).<br>
<br>
Complexity<br>
The igTM method is composed by two phases: inference and analysis. The first one consists in inferring an transducer from the sequences of the dataset.<br>
The execution time of the GI algorithm used in this work is linear with the size of the dataset. The space requirements of this step is bounded by |?|k, where ? is the alphabet of the samples and k is the parameter of the k-tss algorithm. Therefore, depending on the parameter used, the automaton obtained can be relatively big. The transformation of the automaton into a transducer is bounded by a polynomial of degree k.<br>
The execution time of the second phase, the analysis one, is linear respect the size of the string to analyse. The space requirements are bounded by the size of the transducer and the analized string.<br>
<br>
<br>
Authors' contributions<br>
PP wrote the software and carried out the experimentation. MC performed the search of web resources. All authors contributed to the study and interpretation of the results. The paper was written by PP and DL. DL conceived the research and supervised the whole process. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2661053</b><br>
<software>GMFilter</software> and <software>SXTestPlate</software>: software tools for improving the SNPlex? genotyping system<br>
<br>
<br>
Background<br>
Single-nucleotide polymorphisms (SNPs) are the most frequent variation in the human genome and are therefore preferably used for mapping in genetic research. SNP genotyping has become an essential tool for investigating the genetic background of complex diseases and for analyzing genetic variation between individuals. Different genotyping systems are available, depending primarily on the scale of throughput. Laboratories equipped with low-throughput genotyping platforms often use the TaqMan? system, while for higher throughput the SNPlex? system is one of the main tools for SNP genotyping [1,2]. SNPlex? is a cost-efficient, highly flexible and scalable technique, which is based on oligonucleotide ligation (OLA), multiplex polymerase chain reaction (PCR) and capillary electrophoresis. SNPlex? assays are combined in a SNP set, which allows for simultaneous genotyping of up to 48 SNPs. Eighty percent of SNPs in non-repetitive regions of the human genome are suitable for SNPlex? assays [2]. The analysis software <software>GeneMapper</software> generates genotypes from electrophoretic raw data. It supports automated allele calling, visualization and quality control of genotype data.<br>
At the Institute for Clinical Molecular Biology in Kiel we operate a SNPlex? genotyping platform, which produces up to 280,000 genotypes per day. This huge amount of results requires strategies for ensuring quality control and correctness of data. Here we present two programs, which address issues specific for the SNPlex? system.<br>
One key feature of the <software>GeneMapper</software> software is the automated calling of alleles. However, the employed clustering algorithm often produces false positive results, if too many wells have a low signal intensity. The analysis can significantly be improved by removing those wells from the raw data. We have developed <software>GMFilter</software> to automate this task.<br>
Applied Biosystems offers a test system for evaluating the genotype accuracy of SNPlex?. This system consists of the SNPlex? System gDNA Plates Kit and the SNPlex? System Control Pool Kit. Because no software for analyzing a test plate is available, we have developed <software>SXTestPlate</software>. The program calculates the call rate and concordance for each DNA of the test plate and each SNP of the control pool. <software>SXTestPlate</software> can be used independently of the Applied Biosystems test kits, which allows scientists to use custom plates and control pools.<br>
Both programs are intended to reduce genotyping errors. A genotyping error is defined as the discrepancy between the observed genotype and the true genotype. Such errors can seriously affect linkage analysis, decrease the power of association studies or cause incorrect allele identification in population genetic studies. Pompanon et al. provide an in-depth overview of the cause and consequences of genotyping errors and how to quantify error rates [3].<br>
<br>
Implementation<br>
<software>GMFilter</software> and <software>SXTestPlate</software> have been written in Visual Basic 6. They were implemented on a Microsoft Windows 2000 system and run on Windows 2000 and XP platforms. <software>SXTestPlate</software> requires a free <software>Microsoft SQL Server</software> 2005 Express Edition database. It connects to the database via TCP/IP using the OleDb provider. For performance reasons the file import in <software>SXTestPlate</software> uses the bulk insert function of the database. In order to execute bulk insert commands users must belong to the bulkadmin or sysadmin fixed server role. The online help for <software>GMFilter</software> and <software>SXTestPlate</software> is based on Microsoft HTML Help 1.4 and has been written with <software>Microsoft HTML Help Workshop</software>. Both programs are free software, which can be redistributed and/or modified under the terms of the GNU Lesser General Public License.<br>
Our SNPlex? genotyping platform uses 384-well DNA plates. The detection is done with Applied Biosystems 3730xl DNA Analyzers, which are equipped with 96-capillary arrays. The machines are controlled by the <software>Data Collection</software> 3.0 software. Analysis is performed with the <software>GeneMapper</software> 4.0 software.<br>
<br>
Results and discussion<br>
<software>GMFilter</software>: a tool for improving <software>GeneMapper</software> analysis<br>
Under optimal conditions, when most of the SNPlex? assays perform well and when the DNA samples are of superior quality, i.e. giving rise to high signal intensities, the clustering algorithm of the <software>GeneMapper</software> software performs adequately. Bad wells are assigned a well quality of zero and are consequently ignored in the analysis. However, if too many wells have low signal intensities, <software>GeneMapper</software> tends to include bad wells in the analysis, which can lead to false positive genotype assignments. We found that removing bad wells (corresponding to bad samples) from the raw data reduces noise, which significantly improves the analysis. For an automated processing of this task we have developed <software>GMFilter</software>.<br>
Figure 1 shows the workflow of the <software>GMFilter</software> usage: Users first analyze a SNPlex? plate with a special <software>GeneMapper</software> analysis method ("NoCluster") in order to identify bad wells. In <software>GMFilter</software> they enter the folders, which contain the exported sample plot table from <software>GeneMapper</software> and the raw data (fsa files). The tool lets users specify the threshold for the median signal intensity and which wells are being used for controls (Figure 2). <software>GMFilter</software> analyzes the exported sample plot table by importing the file and converting it to an unbound recordset for better internal data handling. It calculates the median signal intensity for each well using basic descriptive statistics. Based on these calculations it generates a normal MS-DOS batch file, which deletes fsa files of wells having a median signal intensity below the specified quality threshold. Deletion of bad wells and reanalysis of the plate with a modified version of the standard Rules analysis method ("RelaxedRules") in <software>GeneMapper</software> leads to much better clustering results (Figure 3). <software>GMFilter</software> needs about 10 minutes for processing the sample plot table of a 384-well plate on a normal desktop computer (Windows XP, 2 GHz processor, 512 MB RAM). The analysis methods "NoCluster" and "RelaxedRules" are available on the <software>GMFilter</software> homepage [4].<br>
<br>
<software>SXTestPlate</software>: a tool for evaluating genotyping performance<br>
Regular checks of the genotyping platform are an important aspect of quality control. To facilitate this task Applied Biosystems offers the SNPlex? System gDNA Plates Kit and the SNPlex? System Control Pool Kit. In combination they are a useful tool for evaluating genotype accuracy and precision, e.g. detection of potential contamination. The control pool contains 48 assays and the test plate contains 44 distinct DNA samples, which are replicated multiple times within one quadrant. Applied Biosystems also provides a file with the "true" reference genotypes. However, evaluating a test run by comparing text files in Excel, for example, can be a tedious work. We thus developed <software>SXTestPlate</software> as a user friendly tool for the statistical analysis of genotypes from the test plate, which was typed with the control pool.<br>
<software>SXTestPlate</software> also supports custom test plates and control pools, making the tool independent of the Applied Biosystems kits. The DNA samples on the test plate may be replicated multiple times. <software>SXTestPlate</software> determines automatically, how often a sample is replicated, by grouping the imported genotypes by identical DNA and SNP identifiers. This flexibility makes the program applicable for genotyping technologies other than SNPlex? as well.<br>
<software>SXTestPlate</software> imports the reference genotypes and the exported genotype table from the <software>GeneMapper</software> software into an <software>SQL Server</software> 2005 Express Edition database and compares them to each other by using complex structured query language (SQL) statements, which are executed as stored procedures on the database server. The tool reports the call rate and the concordance with the expected genotypes for each DNA and SNP (Figure 4). If discrepancies are found, detailed results are shown. On a normal desktop computer (Windows XP, 2 GHz processor, 512 MB RAM) <software>SXTestPlate</software> needs approximately 10?15 seconds for analyzing a 384-well plate, which was typed with a control pool containing 48 assays.<br>
<br>
<br>
Conclusion<br>
Laboratories engaged in high-throughput SNP genotyping have a great demand for automated data management in order to handle the large amount of genotyping data and to ensure quality control. We have developed <software>GMFilter</software> and <software>SXTestPlate</software> as a useful toolkit for SNPlex? genotyping platforms. Both programs are freely available for non-commercial use.<br>
We have demonstrated that removing wells with a low signal intensity from the raw data of a SNPlex? run considerably improves the analysis of the <software>GeneMapper</software> software, since it reduces noise and consequently, the number of false positive genotype assignments. <software>GMFilter</software> has been developed as a user-friendly Windows program for the automated filtering of SNPlex? raw data.<br>
An important issue in a high-throughput laboratory is the routine evaluation of the genotyping performance. Users can make a test run with the SNPlex? System gDNA Plates Kit and the SNPlex? System Control Pool Kit offered by Applied Biosystems or with custom plates and SNP sets. We have developed <software>SXTestPlate</software> for the statistical analysis of a test run. The tool reports the call rate and concordance for each DNA of the test plate and each SNP of the control pool.<br>
<br>
Availability and requirements<br>
Project homepages<br>
<software>GMFilter</software>: , <software>SXTestPlate</software>: <br>
Operating system<br>
<software>GMFilter</software>: Windows NT or later versions<br>
<software>SXTestPlate</software>: Windows 2000 or later versions<br>
Programming language<br>
Visual Basic 6<br>
Other requirements<br>
<software>SXTestPlate</software> requires the free database <software>SQL Server</software> 2005 Express Edition from Microsoft .<br>
License<br>
GNU Lesser General Public License<br>
<br>
Authors' contributions<br>
MT implemented the software and wrote the manuscript. MW helped to develop the <software>GeneMapper</software> analysis methods for <software>GMFilter</software>. SS supervised the genotyping and helped writing the paper. AF performed the genotyping, helped to design the software and contributed to the writing of the manuscript. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2744707</b><br>
p3d ? Python module for structural bioinformatics<br>
<br>
<br>
Background<br>
The increasing number of high-resolution protein structures available in the protein database [1] allows knowledge based approaches [2-4] by comparing structural features throughout non-redundant protein data sets [5]. Such knowledge based approaches can help to identify key parameters in e.g. ligand binding [6] or can be used to estimate favourable structural configurations that are important for de novo protein design or prediction of protein folding [7,8]. However, such approaches require a rapid development of new structural bioinformatic tools that are adapted to the questions asked, which in turn requires a robust framework or module. p3d is such a module for the Python scripting language . Although similar modules exist as part of the BioPython project [9] or part of the biskit package [10], p3d was developed in order to offer a Python module that is powerful and fast, yet intuitive to use. The simplicity of p3d is due to a) the usage of object oriented programming (i.e. atoms are treated as vectors), b) the implementation of a query parser that translates queries written in human readable language into a combination of algebra set operations and c) the fact that no additional Python packages are necessary. The speed is due to the usage of a binary space partitioning (BSP) tree which allows very fast queries in 3D [11]. The additional strength is obtained by the flexible combination of both speed and complexity in the intuitive and thus natural queries to the structural data.<br>
The combination of these factors makes p3d the optimal module to rapidly develop new and powerful bioinformatic tools that follow the Python philosophy of making the source code readable.<br>
<br>
Implementation<br>
p3d is written in python 2.6 and compatible with the upcoming new standard Python 3.0. <database>PDB</database> files are read into the p3d structure, schematically illustrated in figure 1. During pdb loading each atom is converted into an atom object, which inherits all properties form the vector class (see below). These objects are stored into a list and linked to their proper sets, which will be used by the query function. Figure 1 shows some sets and how they intersect. Queries can therefore be directly translated into algebraic set operations, e.g. "select all atoms that are oxygens, belong to the residue name ATP and have a residue id smaller than 20". For fast queries in 3D a binary space partitioning (BSP) tree is generated automatically. There, the structure is divided into small subspaces. Figure 1 illustrates the recursive divisions performed on an aquaporin structure (Chain A, 1RC2.pdb [12]) during tree initialisation. The implemented query functions allow the combination of all sets, of the BSP tree neighbor search and of custom user defined vectors or atoms. Thus very complex queries can be formulated in a human readable syntax (see below).<br>
<br>
Results and Discussion<br>
p3d offers an intuitive and robust interface between the Python scripting language and the complex nature of protein structure files. The input files can be in pdb format or the compressed gzip versions. All following examples, indicated with "&gt;&gt;&gt;" are within the Python IDLE console, but can equally be incorporated into standalone scripts. A more detailed documentation for all modules and functions can be found online [13]. Loading a protein structure is done via:<br>
&gt;&gt;&gt; from p3d.protein import Protein<br>
&gt;&gt;&gt; pdb = Protein('2AXT.pdb.gz')<br>
All atoms are treated as vector objects and can be rapidly accessed via hash tables, algebra of sets, lists, the BSP-tree class and any combination of those. For example all atoms are stored in the list pdb.atoms and the hash table can be found in the dictionary pdb.hash. Detailed information of how the structural data can be directly accessed can be found in the online documentation [13].<br>
An easier and more intuitive access to the structural information is offered via the query() or lookUpAtom() functions. These functions try to return a set of atoms or one atom object, respectively. The usage for the query function is e.g.:<br>
&gt;&gt;&gt; atoms = pdb.query("chain A and resid 13..25")<br>
This will return all atoms that are part of chain A and have a residue id from 13 to 25. The returned list of atom objects can then directly be used in another query (see below) or can be treated like vectors. The complete syntax of the query-string can be found in the online documentation [13].<br>
Such a generalised approach brings a lot of flexibility and robustness. As a result, a lot of exceptions in pdb files can be handled without additional precautions, for example: a) Multiple models/structures (NMR, e.g. 7GAT[14]), b) Alternative side-chain conformers (e.g. 1BPH, [15]) or alternative main chain tracings (e.g. 1AZZ, [16]), c) disordered residues (e.g. 1EN2, (Saul et al. 2000)) and d) non amino acid residues, e.g. DNA (e.g. 7GAT[14]).<br>
Benchmarking BioPython's pdb module against p3d showed that both modules have their strengths and weaknesses and, as usual, the results depend on the choice of the testing routines. For example, BioPython's pdb module performs a faster neighbour search since it calls a subroutine written in c from a different BioPython module whereas p3d relays on its BSP Tree that is written in Python and is as such slower. P3d is however faster in selecting wanted atoms due to its implementation of sets whereas BioPython's pdb module requires looping and unfolding over all entities. As a result p3d performs better if a neighbour search is connected with a complex query, such as all protein oxygen atoms within 3 ? of ATP simply because BioPython's pdb module requires a neighbour search for each ATP atom and additional checking if the found atom is part of the protein.<br>
From a programming point of view, another clear advantage of p3d is its intuitive and simple usage, e.g. during benchmarking sample scripts written in p3d required one line using the query function while BioPythons pdb module required 7 lines including 4 loops over all pdb structure entities.<br>
Another advantage of p3d is that each atom is treated as one object and no additional conversions or translations have to taken into account. The atom objects are created from each line in the pdb files. Each created Atom object holds all information regarding its properties, i.e. the Cartesian coordinates (x, y, z), the atom type, the residue name and number, the peptide chain and parent protein it is part of.<br>
Therefore simple recursive queries through a protein structure can be performed, e.g.:<br>
&gt;&gt;&gt; atom = pdb.lookupAtom("resname ILE &amp; oxygen &amp; resid 30")<br>
&gt;&gt;&gt; chainAtoms = atom.allAtomsOfSameChain()<br>
&gt;&gt;&gt; residueAtoms = atom.allAtomsOfSameResidue()<br>
Additionally the atom information of the alternative conformer labels, the model number (NMR structures) and the beta and user value are part of the atom class, e.g.:<br>
&gt;&gt;&gt; atom.x; atom.beta; atom.atype; atom.resid; atom.chain; atom.model<br>
Since the Atom object inherits its properties form the Vector object, simple vector operations, such as addition, subtraction, length, dot and cross product are possible at the atom level without any additional overhead, e.g.:<br>
&gt;&gt;&gt; O = pdb.lookUpAtom("chain A &amp; resid 1 &amp; atype O")<br>
&gt;&gt;&gt; N = pdb.lookUpAtom("chain A &amp; resid 2 &amp; atype N")<br>
&gt;&gt;&gt; v = O + N; v = O - N; v = O.dot(N); v.length(); v = O.cross(N)<br>
The history of vector operations on atoms is stored in the atom.desc property, thus allowing to keep a record on the performed transformations. Other implemented vector operations can be found in the online manual. The vector class can also be used to define new objects, new points of interest in space. Those can then be used as part of the query function. This interchangeability between structural data and user-defined vectors is unique to p3d. The user can therefore query protein surroundings by defined coordinates in a simple way, e.g.<br>
&gt;&gt;&gt; v = p3d.vector.Vector(18.00,12.00,-23.4)<br>
&gt;&gt;&gt; atomsAroundv = pdb.query("protein and within 4 of ", v)<br>
The current version of p3d features two additional classes based on vector operations. These are the TransformationMatrix (TM) class and the Plane class. Both are part of the p3d.geo module. The TM class returns a matrix object when two sets of three vectors (source and destination) are given. Vectors or atoms that are multiplied with the matrix will be transformed from the source space into destination space. This can be used e.g. to align structures with only a few lines of source code:<br>
&gt;&gt;&gt; alignAtoms = ['N1','C5','N3']<br>
&gt;&gt;&gt; sourceAtoms = []<br>
&gt;&gt;&gt; targetAtoms = []<br>
&gt;&gt;&gt; for atom in alignAtoms:<br>
&gt;&gt;&gt; sourceAtoms.append(pdb1.lookUpAtom('resname ATP and atom type ', atom))<br>
&gt;&gt;&gt; targetAtoms.append(pdb2.lookUpAtom('resname ATP and atom type ', atom))<br>
&gt;&gt;&gt; tm = p3d.geo.TransformationMatrix(sourceAtoms, targetAtoms)<br>
&gt;&gt;&gt; for atom in pdb1.atoms:<br>
&gt;&gt;&gt; print (tm*atom).output()<br>
The complete script can be found on the p3d web site [13].<br>
The Plane class allows e.g. calculations of dihedral angles,<br>
&gt;&gt;&gt; p3d.geo.dihedral(atom1, atom2, atom3, atom4)<br>
&gt;&gt;&gt; alpha = pdb.lookUpAtom("alpha and resid 1 and chain A and model 2")<br>
&gt;&gt;&gt; alpha.calcPhiPsi()<br>
Furthermore, the Plane class can for example also be used to calculate orientations of ligands over flat co-factors if three atoms of the co-factor are used to define the plane. This was used to calculated the orientation of histidine heme ligands relative to the heme by projecting a vector that represents the ligand orientation, (i.e. v = ND1 - CG) onto the heme plane, i.e. heme.projectionOfVector(ND1-CG) [7]. A basic example for this usage is:<br>
&gt;&gt;&gt; a = p3d.vector.Vector(1,0,0)<br>
&gt;&gt;&gt; b = p3d.vector.Vector(0,1,0)<br>
&gt;&gt;&gt; c = p3d.vector.Vector(1,1,0)<br>
&gt;&gt;&gt; plane = p3d.geo.Plane(a,b,c)<br>
&gt;&gt;&gt; k = p3d.vector.Vector(2,2,2)<br>
&gt;&gt;&gt; plane.projectionOfVector(k).info(lvl='coordinates')<br>
[2.000, 2.000, 0.000]<br>
All atom properties can be changed and the altered protein can be easily saved to a new file, e.g:<br>
&gt;&gt;&gt; for atom in pdb.atoms:<br>
... atom.translateBy(k)<br>
... atom.beta = 3.2<br>
&gt;&gt;&gt; pdb.writeToFile(pdb.fullname+'_changed.pdb')<br>
The implementation of a BSP tree accelerates queries in space. The query functions allow the combination of spatial, i.e. BSP Tree queries and set theory, thus very complex queries can be formulated at ease, e.g.:<br>
&gt;&gt;&gt; ATPs = pdb.query("resname ATP")<br>
&gt;&gt;&gt; surrounding = pdb.query("protein &amp; within 3 of ", ATPs, " and not nitrogens")<br>
Example scripts shown online [13] illustrate furthermore the simplicity of Python scripts that use the p3d module. These are for example a script that analyses the distribution of phi and psi angles in a non-redundant protein set similar to the work of Hovm?ller et al. [17]. By using p3d this analysis can be performed using only 26 lines of code. Another example is a script that determines the distances between different protein chains, which can be written with 36 lines of code, documentation included. This data can be used to plot the contact map between different protein chains.<br>
Overall these features and their intuitive usage highlight the possibility to develop tools for structural bioinformatics rapidly.<br>
<br>
Future development<br>
P3d will be kept updated and user requests might be implemented into the source code. Overall p3d will be maintained by the authors and hopefully other programmers will join this open source project. Two future aims will be a) to implement a faster BSPTree, eventually written in c/c++ and b) to add the syntax for spatial queries, e.g. select all "proteins and x-coordinates &lt; 40". Furthermore p3d's website will expand with scripts that are posted by the users/readers.<br>
<br>
Conclusion<br>
The p3d package extends the Python scripting language with a robust and powerful interface to investigate and manipulate protein structure files. The object oriented approach of p3d, the treatment of atoms as vectors, the usage of sets, the implementation of a BSP tree and the combination of all these factors into a query interface that uses human readable language make p3d a very fast and versatile module that allows rapid development of high throughput tools for structural bioinformatics.<br>
<br>
Availability and requirements<br>
Project name: p3d<br>
Project home page: <br>
Operating systems(s): Platform independent<br>
Programming language: Python 2.6+ and 3.0 ready<br>
Other requirements: none<br>
License: GNU GPL V2<br>
Any restrictions to use by non-academics: none<br>
<br>
Authors' contributions<br>
CF, concept, design, manuscript, coding of the protein, atom, vector, geo, protein, library and tree submodules and online manual. MS important contributions to the code design and coding of the query and geo module. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2828439</b><br>
From learning taxonomies to phylogenetic learning: Integration of 16S rRNA gene data into FAME-based bacterial classification<br>
<br>
<br>
Background<br>
Chromatographic fatty acid methyl ester (FAME) profiling is used in many laboratories for bacterial identification. The fatty acid composition of bacterial species is genetically conserved and the measured composition is stable, when highly standardized culture, extraction and analytical conditions are used. More than 300 fatty acids have already been found in bacteria. Differences in chain length, positions of double bonds and the binding of functional groups make them very useful taxonomic markers [1,2]. In the last decades, FAME profiling has become a routine method since it is cheap, fast, automated and high-throughput. As a result, many institutes have set up private FAME databases to store the massively generated numbers of FAME profiles and, recently, a publicly accessible FAME database has been realized by some of the present authors [3]. Such databases are an ideal target for data mining and knowledge discovery. Where bacterial species identification is usually performed by comparing FAME profiles against identification libraries with fixed peak percentages, FAME-based bacterial species identification can be improved by the application of machine learning techniques [4,5]. However, different numerical studies on the resolution of FAME analysis for species discrimination have underscored that FAME profiling cannot be used to discriminate all species from one another [6-9]. Nevertheless, machine learning techniques for multi-class classification are able to maximally exploit the pattern information in the FAME data to delineate the different species that constitute the different classes in this multi-class classification problem [5].<br>
At present, the gold standard for bacterial species discrimination is a DNA-DNA hybridization (DDH) percentage of 70%. Nonetheless, DDH should be performed in a polyphasic study of the species because phenotypic characteristics should agree with this definition [10,11]. Importantly, with the advent of 16S rRNA gene sequence analysis, Stackebrandt and Goebel [12] showed that species having 70% or greater DNA similarity usually correspond to a 16S rRNA gene sequence identity greater than 97%. Furthermore, Konstantinidis and co-workers evaluated the species definition in the perspective of whole-genome sequence analysis and showed that the 70% DDH standard correlates with a 95% average nucleotide identity [13,14]. Even though DNA reassociation is the gold standard for circumscribing the taxonomic rank of species and genome studies flourish, 16S rRNA gene sequence analysis is still widely preferred for species delineation for two important reasons: 16S rRNA gene sequence identity greater than 97% may indicate a specific species and sequencing the 16S rRNA gene has become much cheaper and faster due to technological advances. For these reasons, we also focus on the 16S rRNA gene in this work. It is, however, important to remark that, as a consequence of this explosive trend of gene sequencing, deposits in the public nucleotide sequence databases have witnessed an exponential growth. Nonetheless, sequence analysis and phylogenetic reconstruction studies should rely on high quality nucleotide sequences. With the exponential growth of the sequence databases, the number of poor quality sequences also grows extensively and sequence curation becomes indispensable. To circumvent manual curation, the <database>SILVA</database> database project allows users to retrieve quality controlled and aligned rRNA sequences as stored in the EMBL sequence database [15]. In relation to this work, we tackle the bacterial species classification problem by combining the information represented by aligned 16S rRNA gene <database>SILVA</database> sequences and FAME profiles. Due to technological advances, both these types of data can be easily obtained at very low cost. However, when used alone, FAME data has a limited ability to discriminate among species. Combining the knowledge contained in FAME profiles and 16S rRNA gene sequences could overcome some of these limitations.<br>
At present, machine learning papers describing multi-class classification with classes structured in a taxonomy, or thus a tree topology, mainly focus on the area of web-, document-, text- and ontology-based classification. Many research problems involve multi-furcating tree nodes, and most papers deal with data instances primarily corresponding to multiple classes structured in this kind of hierarchical setting. Classification problems related to this issue are better known as multi-label classification. In machine learning terms, learning by exploiting hierarchical structure information is called hierarchical classification [16-23], learning with taxonomies [24] and structured label learning [25]. However, these studies do not explicitly involve hierarchical classification for single-label multi-class classification, meaning that each data instance is classified at leaf level. From another perspective, hierarchical classification has also been proposed for standard multi-class classification tasks. In this setting, the idea consists of improving multi-class classification methods by constructing a tree of binary classifiers [26-28]. The tree architecture is defined by the considered data and tree inference is based on different algorithms for distance calculation between the considered classes.<br>
In contrast to previous work, where typically a single type of data was used for bacterial species identification, we evaluate the integration of taxonomic and phylogenetic knowledge into FAME-based classification models. To this end, species of the genus Bacillus are considered. We design supervised machine learning techniques to automatically discriminate FAME profiles of bacteria at species level, in a hierarchical classification setting where the labels correspond to the different species. In particular, clustering methods define the taxonomic or phylogenic tree in a first stage and Random Forest (RF) classifiers are trained on FAME profiles in these trees in a second stage. Two different strategies for the integration of taxonomic and phylogenetic knowledge are investigated. As a proof-of-concept, we consider the integration of relationships between species solely based on FAME data. Herein, a FAME tree is constructed by divisive clustering and evaluated for hierarchical multi-class classification. In the core part of this paper, we consider knowledge integration from the perspective of bacterial phylogeny. Using 16S rRNA gene sequence analysis, phylogenetic trees are constructed and subsequently used for hierarchical single-label multi-class classification, in which FAME data serve as input. This last strategy is further referred to as phylogenetic learning, an approach that combines two types of data: 16S rRNA gene data is considered to incorporate phylogenetic knowledge in the form of a hierarchy or tree and the hierarchically ordered classifiers are constructed based on FAME data. Our tests indicate that this new approach resolves some of the classification tasks that classifiers only based on FAME data could not achieve. In relation to other work, the use of phylogenetic tree information has already been considered for classification of protein-protein interaction [29] and multi-class classification in a taxonomic context has already been performed based on genomic sequence data [30,31]. However, the incorporation of phylogenetic information in hierarchical classification models for bacterial species has not been investigated so far.<br>
<br>
Results and Discussion<br>
Because FAME data does not allow for a global discrimination of bacterial species, we tackle the bacterial species classification problem by combining FAME data with taxonomic or phylogenetic knowledge. Therefore, within the framework of bacterial taxonomy, an interesting direction for subsequent machine learning research is that of integrating this knowledge. This is easily achieved by learning in a hierarchical scheme or an inferred tree. Two approaches are considered: tree inference by FAME data and inference of phylogenetic trees based on 16S rRNA gene sequences. In this paper, we evaluate the integration of these two particular types of knowledge into the FAME-based bacterial species classification problem.<br>
Learning taxonomies<br>
As a first step, we investigated the possibility of reconstructing a small part of the phylogenetic structure of the genus Bacillus by FAME data and RFs. Divisive clustering with classifier performance as splitting criterion gives rise to a particular tree. In this tree, the different species are hierarchically ordered by similarities in the FAME data. In the resulting top-down approach, all possible splits between species or classes are initially considered in the root node and, subsequently, the split corresponding to the highest RF accuracy is chosen. Recursively, the same splitting procedure is performed on the corresponding subsets of the initial data set. Since the data set consists of a small number of FAME profiles for the majority of species, we preferred to use a divisive clustering algorithm over an agglomerative clustering algorithm. The latter approach has as disadvantage that it builds a tree in a bottom-up manner, so that, in our setting, the clustering at leaf level could be obtained from the results of unreliable classifiers (due to a small number of FAME profiles for many species). Conversely, we chose to work top-down with a divisive clustering algorithm, because we wanted to avoid this type of instability in the tree construction phase. Initially, we performed a proof-of-concept experiment based on a small data set of 15 species, as selected from the original data set. Only species corresponding to a large number (at least 11) of FAME profiles were selected. About half of the selected species belong to the two known Bacillus species groups, the Bacillus cereus group and the Bacillus subtilis group. Hierarchical divisive clustering starts in the root node with the training and evaluation of 16383 RF classifiers. In subsequent steps of the clustering algorithm, classifier training becomes less time-consuming, because the number of trained classifiers decreases exponentially for the remaining subtrees. In the end, a total number of 18589 classifiers were trained. The computing time to build and evaluate the complete species hierarchy was 65 h 10 m 22 s. By this initial experiment, we evaluated whether a FAME tree constructed with divisive clustering indeed reveals the relations between the species of the different species groups. Figure 1 shows the resulting tree, in which no branch lengths are specified and AUC values of the RF classifiers are given at each internal node. The species representing the Bacillus cereus group or the Bacillus subtilis group are clearly clustered together under the same parent nodes. The two groups are coloured in blue and green, respectively.<br>
Consequently, one can conclude that FAME data allows to discriminate between both species groups. We did not expect such a result because of the large number of combinations and the high similarities between the FAME profiles. However, this experiment clearly shows that RFs take advantage of the relatedness between species and/or groups of species. Consequently, building a FAME tree using classification techniques as treeing method could be a good base for further knowledge integration. Therefore, we evaluated the tree constructed from the different RF models also as a hierarchical classification scheme. This classification task follows the main strategy as reported by [26-28]. Subsequent to the construction of the tree, a RF classifier is retrained at each node of the tree, so that now the different classifier parameters are optimized by a grid search, as mentioned in the subsection Divisive Clustering. To this end, we considered both 3-fold and 11-fold stratified cross-validation.<br>
The corresponding results are reported in the upper part of Table 1. These results show that hierarchical single-label multi-class classification with 3-fold stratified cross-validation performs slightly worse than flat multi-class classification (see bottom part of Table 1). Performing 11-fold stratified cross-validation, however, results in a slightly better performance than flat multi-class classification. In summary, for the 15 species data set, we can conclude that hierarchical single-label multi-class classification results in a performance comparable to that obtained with flat multi-class classification. Nonetheless, we are mainly interested in the classification of the 74 Bacillus species present in our data set. Upscaling this experiment from 15 classes to 74 is, however, computationally infeasible, because the number of classifiers to be trained increases exponentially with the number of classes. When considering these 74 classes in our FAME data set, 273 - 1 classifiers must be trained in the root node. This cannot be realized in a reasonable computing time, even when multiple processors are used in parallel. Furthermore, to obtain a good classification performance in the proof-of-concept experiment, we only selected species represented by a reasonable amount of data. Nonetheless, in the full data set, a lot of classes are present with a small number of FAME profiles (e.g. only 3 or 4 profiles) which may result in an unreliable FAME tree. Even though the results of this experiment with 15 species are promising, for the reasons above, we did not further consider knowledge integration by divisive clustering of FAME profiles.<br>
<br>
Phylogenetic learning<br>
An alternative to the construction of a FAME tree is to infer a tree on data with a good resolution for species discrimination. In this perspective, the best possibilities are DDH, whole-genome sequence analysis and multi-locus sequence analysis (MLSA). The lack of a sufficient amount of high-quality data, however, makes these techniques not very attractive. Therefore, yet another alternative is to focus on 16S rRNA gene sequencing. This technique is widely preferred for species delineation because of improved sequencing technology and the availability of public sequence databases. Nonetheless, the 16S rRNA gene may not allow for a delineation of every species [10,12-14]. Currently, 16S rRNA gene sequence analysis is one of the techniques widely used in microbiology for phylogenetic analysis. We integrated this knowledge in FAME-based bacterial species classification models to evaluate species identification as well as the resolution of FAME analysis for species discrimination within a phylogenetic framework.<br>
When using this technique as a starting point for knowledge integration, high quality 16S rRNA gene sequences can be exported from the <database>SILVA</database> database. This database subjects EMBL 16S rRNA gene sequences to different control procedures and annotates the corresponding sequences with quality scores [15]. In this way, we selected exactly one 16S rRNA gene sequence for each type strain of each Bacillus species present in the original FAME data set. Note that the type strain of a bacterial species is the fixed name bearer of the species (according to the bacterial code [32]) and its phylogenetic position is hence determinative in the taxonomic framework.<br>
After sequence selection, distance matrices were calculated using the Jukes-Cantor nucleotide evolution model and two phylogenetic trees were constructed accordingly, respectively with the neighbor joining method (NJ) and the unweighted pair group method with arithmetic mean (UPGMA) [33-37]. The respective trees are shown in Additional files 1 and 2. Subsequently, we used these binary trees as templates for hierarchical FAME-based species classification. As this hierarchical classification relies on a phylogenetic tree, we call this approach phylogenetic learning. As the binary tree classifier is based on a rooted tree structure, we initially selected the NJ and UPGMA methods as these basically infer rooted trees, even though several other tree inference methods exist (e.g. maximum parsimony and maximum likelihood) [36]. Two different methods were considered in order to allow for a comparison of binary tree classifiers based on different trees.<br>
The constructed RF classifiers were evaluated for distinguishing between the FAME patterns of the two underlying groups of species in every node of the tree. The collection of binary classifiers should be regarded as one classifier wrapping the multiple hierarchically structured classifiers. Three-fold stratified cross-validation for error estimation was performed during the training process of each classifier with pooling of the test results of all folds [38,39], i.e. the predictions on test data are pooled together in one big set, and the performance measures are calculated on this set. The results of phylogenetic learning based on the NJ and UPGMA trees are reported in the middle part of Table 1. These results are compared with those obtained from a FAME-based flat multi-class classification (see bottom part of Table 1), where only one multi-class classifier is trained by the same cross-validation strategy. First of all, we also evaluated phylogenetic learning and flat multi-class classification for the 15 species data set (as selected in the previous subsection). The corresponding results are reported in Table 1. Note that the flat multi-class classification with 3-fold stratified cross-validation in this study differs from the flat multi-class classification strategy performed in [5]. In the latter study, 10 repeated experiments were carried out with averaging of the classifier performance on a randomly sampled test set. Average AUC, sensitivity and precision were then given by 0.988, 0.847 and 0.908, respectively. These metric values are approximately equal to the values obtained in the present study. As a result, the cross-validation with pooled metric calculation in a flat multi-class setting does not lead to very different estimates of classification performance, when compared to the random test set selection carried out in [5].<br>
Even though flat multi-class classification of the 15 species data set results in a very high AUC value of 0.992, it is also interesting to see that higher sensitivity and F-score values are obtained by phylogenetic learning on this data set (based on the NJ tree). Conversely, phylogenetic learning based on a UPGMA tree performs slightly worse than flat multi-class classification. When the study is scaled up to 74 species, flat multi-class classification performs better than phylogenetic learning on both trees. For the NJ and UPGMA trees, the difference in sensitivity between both techniques and flat multi-class classification is 11% and 16.7%, respectively, while the difference in F-score is, respectively, 9.5% and 12.2%. The contrast between the two data sets is, logically, based on the larger number of relations between the different species and the more complex hierarchical structure of the data. The main reason for the lower prediction performance of phylogenetic learning can be attributed to the 16S rRNA gene phylogenetic trees that define the multiple learning tasks. These could become quite hard to solve when classifying the species based on FAME data. Flat multi-class classification is not confronted with these restrictions at all and allows for more flexible solutions. Moreover, in a 74 species hierarchical learning system, the probability of a misclassification along the identification path in the tree is much larger than the misclassification probability in a 15 species hierarchy. Also, in the 74 species data set, some species are known to be very closely related to each other, increasing the probability of misclassification in the hierarchy. Despite a lower classification performance compared to flat multi-class classification, phylogenetic learning allows to evaluate the classification scheme at node level. In this way, it is possible to analyze the resolution of FAME data at different tree levels. Ultimately, the goal of this approach will be to investigate how a particular pruning strategy could be applied by which those species will be grouped that are hard to classify by the machine learning method of interest. As a consequence, it will also become possible to report identification scores for groups of species that are very related in their FAME content.<br>
Further investigation could also be done on the improvement of classification performance. For instance, a variable misclassification cost could be defined along the classification path. As an example, nodes splitting groups of classes could be evaluated differently than nodes splitting one species from a group of classes and splitting two leaves. In the latter case, a more severe misclassification cost can be assigned. Another approach could account for the different branch lengths of the phylogenetic tree.<br>
As the multi-class classification problem is tackled by hierarchically structured binary classifiers, it is also interesting to look at the individual classes. As mentioned in the section Methods, a multi-class confusion matrix is generated by classification of each test profile and counting the different types of errors that are made. As such, this matrix is constructed from several two-class confusion matrices in a one-versus-all manner, in which, for each class, the positive class is the class under consideration, while the negative class corresponds to all other classes. Using the <software>iTol</software> webtool [40], we have plotted a bar diagram of sensitivity and F-score values along the tree and have aligned the corresponding bars with the corresponding leaf or class of the tree. When an F-score resulted in a value of ? (i.e. sensitivity and precision equal zero), no bar is visualized. In this way, rapid inspection is possible to detect those classes that are hard to identify by the phylogenetic learning model and the flat multi-class classifier.<br>
The results of phylogenetic learning with NJ and UPGMA trees and those of flat multi-class classification are displayed in Figures 2, 3 and 4. In case of flat multi-class classification, the metric values are displayed along the 16S rRNA gene NJ tree. When comparing the sensitivity values of each species obtained by phylogenetic learning based on the two considered 16S rRNA gene trees to those obtained by multi-class classification, only 15% of the species have a higher sensitivity value. 57% and 61% of the species have a lower sensitivity value, for the NJ and the UPGMA tree, respectively. In case of the F-score, 22% and 19% of the species have a higher F-score value, while 69% and 70% of the species have a lower sensitivity value, for both trees respectively. Nonetheless, when looking more deeply into the results, those classes that are hard to distinguish from the other classes in a multi-class classification setting are better identified in the hiearchical classification setting. This is clearly illustrated by the cumulative plot in Figure 5. In this figure, identification by phylogenetic learning is compared to flat multi-class identification at class level. Even though phylogenetic learning performs globally worse than flat multi-class classification, it is clear that, when considering a threshold of 0.5-0.6, phylogenetic learning has an added value due to better identification of classes that are not well identified by multi-class classification.<br>
As mentioned above, a hierarchical classification structure allows to analyze where misclassification occurs in the tree. This offers new possibilities to further analyze the resolution of FAME data for species discrimination. Furthermore, it is also interesting to calculate an average misclassification path length. The results for phylogenetic learning based on the NJ tree are visualized in Figure 6. The results for phylogenetic learning based on the UPGMA tree are similar and are visualized in Additional file 3. Herein, importantly, we only considered misclassified test profiles. It becomes clear from both figures that misclassification mostly occurs at nodes near the correct leaf. This is not very surprising as, based on FAME data, a lot of species cannot be distinguished from each other. This again shows that the resolution of FAME analysis is restricted to distantly related species and species groups.<br>
<br>
<br>
Conclusions<br>
In this work, we combined FAME data and 16S rRNA gene sequences in bacterial species classification models. Supervised machine learning techniques have shown their value in the past for distinguishing FAME profiles of different Bacillus species. However, bacterial species can be closely related in terms of FAME content, making it hard to achieve a high classification performance. With this study, we approached the classification problem from a taxonomic perspective, in which a hierarchical classification scheme with binary tree classifiers was adopted. This machine learning technique is a perfect method for the integration of taxonomic relationships between the different classes or species.<br>
Two strategies were followed with regard to tree inference. First, a FAME tree was constructed by divisive clustering, in which classifier performance was considered as splitting criterion. In this setting, one can see knowledge integration as the inclusion of similarities in the FAME profiles of the considered species. Due to a computational bottleneck, this approach was restricted to a proof-of-concept experiment with a data set of 15 species. Relatively good results were obtained, as closely related species could be retained out of the massive amount of computed clusters. Furthermore, for these 15 species, we observed a good classification performance that is comparable to flat multi-class classification.<br>
As a second strategy, we considered knowledge integration based on 16S rRNA gene data. Using quality controlled 16S rRNA gene sequences, two rooted phylogenetic trees were constructed for the type strains of 74 Bacillus species. The two trees, respectively constructed with the NJ and UPGMA treeing methods, subsequently served as taxonomies for a hierarchical classification scheme with base classifiers trained on FAME data at each node. Due to the integration of 16S rRNA gene as a phylogenetic marker, we called this approach phylogenetic learning. The main advantage of phylogenetic learning, when compared to flat multi-class classification, lies in the exploitation of the taxonomic relationships, so that the results can be easily visualized. Moreover, the results can be better interpreted in a post-processing phase, for example by pruning the classification tree based on the classifier performance along the tree. Given the limitations of FAME data for species discrimination, pruning allows to restrict identification to internal nodes and put the identification results in a taxonomic context. And, herein, a hierarchical classification scheme is a perfect choice, contrary to flat multi-class classification models, which cannot easily exploit taxonomic relationships, and do not produce a score of separability or identification of a certain species, as compared to (closely) related species.<br>
Furthermore, when evaluating the identification performance of phylogenetic learning at leaf level, another clear advantage is seen. Due to the hierarchical class structure, the phylogenetic learning approach improves the identification for species that are incorrectly classified by flat multi-class classification. So, in some sense, the method also results in a better performance, although we have to admit that phylogenetic learning with the NJ and UPGMA trees shows to be less accurate than flat multi-class classification, when evaluating the method as a global classification scheme. Yet, as explained above, this was not the goal of this study.<br>
<br>
Methods<br>
Fatty Acid Methyl Ester data<br>
The Bacillus fatty acid methyl ester (FAME) data set of Slabbinck et al. [5] was used. Basically, gas chromatographic FAME profiles were generated after growing the bacteria, as described in the protocol of the Sherlock Microbial Identification System of MIDI Inc. (Newark, DE, USA). This protocol defines a standard for growth and culturing of bacterial strains and reproducibility and interpretability of the profiles is only possible by working under the described conditions. Specifically, this protocol recommends 24 h of growth on trypticase soy broth agar at a temperature of 28?C. Subsequent to gas chromatographic analysis, the use of a calibration mix and the TSBA 50 peak naming table, the resulting FAME profiles are standardized by calculating relative peak areas for each named peak with respect to the total named area. Our data set covers 71 identified fatty acids (or features) and 74 validly published Bacillus species (or classes). This number is about one half of the Bacillus species published by IJSEM as of March 2008 [41]. The total number of FAME profiles (or data instances) is 961. The resulting data set was exported from the joint FAME database of both the Laboratory of Microbiology (Ghent University, Belgium) and the BCCM?/LMG Bacteria collection.<br>
<br>
Random Forests<br>
In 2001, Breiman [42] proposed a new machine learning technique consisting of an ensemble of classification trees, better known as Random Forests. A Random Forest (RF) classifier can be defined as a classifier consisting of a collection of decision trees, where each tree casts a unit vote for the most popular class at each input. Each tree in the forest is first grown using N training data points that are randomly sampled from the original data set, and subsequently evaluated by the remaining test data points. N equals about two-thirds of the size of the original data set. Importantly, the random sampling procedure is done with replacement, better known as bootstrapping. Aggregation of the classifiers built upon the bootstrap samples is called bagging or bootstrap aggregation. Bagging leads to a specific data set and the remaining data points are called 'out-of-bag'. When using the latter data set for evaluating the accuracy of the grown tree, the prediction error is, therefore, called the out-of-bag error. Randomly sampling data sets to grow the different trees of the forest corresponds to one of the two randomness factors of RFs. The second factor lies in the random split selection. When M features are present in the original data set, m features (m &lt;&lt;M) are sampled randomly out of M features to split each node of the tree. The final split is determined by the best split based on the m randomly sampled features [42,43]. Note that m is held constant during the growth process of the forest. Each tree is maximally extended and no pruning occurs. In our study, a grid search was performed to optimize the number of trees and the number of split variables. All numbers of features were considered for split variable selection and 1000 to 4000 trees in steps of 250 trees were selected for tuning the number of trees.<br>
Evaluation of the classification accuracy of a RF classifier is based on the different out-of-bag data sets. For each data point i of the data set, all out-of-bag data sets containing i are considered. Data point i is put down the trees corresponding to the respective out-of-bag data sets. Class j is set to be the class that got most votes every time data point i was out-of-bag. The proportion of times that j is not equal to the true class averaged over all data points corresponds to the out-of-bag error estimate. A RF classifier consisting of low-correlated trees with a high individual strength results in optimal generalization and a high accuracy. Moreover, from the law of large numbers and the tree structure, it follows that the generalization error of RFs converges to a certain value, implying that a RF classifier does not overfit, given a large number of trees in the forest [42]. However, Hastie et al. [44] remark that changing the number of trees in the forest does not cause overfitting, given that not all features are used as split variables and the number of noise variables is reduced.<br>
Modeling was performed by the <software>RFs</software> software available at the website of Leo Breiman [43].<br>
<br>
Divisive clustering<br>
A divisive clustering algorithm builds a top-down cluster hierarchy or tree, starting from a single cluster that corresponds to the entire dataset (i.e. the root of the tree). In every iteration of the algorithm, one cluster is selected and split into two new clusters. If the distances between all points in the clusters are considered, then this results in a traditional unsupervised clustering procedure. Conversely, supervised divisive clustering also takes the class labels of the respective data points into account, and calculates only distances between the data points with differing class labels. As a consequence, the final number of clusters in supervised clustering equals the number of classes present in the original data set [45,46].<br>
Popular divisive clustering strategies are single linkage, complete linkage, average linkage, Ward linkage, etc. In these strategies, multiple metrics can be applied as distance measure. In our initial study, for the construction of a tree using solely FAME data, we calculated the distance metric based on the performance of binary classifiers that were trained for all possible splits of the data. In this setting, cluster distances are not simply computed from FAME profiles, but also the class labels are taken into account. As first splitting criterion, the area under the ROC curve (AUC) was considered, and in case of ties, the splitting was refined by accounting for the average linkage of the probability estimates of both classes. The rationale behind this last splitting criterion is that the classifier corresponding to the largest average distance between the probability estimates should be preferred over other classifiers. The Euclidean distance was considered as metric on the probability estimates.<br>
For each level in the divisive top-down setting, a RF classifier is built for all possible two-group combinations of all considered species or classes. As such, all combinations correspond to a two-class classification task. For each node or level, this results in 2n-1 - 1 combinations, with n the number of classes considered. Note that, when considering four classes, the combination of classes 1 and 2 automatically excludes the combination of classes 3 and 4. The divisive clustering stops when only two-class clusters are retained. To speed up the divisive clustering and classification process, no grid search and no cross-validation were considered. Specifically, the initial FAME data set was randomly splitted in a stratified train and test set. One-third of the data set was used for testing. For each combination, the according profiles for training and testing were sampled from these two subsets. The forest size was optimized using the default number of split variables (m = , with M the number of features). After fixing the forest size resulting in the lowest error rate, the optimal number of split variables was selected among , 2M and , again corresponding to the lowest error rate. Ultimately, a rooted tree was constructed with equal branch lengths and the different nodes were labeled with the corresponding AUC value. The resulting tree was visualized with the treeing method of the <software>TaxonGap</software> software [47].<br>
As an initial proof-of-concept, 15 Bacillus species were selected from the original data set. Selection was based on classes with reasonable sample size and classes that are taxonomically closely related to each other, e.g. species of the Bacillus cereus and Bacillus subtilis groups. The first selection criterion was chosen to avoid heavily imbalanced data subsets. The following species with respective number of profiles were selected: Bacillus aquimaris (12), Bacillus atrophaeus*s (21), Bacillus cereus*c (62), Bacillus coagulans (32), Bacillus drentensis (38), Bacillus fumarioli (28), Bacillus galactosidilyticus (12), Bacillus licheniformis*s (74), Bacillus megaterium (28), Bacillus mycoides*c (11), Bacillus patagoniensis (12), Bacillus pumilus*s (57), Bacillus sporothermodurans (17), Bacillus subtilis*s (64) and Bacillus thuringiensis*c (12). Species annotated with '*c' belong to the Bacillus cereus group, while species annotated with '*s' belong to a species of the Bacillus subtilis group [41,48,49]. It is expected that the species of these two groups cluster together.<br>
To further speed up the clustering process, computations were performed in parallel on an Intel Blade cluster.<br>
<br>
Phylogenetic analysis<br>
The <database>SILVA</database> database was used for 16S rRNA gene sequence selection. <database>SILVA</database> provides quality checked and aligned 16S rRNA gene sequences. For each type strain of each species present in our data set, a 16S rRNA gene sequence was selected. If multiple 16S rRNA gene sequences for each type strain were available, selection of the final sequence was based on best quality and longest sequence length. In <database>SILVA</database>, quality is denoted three-fold: pintail quality for sequence anomaly detection, sequence quality and aligment quality [15]. A list of the selected accession numbers can be found in Additional file 4.<br>
Sequence distance calculation was performed by <software>PHYLIP</software>, the <software>PHYLogeny Inference Package</software> version 3.68, using the program <software>Dnadist</software> [35,36]. The Jukes Cantor evolution model was used for correcting the nucleotide distances. This DNA sequence evolution model assumes an equal and independent change rate for each nucleotide. So, substitution of one nucleotide by one of the three other nucleotides occurs with equal probability. All other parameters were used as default except for the input format. Based on the resulting distance matrix, a NJ and a UPGMA tree was created using the <software>PHYLIP</software> program <software>Neighbor</software> [33,34,37]. Default parameter settings were used, except for a randomized input order of the species. Phylogenetic trees were created for the species present in both the full data set and in the 15 species data set. All trees are visualized with the <software>iTol</software> webtool version 1.5 [40]<br>
<br>
Phylogenetic learning<br>
Based on the 16S rRNA gene phylogenetic trees, a classification scheme with a hierarchical class structure was developed. As such, a rooted phylogenetic tree can be regarded as a directed acyclic graph with bifurcating nodes. The main idea is similar to that of binary tree classifiers [26-28]. However, in contrast to our study, these authors inferred a tree from the data used for classification, while we considered phylogenetic information (16S rRNA gene) for tree inference and used FAME data solely for classification. We call this approach phylogenetic learning. As a simple, na?ve approach, at each node of the 16S rRNA gene phylogenetic tree, a two-class RF classifier was trained, based on a subset of the FAME data set. Herein, only the subset of profiles belonging to that part of the tree was taken for training and testing. The two branches of the node defined the two groups of the binary classification task, and at each node, a positive and negative dummy class label was created.<br>
Given the tree hierarchy, classifiers constructed on terminal nodes and a certain number of parent nodes can become biased due to a small training set size. Herein, terminal nodes are regarded as nodes splitting into two leaves. As a consequence of the small sample size for certain species, splitting the data set into a training set and a test set was not an option. We simply overcame this issue by using cross-validation techniques, as explained in the next paragraph. Furthermore, the classification performance could easily be evaluated, since each profile was presented to the classification hierarchy and its path was fixed. In the case of an incorrectly classified profile at a specific node in the tree, propagation along the true path stopped, and the corresponding profile was further identified along the predicted path. Therefore, the path and ultimate predicted class of each profile could be determined and a multi-class confusion matrix could be generated for statistical analysis (discussed in the next subsection). As an interesting feature, this method offers the possibility to investigate where misclassification mostly occurs along the phylogenetic tree. Hence, the misclassification distance for each species could be estimated by averaging the correct path length of each incorrectly classified profile. This implies that, for such a profile, the correct path length was incremented each time the corresponding classifier resulted in identification of the true branch.<br>
Incrementing continued until the considered profile was incorrectly classified. Note that the node resulting in misclassification also incremented the path length and that the path length was also incremented when ultimate identification occurred in the correct leaf. In the latter case, the correct path length equals the maximal path length. For each class, the average correct path length was plotted against the maximal path length. An example is visualized in Figure 6.<br>
<br>
Cross-validation and statistical analysis<br>
In machine learning studies, data sets are typically split in a training set and a test set, where the latter contains mostly one-third to one-half of the data set. This test set is used as hold-out set for estimation of the final classifier error rate. To prevent over- and underfitting, parameter optimization during the training phase should not be performed on the test set, but on a separate validation set [45]. To overcome this problem, cross-validation can be used. Herein, the training set is split in k separate validation sets for which k-fold cross-validation should be performed. In each step of the cross-validation, a different validation set is used to retrieve the optimal parameter combination, tuned during training on the k - 1 other validation sets [45]. In relation to our problem setting, two important issues should be taken into account. Our data set is not particularly large and some classes correspond to a (very) small number of profiles. Due to the small class sizes present in our data set, we chose to use the test set also as validation set, even though it is still better experimental practice to perform a cross-validation on the training set for larger sample sizes. In case of our FAME data set, two additional problems arise: classes are imbalanced, meaning that a different number of profiles is present for each species, and as mentioned above, many species contain only a small number of profiles. To tackle a possible imbalance effect on the classifier performance, the true error rate can be estimated by stratifying train and test sets [38]. For the second case, classification will also become problematic when two-class classifiers are created based on small data sets. This can be solved by performing cross-validation for performance estimation [39]. A three-fold stratified cross-validation was performed for both the hierarchical classification and flat multi-class classification. To prevent overfitting, the number of folds is set equal to the minimum number of profiles over all bacterial species, which is three in our case. In this perspective, the stratification proportion equals one-third. Given the identical nature of the probability estimates resulting from each RF model, we chose to aggregate all test sets in a joint test set for performance evaluation. This method is also better known as pooling [50]. Finally, for the pooled test set, an average of the error estimate for each class in a one-versus-all setting was calculated, next to the average and standard deviation of the error estimates over all classes. Statistics calculated were the AUC, accuracy, sensitivity, precision and F-score.<br>
Besides the calculation of global performance measures, the performance at class level between flat multi-class classification and phylogenetic learning was also compared. The comparison is visualized in a cumulative plot (see Figure 5). Initially, flat multi-class classification and the corresponding classification results of each class were considered. A threshold was set on a metric using steps of 0.01. As metric, sensitivity and F-score were further analyzed. The corresponding thresholds are plotted along the X-axis.<br>
For each threshold, those classes were selected corresponding to sensitivity or F-score values smaller than or equal to the threshold. Secondly, for each threshold and, thus, for each selected set of classes, the corresponding metric values obtained by phylogenetic learning were evaluated. The number of phylogenetic learning metric values that were larger than the corresponding metric values resulting from flat multi-class classification are plotted against the Y-axis on the left. Also, this number is expressed as a percentage of the corresponding class set size. The corresponding percentages are plotted against the Y-axis on the right.<br>
<br>
<br>
List of abbreviations<br>
AUC: Area under the ROC curve; DDH: DNA-DNA hybridization; FAME: Fatty acid methyl ester; MLSA: Multi-locus sequence analysis; NJ: Neighbor joining; RF: Random Forest; ROC: Receiver operating characteristic; UPGMA: Unweighted pair group method with arithmetic mean.<br>
<br>
Authors' contributions<br>
BS carried out the full study and drafted the manuscript. WW, BDB and PD participated in the classifier development and statistical analysis. PDV participated in the microbiological background of the work. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2850362</b><br>
<software>YODA</software>: Software to facilitate high-throughput analysis of chronological life span, growth rate, and survival in budding yeast<br>
<br>
<br>
Background<br>
The ability to accurately monitor survival and growth rate of cells is essential for many assays employed in studies of the budding yeast. Changes in growth rate and survival over time are often monitored in response to a chemical treatment, environmental change (e.g. temperature, starvation, etc.), or genetic variant. For example, the yeast ORF deletion collection, which consists of &gt;5000 unique single-gene deletion strains in an isogenic background, has been queried for more than 100 unique phenotypes by monitoring growth or viability under different conditions [1]. Growth rate (doubling time) of yeast cells can be quantified by monitoring the change in optical density at 600 nm (OD600) of a yeast culture under specified conditions. Survival of yeast cells has traditionally been quantified by plating the cells onto rich growth medium (yeast peptone dextrose, YPD) and counting colony forming units (CFUs) before and after treatment.<br>
One important assay that involves monitoring survival of yeast cells over time is measurement of chronological life span (CLS), which is defined as the length of time a yeast cell is able to maintain viability during post-diauxic growth arrest [2]. Yeast CLS has emerged as a useful paradigm in aging-related research and has led to the identification of several dozen genetic modifiers of yeast longevity, some of which play a conserved aging-related function in multicellular eukaryotes [3]. CLS has traditionally been performed by culturing cells in a synthetically defined medium and monitoring survival over time (every 2-3 days) by periodically removing an aliquot of the aging culture, serially diluting that aliquot, plating the cells onto YPD and counting CFUs [4].<br>
We have recently developed a high-throughput method of measuring CLS that involves quantifying survival based on measuring the outgrowth of a defined number of cells by monitoring OD420-580 using a combined shaker/incubator/plate reader, the Bioscreen C MBR machine (Growth Curves USA) [5,6]. This method is based on the fact that the optical density of a culture after a fixed period of growth will be proportionate to the number of viable cells present in the culture initially. We have used a qualitative version of this method in which only a single outgrowth optical density measurement was taken to screen the yeast ORF deletion collection for long-lived single-gene deletion strains [7], and more recently, the quantitative Bioscreen C MBR method has been used to quantify the effects of media composition on CLS, and to define a molecular mechanism of chronological aging in yeast [8,9]. Briefly, this method involves culturing cells in individual 5 mL cultures in tubes on a rotating drum at 30?C (the aging cultures). At each age-point (e.g. day 2, 4, 6, etc.), 5 ?L of each aging culture is inoculated into 145 ?L of YPD in one well of a Bioscreen Honeycomb plate. Outgrowth of the cells from each aging culture is then determined by taking OD420-580 measurements for each well every 30 minutes for 24 hours using the Bioscreen C MBR machine. The Bioscreen C MBR machine has a maximum capacity of two 100-well Honeycomb plates, allowing for simultaneous measurement of outgrowth for up to 200 individual aging cultures. Survival at each age-point is determined by quantifying the rightward shift of the outgrowth curve along the time axis for each aging culture relative to the outgrowth curve for the aging culture at the initial age-point (day 2 of culture) using the formula:<br>
where sn is the survival percentage, ?tn is the time shift, and ?n is the doubling time. Detailed methodology and a video protocol describing the CLS assay have been published, and we refer the interested reader to these references [5,6].<br>
In order to automate analysis of the data generated during a CLS experiment, we developed a software package called <software>YODA</software>, the <software>Yeast Outgrowth Data Analyzer</software>. <software>YODA</software> accepts as input single or multiple text files containing OD values as a function of time (one file for each age-point) and returns several useful parameters, including maximal growth rate for each well, survival at each age-point for each aging culture, and the survival integral (SI) for each strain, which is defined as the area under the survival curve. <software>YODA</software> also has the capacity to group data from replicate cultures and perform simple statistical analyses of each group of replicates both individually and relative to experiment-matched control replicates. <software>YODA</software> is provided to the scientific community as a freely available utility on our website at http://www.kaeberleinlab.org/yoda and on the SAGEWEB website at http://www.sageweb.org/yoda. We have also provided the <software>YODA</software> source code for download along with an issue tracker at http://code.google.com/p/sageweb-yoda/.<br>
Here we describe the key features of <software>YODA</software> and provide a demonstration of how <software>YODA</software> can be used to analyze data from a typical CLS experiment. We also provide examples of how <software>YODA</software> can be used for additional types of experiments in yeast where quantitation of cell survival or growth rate are desired and describe how data generated from sources other than a Bioscreen C MBR machine can be analyzed with <software>YODA</software>.<br>
<br>
Implementation<br>
Uploading outgrowth data from a CLS experiment<br>
Detailed instructions for performing chronological aging experiments using the Bioscreen C MBR machine are published [5,6] and are also available on our website. At each age-point, the Bioscreen "<software>EZExperiment</software>" software produces a comma-delimited file containing OD420-580 values for each of the 200 wells corresponding to the maximal loading capacity of the machine (2 ? 100 well plates). The first column of the resulting table contains the "Time" data (30 minute intervals by our protocol [5,6], although this can be varied) and each subsequent column contains the data for a single well. In order to upload your data into <software>YODA</software>, the outgrowth data for each age-point should be saved in a separate comma-delimited (.csv) or <fileFormat>Microsoft Excel</fileFormat> (.xls) file. It is important that well position be maintained for each aging culture throughout the entire experiment (e.g. wild type replicate #1 always in well #1, mutant A replicate #1 always in well #2, wild type replicate #2 always in well #3, etc.). This is ensured by loading the wells in the same order at each age-point. Save each file with an appropriate identifying name, such as 'name_date.csv'.<br>
Once your files are formatted and named appropriately, they can be uploaded to <software>YODA</software> by choosing the 'Upload' link in the Analyzer Menu. This will display the Upload Experiment form (Fig. 1). Fill in the experiment name, start date, and description of your experiment. If you want to use named wells for grouping, fill in the well info input (click the question mark next to the Well Info text box on the Upload Experiment form for detailed information about the format).<br>
Upload each of your age-point outgrowth data files. Assign each run a day (for example, corresponding to the age of the culture at that age-point) so that <software>YODA</software> can determine which point in the survival curve belongs to which run (survival curve integrals automatically take into consideration days between runs). If you have named your wells using the well info input, type the name of the media-only well in the background field to compute a background reading for the run. The background reading defines the optical density value for a well with media only. Otherwise, select 'value' from the background type drop-down and type in a custom background (the default is 0.15, which is approximately the OD reading for 150 ?L of YPD in the Bioscreen). When all of the data files for the experiment have been selected, click the 'Upload' button. <software>YODA</software> will tell you what the computed backgrounds are and if any errors have occurred in the results window (Fig. 2).<br>
<br>
Quantifying growth rate and survival with <software>YODA</software><br>
To analyze your CLS data and export it for graphical presentation, click the "Export" link in the Analyzer Menu. This will display the Export form (Fig. 3). Select the desired experiment, data sets (age-points), and well positions. <software>YODA</software>'s default settings will select all data sets and well positions (1-200) for a particular experiment, unless otherwise selected.<br>
Under the Export window, set the "output" drop-down to either "runs" or "lineages" (Fig. 4). By default, selecting 'runs' will output well name, run name, run day, doubling times, and background normalized OD420-580 readings (click the "show options" link to adjust what parameters are exported and how they are calculated. See Table 1). To export the OD420-580 readings for a single well position across all data sets for a given experiment, select just the desired well in the select box and set the output drop-down to 'runs'. Selecting "lineages" will output doubling time, survival values at each age-point, and survival integrals for each well across age-points. If replicates have been grouped using the "Well Info" option, the average or median values for a well group can be output by selecting the desired option under "grouping". In order to calculate survival values at each age-point, the first age-point is defined as maximal survival (1.0) and subsequent survival values are calculated as the relative fraction surviving at each age-point. To graphically present survival curves from <software>YODA</software>, plot the fractional survival (y-axis) as a function of age (x-axis) for each well or replicate.<br>
<br>
Using <software>YODA</software> for non-aging assays<br>
<software>YODA</software> can be easily adapted for analysis of data for a variety of assays in which growth rate or survival is the parameter of interest. To quantify growth rate from OD420-580 readings obtained with a Bioscreen C MBR machine, simply upload a single comma-delimited "<software>EZExperiment</software>" output file as described above and export the data using the "runs" option from the "output" drop-down menu under the Export window. Quantification of survival in response to different experimental stimuli can be performed in a manner analogous to that described for CLS above, with the control treatment assigned as the first age-point in the CLS experiment and each experimental group assigned as a subsequent age-point. Specific examples are provided in the Results section below.<br>
<br>
Using <software>YODA</software> with alternative methods of optical density determination<br>
Although <software>YODA</software> is designed for simplified analysis of OD data obtained from the Bioscreen C MBR machine, OD measurements obtained from alternative sources, such as a standard plate reader or spectrophotometer, can also be analyzed with <software>YODA</software>. For such uses, simply ensure that the OD data is entered into .csv or .xls files in the manner described above for data obtained from the Bioscreen machine. All subsequent steps in the analysis are identical.<br>
<br>
<br>
Results<br>
Quantitation of yeast CLS with <software>YODA</software><br>
In this exemplary yeast CLS experiment, the aging potential of a control and four single-gene deletion mutant strains was determined using a Bioscreen C MBR machine. Outgrowth analysis was performed as described above by inoculating 5 ?L of each aging culture into 145 ?L of YPD in individual wells of a Bioscreen Honeycomb plate. Each strain was assayed in triplicate using biological replicates (3 independently derived aging cultures per genotype). Outgrowth was determined at the following age-points: days 2, 4, 6, 9, 11, and 13. Outgrowth data for each age-point was retrieved as comma-delimited text files from the <software>EZExperiment</software> software (Additional File 1) and uploaded to <software>YODA</software> as described above. 'Average groups' was selected under the 'Grouping' options in the Export screen of <software>YODA</software> and the data was exported as a <fileFormat>Microsoft Excel Worksheet</fileFormat>. Survival curves were plotted from the data generated by <software>YODA</software> along with the standard deviation at each age-point for each strain (Fig. 5).<br>
<br>
Quantitation of yeast survival following heat shock<br>
In addition to determining the CLS of yeast strains, <software>YODA</software> can also be used to quantify cell viability following an environmental stressor such as heat shock. As an example experiment, an overnight culture of wild type cells was split into 100 ?L aliquots and subjected to varying lengths of 55?C heat shock (0, 5, 10, 15, and 20 minutes). Following heat shock, 5 ?L of each treatment was inoculated into 145 ?L YPD in individual wells of a Bioscreen Honeycomb plate. Each treatment was assayed in triplicate.<br>
To quantify viability following heat shock, the data output by the Bioscreen was split into individual comma-delimited (.csv) files, based on the length of time the cells were subjected to heat shock. For example, the first three wells of the Bioscreen plate were inoculated with cells subjected to 0 minutes of heat shock, the fourth through sixth wells were inoculated with cells subjected to 5 minutes of heat shock, and so forth. The data from the first three wells were put into a new comma-delimited file along with the data from one blank well; the data from the fourth through sixth wells were put into a second comma delimited file along with the data from the same blank well, and so on until all treatments were accounted for. Once each treatment had been placed into a separate .csv file, all files were then uploaded to <software>YODA</software> (see Additional File 2, for the original Bioscreen output file and the modified .csv files loaded into <software>YODA</software>). In the 'Upload Experiment' form, "0 minutes, 5 minutes, 10 minutes, etc." were filled in the 'Day' field next to the corresponding .csv files. 'Average groups' was selected under the 'Grouping' options in the Export screen of <software>YODA</software> and the data was exported as a <fileFormat>Microsoft Excel Worksheet</fileFormat>. Outgrowth and survival curves were generated from two independent experiments (Fig. 6). For comparison, viability following heat shock was also quantified using the traditional labor-intensive method of plating cells onto solid media following serial dilution and counting CFUs. CFU counts showed a similar loss in viability due to heat shock as was observed using the Bioscreen.<br>
<br>
Quantitation of growth inhibition by rapamycin<br>
As an alternative to survival, it is often desirable to monitor changes in growth kinetics in response to a stress or stimulus. To demonstrate how to use <software>YODA</software> for these types of assays, we examined the response of wild type, tor1?, and fpr1? cells to rapamycin, a chemical inhibitor of the target of rapamycin kinase and a putative anti-aging compound [10,11]. For this experiment, cells were grown overnight in YPD and 5 ?L of each culture was inoculated into either 145 ?L of YPD or YPD containing 10 ng/mL rapamycin in individual Honeycomb plate wells. Each strain was assayed using five technical replicates. Outgrowth data was obtained using the Bioscreen C MBR machine, as described above. The resulting comma-delimited text file was uploaded to <software>YODA</software> with 'average groups' selected under 'grouping' as well as 'media' under the 'append properties' category. Growth curves were plotted using the data generated by <software>YODA</software> in <software>Microsoft Excel</software> (Fig. 7). As predicted from previously published data [12], the tor1? strain was sensitive to rapamycin, while the fpr1? strain was resistant.<br>
<br>
<br>
Conclusions<br>
<software>YODA</software> provides a web-based platform for quantifying survival and growth inhibition in the budding yeast. <software>YODA</software> was designed to facilitate high-throughput studies of yeast CLS, but is equally suitable for identifying genetic variants or environmental interventions that modify cell survival or growth rate. In addition to providing information not attainable using the traditional CFU method, such as growth rate of each strain, the use of <software>YODA</software> with a machine such as the Bioscreen C MBR enhances efficiency in the laboratory. For example, the method described here can accommodate up to three biological replicates of 66 strains in a single experiment (198 out of 200 wells used). We estimate that an entire experiment of this nature requires approximately 4 hours of effort for an experienced researcher, including preparation time, strain handling, and data analysis. In contrast, we estimate that the traditional CFU method with serial dilutions would require at least 15-fold longer for the same researcher to obtain equivalent data. In terms of resources, both methods require equal volumes of media for aging cultures; however, the method described here requires only 30 mL of liquid media and two multi-well plates per age point, whereas the traditional CFU method requires approximately 5 L of agar-based media and 198 Petri dishes per age-point (25 mL per plate ? 198 strains). Thus we conclude that <software>YODA</software> in combination with the methods described here results in a significant reduction in time and resources required to measure CLS or perform other survival-based assays in yeast.<br>
<br>
Availability and Requirements<br>
<software>YODA</software> is provided to the scientific community as a freely available utility on our website at http://www.kaeberleinlab.org/yoda and on the SAGEWEB website at http://www.sageweb.org/yoda. <software>YODA</software> can be used anonymously without a username and password. We have also provided the <software>YODA</software> source code for download along with an issue tracker at http://code.google.com/p/sageweb-yoda/. Installation requires a web server with PHP 5.2 support and <software>MySQL</software> 5.1 or above.<br>
<br>
Authors' contributions<br>
BO, CJM, and MK conceived of the project. MK wrote the manuscript. BO developed the software. CJM performed the yeast experiments. All authors read and approved the final manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2873534</b><br>
<software>JISTIC</software>: Identification of Significant Targets in Cancer<br>
<br>
<br>
Background<br>
A comprehensive study of the genomic alterations that occur in cancer is vital for understanding this disease. Technological advances have made it possible to detect chromosomal regions of amplifications and deletions genome-wide and at high resolution. Datasets measuring such aberrations in patient tumors are accumulating at a staggering rate for multiple types of cancer [1-4]<br>
However, tumors harbor a great number of copy number alterations and it is difficult to distinguish between driver aberrations (functional changes vital for cancer progression) and passenger aberrations (random and with no selective advantage). Thus, the distinction between driver and passenger mutations has become one of the key challenges in cancer genomics. A very successful algorithm to address this is "Genomic Identification of Significant Targets in Cancer (GISTIC)" [1], that identifies aberrant regions more likely to drive cancer pathogenesis. GISTIC calculates the background rate of random chromosomal aberrations and identifies those regions that are aberrant more often than would be expected by chance, with greater weight given to high amplitude events that are less likely to represent random aberrations. There are other algorithms that tackle this task such as GLAD [5], RAE [6] and STAC [6]. However, GISTIC is unique in its ability to combine magnitude and frequency of the alteration into a statistical score. This algorithm has been successfully applied to various datasets [2,4,7,8] and the approach is becoming widespread.<br>
GISTIC identifies those regions of the genome that are aberrant more often than would be expected by chance. While successful in most scenarios, GISTIC has trouble identifying the relevant sub-region when a very large region is amplified or deleted. Such large chromosomal aberrations frequently occur in cancer and this leaves the user with two less than optimal options - consider only a single peak within the region, or consider an entire chromosomal arm. However, we have observed that in many cases there are other small sub-regions for which the aberration is significantly stronger than in the rest of the large region. Moreover, these regions often contain known oncogenes.<br>
To address this issue, we developed <software>JISTIC</software>, a tool that implements all of GISTIC's capabilities, with an additional new variant of the algorithm capable of detecting multiple significant sub-regions within large aberrant regions.<br>
<br>
Implementation<br>
<software>JISTIC</software> is based on the GISTIC algorithm [1]. <software>JISTIC</software> implements the previously published variants of GISTIC (standard, focal and arm-peel-off) and can also deal with LOH in the same way that the original algorithm does. More detailed information on GISTIC can be found in the Supporting Information of [1].<br>
In brief, GISTIC calculates a statistic (G-score) which represents the strength of the aberration for each marker. The G-score for a marker m is the summation of the copy number across all samples. For this summation, the samples in which the copy number is less significant than an empirical aberration threshold (?AMP for amplification, ?DEL for deletion) is set to zero. Therefore, the G-score in the case of amplifications is:<br>
Where I(x) is the indicator function and CN(m, i) the copy number for marker m and sample i. The score is defined similarly for deletions, taking into account the change in sign. While standard GISTIC uses a fixed aberration threshold for each type of aberration, focal GISTIC uses sample-specific high-level thresholds. This threshold is set for each sample by adding the standard threshold to the maximum (minimum for deletions) of medians observed for each chromosome arm.<br>
GISTIC assesses the significance of the G-score for each marker by comparing this score to results expected by chance using genome-wide permutation testing. This significance is then corrected using False Discovery Rate (FDR) using Benjamini and Hochberg method [9], and a q-value is obtained. All regions with a q-value below a threshold (0.25 in previously published articles) are deemed significant. For large aberrations, the sub-region with a minimal q-value is identified as a peak driver region.<br>
To identify independent peaks within a region and discard spurious peaks, GISTIC uses a peel-off algorithm. The algorithm iteratively finds the most significant peak and then, for each sample, if the peak is included in the aberration, it sets to zero all markers consecutive with the peak, thus removing all aberrations overlapping the peak. Once a peak has been detected, the peak region is extended by leaving each sample out in turn, and recalculating the peak boundaries.<br>
Typical to cancer genomes is the presence of broad copy number aberrations, defined as aberrant regions at least as large as one half of a chromosomal arm [1]. Peel-off on broad regions using standard GISTIC usually results in identifying only a single peak. GISTIC also has a focal variant of the algorithm which can potentially capture more aberrations, but misses many peaks in practice.<br>
The crux of the matter is to distinguish between genuine multiple peaks and a single peak within noisy fluctuations of microarray intensities. Focal peel-off is designed to deal with this issue, but since the focal threshold is defined according to the highest broad aberration genome-wide, this variant not only requires the aberration to have a strong focal behavior, but also depends on the strength of other broad aberrations across the genome. In Figure 1 we demonstrate how different thresholds (determined by broad aberrations in other chromosomes not shown in the figure) can lead to either detection or failure to detect a second peak. In this example, standard GISTIC only captures a single peak and misses a second peak that seems equally significant, with just a slight difference in G-score. In this type of situations standard GISTIC is doomed to fail. The detection of the second peak with focal GISTIC highly depends on the focal threshold. This threshold is set for each sample by the highest broad aberration genome-wide and there are cases, such as C in Figure 1, in which the focal events that define the aberration are masked by broad aberrations in other chromosome (not shown in Figure). In the next section we will see that this problem is prevalent in real datasets.<br>
For <software>JISTIC</software>, we developed a new variant called limited peel-off, designed to tackle the problem posed in Figure 1. While GISTIC sets to zero any aberration containing the peak, our algorithm "peels off" only part of the aberration. Limited peel-off uses the global G-score to determine the extent of "peel-off. Given a single aberration, we expect G(m) would decrease as we get further from it until it reaches the noise level. The idea behind limited peel-off is to decompose the G-score for a marker in two parts, one that represents what remains from the peak that we are peeling off (Gr(m)) and another that depicts contribution independent of the peak (Gn(m)). We use a threshold value (Gthres) to estimate whether Gn(m) contains only noise or the aberration is likely due to an independent peak.<br>
Denote G(m, i) as the G-score contribution of sample i in marker m, which can be defined as:<br>
For each sample i, the algorithm iteratively calculates on both sides of a newly discovered peak the magnitude Gr(m, i). For the right side of the peak:<br>
the left side of the peak is the symmetric equation:<br>
Gr(m, i) represents the amount of aberration that remains from the primary peak in marker m. JISTIC considers any reduction in the aberration that is consistent for at least a minimal number of markers, s. This parameter was introduced after observing that transient fluctuations in copy number result in a greater number of false positives. For s = 0 a total of 147 peaks are detected with limited peel-off. However, many of those peaks are located next to each other in the genome and the biological assessment suggested that those were spurious peaks. The number of reported peaks decreases when the parameter s increases, eliminating most of those spurious peaks.<br>
In order to determine whether a new peak exists, we need the complementary component of Gr(m, i), Gn(m, i), measuring the component of the aberration that is independent from the primary peak, for each sample:<br>
The condition that <software>JISTIC</software> checks in order to abort the peel-off at a marker m is<br>
The results critically depend on the threshold Gthres. If Gthres is too high, the algorithm will obtain exactly the same result than the standard variant of GISTIC. If the value is too low, the algorithm will obtain many spurious peaks. For consistency, we use the G-score value that corresponds to the threshold q-value when running the focal variant, thus using the same threshold both for a peak on baseline copy number and a sub-peak within a broader region.<br>
Figure 2 shows how limited peel-off successfully detects the second peak in the example from Figure 1. Contrary to the focal variant, this success is independent of broad regions in other chromosomes. It is important to note that limited peel-off fully replaces the standard variant, as any peak captured by the latter, will also be captured by the former. On the other hand, focal and limited peel-off can be considered complementary. While limited peel-off looks at the average of the aberration across samples, focal concentrates on the most acute aberrations for each sample.<br>
<br>
Results<br>
We tested <software>JISTIC</software> on 178 glioblastoma samples that had been previously used to test the latest version of GISTIC [10]. This dataset was generated by Harvard Medical School using Agilent 244 K Arrays.<br>
The parameter s was tuned using the histogram of segments sizes to obtain candidate values and the optimal value was chosen based on the distribution of minimum distance between peaks. In the test dataset, the histogram of segment size showed that the number of segments decreases with the segment size, as it was expected. We observed a stronger decrease in the number of segments for 3 different segment sizes (3, 8 and 12); those sizes were considered as candidate values for s. The distribution of minimum distance between peaks was used to estimate the number of spurious peaks for each of those values. This distribution shows a considerable change when s was increased to 8, as the number of adjacent peaks decreased. The distribution did not show any significant change when s was increased to 12 and we adopted a conservative approach by setting s = 10, which corresponds to 126.87 kb on average. Limited peel-off detected 124 peaks using s = 10.<br>
Table 1 shows the peaks and genes obtained with <software>JISTIC</software> in the three variants: standard, focal and limited peel-off. Note that standard and focal are equivalent to implementations of the GISTIC algorithm [10]. When looking at peaks by chromosome (Table 2), we observe that most of the novel peaks appear in chromosomes with a broad region for which standard detects only a single peak and focal detects one or no peaks.<br>
Figure 3 demonstrates an example of such a broad peak in chromosome 19. Standard (A) captures only a single peak and focal (B) fails to capture any aberration. On the other hand, limited peel-off (C) successfully captures multiple relevant regions. The ultimate assessment is whether the additional peaks identified are indeed drivers. To evaluate this we assessed whether the new peaks in chromosome 19 contained solid candidates for oncogenes. Standard GISTIC identified a single peak containing the gene (ZNF626), whose role is unknown. In comparison, limited peel-off detected 7 additional regions, where each includes at least one oncogene and multiple genes related to cell replication. Examples of oncogenes include AP1 M2 [11], Cyclin E1 [12], LYPD3 [13], see Table 3 for list of potential oncogenes in each peak.<br>
Figure 4 shows another example, a deletion of the entire chromosome 22. Standard finds a single peak and focal finds no peaks, whereas limited peel-off finds 3 additional peaks. The peak identified by standard includes FBLN1, a gene that has been previously selected as candidate tumor suppressor [14]. Each additional peak identified by limited peek-off has at least one tumor suppressor, such as SMARCB1 [15], HIRA [16] and LARGE [17], see Table 4 for list of potential tumor suppressors in each peak.<br>
For a systematic evaluation we designed an automated statistical test to study the limited peel-off's specificity and establish that the increased number of peaks detected by limited peel-off does not increase the false positive rate. Our permutation based test estimates the number of candidate cancer driver genes expected in random regions matching in size with the regions detected by <software>JISTIC</software>. We compiled a list of 1880 likely driver genes based on two different sources:<br>
? Genes listed in the <database>Sanger Cancer Gene Census</database> [18].<br>
? Genes annotated in <database>GO</database> [19] for at least one of the following processes: cell cycle, cell proliferation, cell death and neurogenesis.<br>
We generated 10,000 random outputs with similar characteristics to the real output. A random output is generated by iterating over all detected peaks and shifting the peak's location to a random position in the genome. For each output, we count the number of regions containing at least one gene in the list of driver candidates above. This provides us with a distribution for the number of potential driver genes expected in the null model.<br>
To compare focal and limited peel-off, we tested three sets of peaks: the peaks that are exclusive to each of the two methods and the peaks that are common to both. Note that all the peaks obtained by standard GISTIC are also obtained by limited peel-off. The results of the 10,000 iterations for each of the three sets are shown in Table 5.<br>
As expected, the p-value obtained for the overlapping peaks is more significant than the p-value for the exclusive peaks for each method.<br>
For the 78 peaks obtained exclusively by limited peel-off, 25 contained likely driver genes, only 23/10,000 permutations reached the value obtained by the true output, giving a p-value of 0.0023. In comparison, for exclusive peaks for the focal variant, 9 of the 49 peaks contained likely driver genes. In 73/10,000 permutations the test obtained as many peaks with likely driver genes, giving a p-value of 0.0073.<br>
<br>
Conclusions<br>
The analysis performed on limited peel-off against standard and focal GISTIC demonstrates the superiority of limited peel-off to achieve both better specificity and dramatically increase recall by obtaining a large number of novel peaks.<br>
In conclusion, <software>JISTIC</software> is a significantly improved algorithm to distinguish between driver and passenger copy number aberrations in cancer genomes. Importantly, it detects a significant number of additional driver regions while maintaining a similar false positive rate. We conclude that both limited peel-off and focal GISTIC should be used together as they provide complementary and significant results.<br>
The tool is implemented in Java, has been tested on Linux and Windows. It can be downloaded from http://www.c2b2.columbia.edu/danapeerlab/html/software.html. It does not have dependencies to external libraries and can be downloaded as a single Java JAR file. The execution time for the glioblastoma dataset on a standard desktop computer (Intel Xeon W3505 @ 2.53 GHz, 3GB of RAM) was 8 minutes for all variants.<br>
<software>Matlab</software> scripts are provided in order to visualize the output and obtain different statistics. The tool can also convert to the format used by the open-source visualization tool <software>IGV</software> [20], used to display cancer genomic data using a user-friendly interface, demonstrated in Figure 5.<br>
<br>
Availability and Requirements<br>
? Project name: <software>JISTIC</software><br>
? Project home page: http://www.c2b2.columbia.edu/danapeerlab/html/software.html<br>
? Operating system(s): Platform independent<br>
? Programming language: Java<br>
? Other requirements: Java 1.5 or higher<br>
? License: LGPL<br>
? Any restrictions to use by non-academics: None<br>
<br>
Conflict of interests<br>
The authors declare that they have no competing interests.<br>
<br>
Authors' contributions<br>
DP, FSG and UDA designed research. FSG and UDA developed the limited-peel off algorithm EM and FSG implemented the JAVA code. FSG and UDA analyzed and evaluated the algorithm. FSG and DP wrote the manuscript. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2911457</b><br>
<software>CGHpower</software>: exploring sample size calculations for chromosomal copy number experiments<br>
<br>
<br>
Background<br>
Array comparative genomic hybridization (aCGH) is a technique that uses microarrays to perform high-resolution and genome-wide screening of DNA copy number changes. Its most important applications are in cancer research [1] and clinical genetics [2]. In this paper we focus on aCGH experiments comparing two groups of cancer samples. Previously, we introduced the Wilcoxon test with ties to identify chromosomal copy number differences when comparing two groups [3]. The goal of comparing two groups is generally to identify disease biomarkers, chromosomal regions (or genes therein) for survival, therapy, progression, et cetera. An important problem that arises in the planning of aCGH experiments is the choice of the sample size, which we explore here. Data analysis of microarray experiments comparing two groups generally involves calculating a test statistic for each array element and setting a cutoff for rejecting the null hypothesis of no difference between the groups. With a single array element, there are therefore two typical errors that can occur in the process. A type I error occurs when the null hypothesis is rejected even though it was actually true and the cut-off was exceeded only by chance. A type II error involves accepting a null hypothesis that should have been rejected, thus failing to identify a true difference. To broaden the perspective from individual array elements to the framework of multiple testing covering the entire microarray, two concepts are used: false discovery rate (FDR) [4] and average power. FDR is the expected percentage of discoveries that are false. Statistical power is the probability of recognizing a single array element with a true difference, and average power refers to the expected percentage of true positives that is identified. In general, it is desirable to have the FDR as close to zero and average power as close to one as possible. Setting the cut-off for rejecting the null hypothesis is a delicate balance between sensitivity and specificity; while a stringent cut-off lowers the FDR, it also lowers average power and vice versa.The only way to improve both, or one without affecting the other, is to increase the number of biological replicates and thus perform more arrays. Sample size calculations can generally be divided into two categories. The first category asks the user to define values for certain parameters, such as the effect size (fold change of a differentially expressed gene) and the proportion of genes that are truly differentially expressed [5-9]. The second category estimates these parameters from existing data [10,11]. The method proposed here follows the latter approach and therefore requires pilot data.<br>
To adapt mRNA expression array power calculations for aCGH and copy number changes, two key aspects need to be taken into account. Instead of concentrations of individual mRNA molecules, the underlying biology measured by aCGH consists of blocks of chromosomal DNA. Each block is (presumably) present in a normal copy number of two, but may contain areas of one or two-copy losses and one or more gains. Higher level amplifications can also be present. The aberrations contain both driver and passenger genes, and the breakpoints may vary from one sample to another.<br>
As the entity being measured is DNA present in a discrete number of copies (0, 1, 2, 3, 4, ...), but individual array elements yield log2 ratios, aCGH data preprocessing generally involves the following steps that aim to better capture the biological relevance. Normalization first removes technical artifacts and makes the log2 ratios comparable across different hybridizations. Segmentation then identifies areas that share a common copy number and are separated by breakpoints. Finally, calling determines a discrete copy number level for each segment. At the moment, there is no clear consensus regarding the optimal stage of preprocessing from which the data should be used for downstream analysis. We discussed the topic and proposed that in most cases the recommended choice be to use calls, which have the clear advantage of having an attached biological meaning [12]. For power calculations however, the use of calls is problematic, as it would require the use of the chi-square test, for which no method of sample size calculation in large FDR-based multiple testing contexts is presently available. While both normalized and segmented log ratios allow the use of a t-test, they fail to take full advantage of the adjacency of consecutive array elements. Aberrations typically show great variation in their sizes ranging from focal amplifications to gains and losses of entire chromosome arms. Working directly with the original array elements does not take this into account, and gives larger aberrations significantly more weight than smaller ones as they contain more array elements. A possible improvement is therefore to replace array elements with regions, which are defined as a series of neighboring array elements sharing the same copy number signature. This reduces dimensionality with little loss of information [13]. Throughout this paper, the term regions is used to refer to the results of this analysis step.<br>
For <software>CGHpower</software>, we are combining the advantages of regions with the feasibility of log ratios, by replacing the hard calls with median log ratios of all the array elements within a region. Together with these region-wise log ratios (RWLRs), the regions are then taken as a representation of the underlying biology (i.e. chromosomal regions with varying copy number levels). Each region is coupled to a null hypothesis stating that the means of the two groups do not differ from each other, which is the framework required for the power calculations proposed here. Regions that have a true difference between the two groups (generally normal copy number in one group and a gain, loss or amplification in the other) will be referred to as "differentially behaving regions".<br>
After this preprocessing, power calculations are performed using regions as Ferreira et al. [14] previously described for both real and simulated gene expression data. T-statistics and p-values are calculated for each region from the RWLRs. All p-values from non-differentially behaving regions are expected to follow a uniform distribution, while those from the differentially behaving ones should follow another, unknown distribution (G). Two separate estimators of G are calculated: a non-parametric () and a parametric one (?n), which assumes that G follows a normal distribution. Both of these estimators depend on another unknown parameter, ?, which is the proportion of non-differentially behaving regions. When the estimate of ? used to calculate ?n and  moves away from its true value, the difference between the two G estimators increases. The estimate of ? is therefore chosen so that this difference is minimized. The limiting density of effect sizes (?) is then estimated using deconvolution, and so is G. Once these estimates have been calculated, approximate sample size calculations can be made using an adaptive version of the Benjamini-Hochberg method for multiple testing. While the original method [4] allows control over the FDR, the adaptive version also allows the estimation of average power [10].<br>
While optimizing the protocol, there were certain options that we considered: whether to calculate the RWLRs as the mean or median of the log ratios, whether to use the Student's t-test assuming equal variances or Welch's t-test that allows unequal variances, and finally whether to calculate the p-values from normal or Student's t-distribution. All of the possible combinations were tested, and the optimum performance was observed with median log ratios, unequal variances and the normal distribution. These choices were then fixed in <software>CGHpower</software>.<br>
<br>
Implementation<br>
Evaluation Data Sets<br>
To evaluate the performance of <software>CGHpower</software>, eight recently published aCGH data sets that could be divided into two groups were collected. They will be referred to as Chin et al. [15], Douglas et al. [16], Fridlyand et al. [17], Myllykangas et al. [18], Nymark et al. [19], Postma et al. [20], Smeets et al. [21] and Wrage et al. [22] A total of five different array types were used among the data sets: VUmc 30 K spotted oligo [23] for data sets [15,20,22], Agilent Human 1 cDNA Microarray for [18,19], 3 K BAC array [24] for [16], 2 K BAC array [25] for [17] and 6 K BAC array for [21]. Table 1 provides a summary of the cancer and array types, together with group definitions and sizes.<br>
<br>
Simulated Data Sets<br>
In addition to real data sets, evaluation was also performed with simulated data. While generating the simulations, we attempted to implement realistic aspects of both signal and noise of tumor profiles. In the context of an aCGH experiment comparing two groups, the signal consists of aberrant regions that are specific to one of the groups. Noise consists of regions common to both groups, random aberrations in individual samples, and technical noise. Further characteristics are also that the sizes of the aberrant regions vary from entire chromosomes to focal aberrations, the exact start and end positions of a region vary slightly from one sample to another, and even a "common" region might not be be present in all of the samples.<br>
The simulated data were generated by introducing artificial aberrations into a data set of clinical genetics samples of patients with mental retardation and no or few chromosomal aberrations [26]. To achieve a simulated data set of the desired size, resampling was performed with replacement. Aberrant regions were then randomly introduced as follows. A single array element was chosen at random as the starting point of a region. The size of the region was then chosen at random with a 10% probability for a single cytoband, 30% for three consecutive bands, 30% for six consecutive bands, 20% for the whole chromosome arm, and 10% for the entire chromosome. The type of the aberration was randomly chosen as a gain or loss with equal probabilities, but for the smallest aberrations of individual cytobands, a 2% probability for amplifications was also included. When introducing a region to a set of samples, the exact samples receiving the aberration were sampled from the Bernoulli distribution with p = 70%. Randomness was also introduced to the exact start and end positions of aberrations in individual samples by shifting the starting and ending array elements by a random number between -10 and 10.<br>
A simulated data set of 15 + 15 arrays was generated with 30 common regions, and 5 regions for each individual sample. These copy number changes do not separate the two groups from each other, and therefore represent background noise. This data set is referred to as Simulation 0. Single regions specific to the two groups were then introduced to Simulation 0 yielding data set Simulation 1. This process was repeated ten times resulting in a set of 11 simulations with the amount of differential signal ranging from none in Simulation 0 to 10 regions specific to each group in Simulation 10. Only Simulations 0, 5 and 10 are presented in this paper, but the full <software>CGHpower</software> outputs for all of them are available on the program's web page.<br>
<br>
Preprocessing<br>
All evaluation data sets were preprocessed starting from raw log2 ratios. First, the data were median normalized. Wavy patterns typically seen in many aCGH profiles were removed [26] from the 30 K arrays [15,20,22]. Normalized log ratios were segmented using the DNAcopy algorithm [27] and called by <package>CGHcall</package> [28] to identify gains, losses and amplifications. Regions between breakpoints were then collapsed into single data points, when shared between most of the samples [13]. Finally, the median log ratio was calculated for each of these regions in each sample, resulting in region-wise log ratios (RWLRs). All algorithms were run with default parameters, and sex chromosomes were excluded from the data.<br>
<br>
Sample Size Calculations<br>
For each region, t-statistics were calculated with a Welch's t-test allowing unequal variances and p-values computed from the normal distribution. The proportion of non-differentially behaving regions (?) was estimated by minimizing the difference between parametric (?n) and non-parametric () estimators of G, which is the unknown distribution of the p-values from differentially behaving regions. The limiting density of effect sizes (?)and G were then estimated using deconvolution. Finally, with FDR fixed at 10%, these parameter estimates were used to approximate average power as a function of sample size.<br>
<br>
<br>
Results and Discussion<br>
Estimates of average power as a function of sample size were calculated for the eight evaluation data sets and 11 simulations (Figure 1). The reliability of the power calculations depends directly on the the quality of parameter estimation, which in turns depends on compliance with required assumptions. The first assumption is that the proportion ? of non-differentially behaving regions be "substantially" smaller than 1 (e.g. ~0.9 will typically do, but 0.99 will not). The second assumption is that the RWLRs be approximately normally distributed, being neither particularly asymmetric (skewness) nor heavily tailed or extremely peaky (kurtosis). The complete <software>CGHpower</software> program output contains diagnostic plots from different stages of the power calculations procedure. These plots help determine to which extent these assumptions are fulfilled. While it is impossible to know what the true values of ? and ? are, one can easily evaluate how well the two estimators of G agree with each other (the "goodness-of-fit"). If they show a clear discrepancy, the accuracy of parameter estimation is questionable and the resulting power calculations consequently unreliable. Different scenarios in the quality of parameter estimation observed with the evaluation data sets are examined for each of the data sets to estimate the reliability of the calculated power.<br>
The data sets Douglas et al., Smeets et al., Fridlyand et al. and Chin et al. are examples where the goodness-of-fit of the G estimators was satisfactory, ranked in this order according to their fits (Figure 2A). What appears to be the most important factor distinguishing these data sets from the others, is the density of the p-values. If there is no difference detected between two groups, p-values are expected to follow a uniform distribution, and their density function appears as a flat line. When the number of differentially behaving regions increases (? moves away from 1), density at low p-values increases and the function is expected to be convex (Figure 2B). This can also be seen on the simulations where the amount of differential signal gradually increases from Simulation 0 to Simulation 10. Along with the increase in density for low p-values, also the goodness-of-fit systematically improves (data and figures at http://www.cangem.org/cghpower/).<br>
Less satisfactory performance was observed with data sets of Postma et al. and Myllykangas et al. The goodness-of-fit shows more disagreement between the two estimators of G (Figure 2C) and as a result power estimates are less reliable. The density is increasing for low p-values, but slightly less and the function is not convex as expected (Figure 2D). Compared to Simulation 0, which has no true differences between the groups, the increase in p-value density for the data set of Myllykangas et al. is very small. One explanation is that there is simply not enough differential signal that is detectable with a t-test. Alternatively, the number of differentially behaving regions might be too low ( i.e. ? is too close to 1). While these data sets do give ? estimates of 0.75 and 0.55, respectively, these estimates cannot be trusted if the estimates of G disagree with each other. Therefore it is recommended that the goodness-of-fit plot be used to assess the reliability of the estimates of other parameters. Also, judging from the results with the simulated data sets, <software>CGHpower</software> seems to underestimate the true value of ?.<br>
While assumptions regarding ? seem to be most important, the RWLRs are also assumed to be normally distributed. The program output contains histograms of the skewness (asymmetry) and kurtosis (peakedness) of the RWLRs, superimposed with those of a normal distribution (data on the <software>CGHpower</software> web page). Assumptions of normality become more critical with small sample sizes and less important with large ones. Within the evaluation data sets, most violations of normality were observed with the Chin et al. data set, yet this is one of the better-performing ones in terms of goodness-of-fit. This might be explained by the relatively large sample size (170) of the study. Another factor besides the number of arrays, is the number of regions found after the preprocessing step. The larger the number of regions, the better the performance of the parameter estimation and therefore the reliability of power calculations. The assumption of normality is therefore more crucial with samples containing very few biological differences.<br>
The data sets of Nymark et al. and Wrage et al. are examples where our method failed to work, despite the differences reported and technically as well as biologically validated. In the case of Nymark et al. the obtained power curve is a flat line (Figure 1). This can happen when parameter estimation fails. The explanation can be found from the density of the p-values, but now the assumptions were violated more severely than in the cases of Postma et al. and Myllykangas et al. The density function is actually concave and shows even less density at low p-values than would be expected by chance (Figure 2F). With Wrage et al., failure can be observed at the preprocessing step, as only 23 regions are detected (Table 1). Since the sex chromosomes are excluded from the analysis, this means that only one copy number breakpoint was detected in the whole genome using the fixed <software>CGHpower</software> preprocessing described above. As preprocessing and power calculations procedures are fixed earlier in <software>CGHpower</software>, it was not optimized it for every aCGH platform or data set. Allowing the user to fine-tune different settings and immediately see the result of each change would require implementing a more complex user interface, similar to desktop software, which would be impractical for a single-purpose web tool. As an alternative option, if the goodness-of-fit and density plots indicate that power calculations failed, users can perform preprocessing independently, turn off the preprocessing step from the program, and perform the power calculations only.<br>
Consistency as the Pilot Size Is Increased<br>
<software>CGHpower</software> was initially developed to be used on smaller pilot data sets in the planning stages of larger microarray experiments or for verifying power achieved in past experiments. We wanted to evaluate whether the resulting power estimates hold while more and more arrays are added to the data set. Assuming that a pilot of 10 + 10 arrays has estimated an experiment with 40 + 40 arrays should result in an average power of approximately 70%. The data set of 80 arrays is then generated and for verification the power calculations are repeated with the entire data set. If the new results indicate that the achieved power is in fact only 50%, and that 20 + 20 new arrays are needed in order to achieve our goal of 70%, then the two power calculations have to be declared inconsistent. To evaluate whether the power estimates remain consistent while the pilot size is increased, power was calculated with smaller subsets of the Chin et al. data set, since it is our largest one. This data set contains a total of 170 arrays (113 vs. 57), which was split into smaller subsets to represent pilots of a larger study. Nine resamplings ranging from 10% (11 vs. 6 arrays) to 90% (102 vs. 51) of the original data set were randomly selected for the power calculations. Each resampling was repeated 10 times and the results were averaged. Two of the ten repetitions of the 10% subset and one repetition in the 20% subset experienced a failed power estimation resulting in flat power curves as with the Nymark et al. data set. These cases were removed before averaging the results. A plot of the resulting power estimates shows that except for the smallest subset (11 vs. 6 arrays), the results appear to be consistent (Figure 3). This suggests that as long as the pilot is of sufficient size, power estimates generated with <software>CGHpower</software> using smaller pilot data sets are in fact representative of a subsequent larger study. While the exact requirement for a "sufficient pilot" is hard to define beforehand, the power calculations can be repeated when more arrays are performed to see whether power estimates are still changing or have been stabilized.<br>
<br>
<br>
Conclusions<br>
We have explored sample size calculations in the context of aCGH and copy number changes and propose a dedicated tool for this purpose. From a pilot data set, <software>CGHpower</software> estimates the biological diversity between two groups of cancer samples and estimates average power as a function of sample size using an adaptive version of the Benjamini-Hochberg method for multiple testing [4,10]. Pilot data is used for parameter estimation and this requires certain assumptions to hold in an approximate sense. We have evaluated the performance of <software>CGHpower</software> with eight published data sets, four of which show satisfactory performance using predefined preprocessing measures. Among these data sets were BAC and oligo-based array platforms, whose resolution varied from less than 2 K for BACs to almost 27 K for oligos. The differences in resolution did not have a direct impact on the obtained power estimates, which should be determined more by the amount of biological variation between the two groups.<br>
In two data sets violations of critical assumptions lead to problems in parameter estimation and therefore power estimates are less reliable. More severe violations and/or the inflexibility of a completely predefined analysis procedure lead to failed execution for the two other data sets. Even though the proposed method has its limitations, it is to our knowledge the only proposed one for aCGH data and copy number changes. As the program allows performance evaluation through diagnostic plots, critical judgement can be applied for each data set.<br>
As a summary on the evaluation of <software>CGHpower</software> results, users should consider paying attention to the following: 1) Do the copy number profile plots appear similar to the aberrations that you have detected in your own analysis? If <software>CGHpower</software> does not seem to detect the important aberrations, consider performing the preprocessing before uploading and use <software>CGHpower</software> only for the power calculations. 2) Do the estimators of G agree with each other? If the goodness-of-fit is poor, so will other parameter (and resulting power) estimates. 3) Is the density function of the p-values convex, and showing a higher density at small p-values? A straight or concave function might be caused by too small effect size, or ? being too close to one. 4) Excess skewness and/or kurtosis in the data might also affect the performance, but this seems to be less crucial.<br>
The proposed method uses log ratios instead of calls, even though we feel the latter is generally the preferred choice when working with aCGH data. Calls have the benefit of a clear biological meaning and are therefore easier to interpret. However, their use for power calculations in the context of FDR is problematic, as it would require using the chi-square test, a setting that is not as well developed as the Gaussian one. Also, as log ratios are the basis for calls in the first place, they do contain all the necessary information even though they are not as clear to interpret.<br>
In comparison to sample size calculations for mRNA expression arrays, the differentiating factor for aCGH studies is the concept of regions, which stems from the different biological phenomenon underlying the microarray log2 ratios. Compared to the number of array elements, the number of regions is relatively small, which presents challenges to parameter estimation from the data. As the total number of regions is remarkably smaller than with expression arrays, the estimation might fail if the number of differentially behaving regions is too small, even if there is a true difference between the groups.<br>
An important concern when performing power calculations is the actual power requirement. A power curve typically plateaus out at some point, indicating saturation. Increasing the average power from e.g. 60% to 70% requires a significantly bigger increase in sample size than is needed for an increase from 50% to 60%. Therefore it is difficult to set a a predefined gold standard of adequate power. One option is to try to find where the slope of the power curve is decreasing rapidly. This should give a reasonable compromise between statistical power and cost of the experiment. Another aspect worth pointing out, is that the level of power needed also depends on the research question. For example, if the goal is to construct a classifier that can classify future samples into one of the two groups, a lower level of average power might yield a perfectly satisfactory classifier even though not all differences are detected.<br>
<br>
Availability and requirements<br>
<software>CGHpower</software> is a web-based application and can be freely accessed at http://www.cangem.org/cghpower/. It allows direct uploads and can also automatically retrieve data stored in the <database>CanGEM</database> database [29]. The computation times of <software>CGHpower</software> may vary considerably depending on the number of samples and array elements in the data set, and also on the prevailing load of the Linux cluster where the calculations are performed. As an example, running times for a data set of 30 samples and 42 K array elements have been around 1-1.5 hours in our test runs. The software has been implemented in R [30] and the source code is available upon request.<br>
<br>
Authors' contributions<br>
BY conceived the study. IS, JAF, MAW and BY designed <software>CGHpower</software>. IS performed the implementation. IS, JAF, MAW and BY wrote the manuscript with critical comments from SK and GAM. All authors read and approved the final manuscript.<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2921412</b><br>
The effect of prior assumptions over the weights in <software>BayesPI</software> with application to study protein-DNA interactions from ChIP-based high-throughput data<br>
<br>
<br>
Background<br>
In our previous study, we developed a Bayesian neural network type of model - <software>BayesPI</software> - to study protein-DNA interactions, using ChIP-based high-throughput data [1]. In <software>BayesPI</software>, the model error function (data error) is interpreted as defining a likelihood function, and the model regularizer (a penalty term to the error function) corresponds to a prior probability distribution over the weights, and such a framework is considered as a Bayesian hierarchical model. In addition to the common model parameters, <software>BayesPI</software> includes unknown hyperparameters (e.g. weight decay rate ? and model noise level ?) that need to be learned from the data. There are three possible implementations to control the model hyperparameters when using Bayesian neural networks to infer the model parameters: 1) using Markov chain Monte Carlo methods to simulate the probability distribution - MCMC [2]; 2) integrating out the model hyperparameters analytically before the application of Gaussian approximation of posterior distribution, and subsequently maximizing the true posterior over the model parameters - Maximum A Posterior Probability (MAP) [3]; and 3) integrating out the model parameters first, and then maximizing the resulting evidence over the hyperparameters - the Evidence Approximation [4]. Descriptions of the first two implementations can be found in the earlier papers [2,3], and in this study, we will focus only on the last approach (the evidence approximation) implemented in <software>BayesPI</software>.<br>
Three motivations inspired us to pursue an investigation on the effect of prior assumptions over the weights (the evidence approximation) in Bayesian neural networks to study protein-DNA interactions from ChIP-based high-throughput data: 1) With regard to others' concern, before <software>BayesPI</software> paper was published, we received some criticisms about the treatment of hyperparameters in Bayesian neural networks. For example, do alternative definitions of hyperparameters according to the model parameters (e.g. divide the hyperparameters ? into several classes based on either the structure of neural networks or the property of the model parameters) strongly influence the model inference? 2) With regard to our own interest, how significant will a different assignment of prior distribution (e.g. Gaussian prior, Laplace prior or Cauchy prior) to weights affect the outcome of Bayesian neural networks (e.g. prediction accuracy and computational time cost)? 3) With respect to a general survey of the application of Bayesian inferences in ChIP-based experiments, we searched <database>PubMed</database> using the keywords "Bayesian, chip" or "Bayesian, ChIP-chip," and then downloaded the search results that had been recorded before May 28, 2010. From this search, we obtained 33 papers that contained the above-mentioned keywords. Subsequently, we carried out a literature study of these 33 papers. To our surprise, only 14 of the 33 papers had applied Bayesian methods on issues related to motif discovery (e.g. DNA binding site identification) by using ChIP-based high-throughput data, and the remaining 19 papers had applied Bayesian methods in data integration, clustering and network reconstructions, etc. A detailed examination of the 14 papers relevant to protein-DNA interaction study reveal that <software>BayesPI</software> applied used evidence approximation to solve the posterior distribution in Bayesian inference, while the remaining 12 papers utilized the sampling methods (e.g. MCMC and Gibbs sampling) to simulate the posterior distribution of the Bayesian models (one paper cannot be determined because of lack of method description; detailed information of the 33 papers is available in [Additional file 1: Supplemental Data]). Though the present implementation (the evidence approximation) in <software>BayesPI</software> for handling hyperparameters has been rarely applied earlier, there are clear advantages of using it to solve the data mining problems [5]. Thus, by being motivated by the last finding along with the earlier two inspirations, we decided to carry out a follow-up study on the effect of prior assumption over the weights in <software>BayesPI</software>. Our study may pave the way for the future development of evidence approximation in Bayesian inferences as well as for the further application of the Bayesian methods in bioinformatics research.<br>
<br>
Results<br>
Performance comparisons from simulated ChIP-chip datasets<br>
To evaluate the performance of <software>BayesPI</software> under (15) different prior assumptions over the weights, we first tried each of them on the same set of simulated ChIP-chip experiments (16 synthetic ChIP-chip datasets), where the synthetic DNA sequences and ChIP-chip log ratios were generated using <software>MATLAB Bay Net</software> toolbox and <software>MATLAB</software> build-in random number generator, respectively [1]. The accuracy of the predictions was accessed from motif similarity scores by comparing the predicted motif energy matrix with the corresponding <database>SGD</database> consensus sequences [6]. In Figure 1, we have illustrated the outcomes of the above-mentioned simulations in 15 different prior assumptions, where both the CPU hours required for the calculation and distribution of the motif similarity scores among all the tests are shown. The results are very interesting because no significant changes of the prediction quality could be observed across the tests after changing either the prior probability assumption or the number of subclasses for ? hyperparameters, except for the tests with Gaussian approximation (e.g. comparing the distribution of motif similarity scores using Wilcoxon rank-sum test: Gaussian vs. Cauchy, p &lt; 0.03; Laplace vs. Cauchy, p &lt; 0.04). However, the CPU hours used for various tests differed significantly. Particularly, the selection of prior probability assumption over the weights in Bayesian neural networks had a much stronger impact on the cost of CPU hours than that by tuning the number of subclasses of hyperparameters. For examples, by using a Laplace assumption over the weights in <software>BayesPI</software>, the CPU hours used for the calculations were shortened by almost two to five times when compared with the assumptions of the weights by the other two probability distributions (e.g. comparing the distribution of used CPU hours by Wilcoxon rank-sum test: Gaussian vs. Laplace, p &lt; 1.4e-9; Cauchy vs. Laplace, p &lt; 5.8e-8). It is worth noting that the assignment of Laplace prior probability to weights utilizes the least CPU hours for the calculation, but provides the best prediction accuracy. Thus, we can expect Laplace approximation over the weights to provide the most efficient computation for <software>BayesPI</software> if real ChIP-chip datasets are used.<br>
<br>
Performance comparisons from real ChIP-chip datasets<br>
After testing the effect of prior assumptions over the weights in <software>BayesPI</software> using the synthetic ChIP-chip datasets, we tried it on the real protein-DNA interaction datasets from ChIP-chip experiments. We collected ChIP-chip datasets for nine yeast TFs in rich medium condition [7], among which four (SWI4, INO4, ACE2, and XBP1) had the same consensus sequences as the TFs in the synthetic datasets. In the earlier tests, we neither found a significant variation in the prediction accuracy nor observed a strong perturbation of the computational time cost (Figure 1) through tuning the number of subclasses of ? hyperparameters: hence, we decided to select only four subclasses for the hyperparameters in the rest of the studies. The results of these tests both with and without the inclusion of nucleosome information are presented in Figure 2 that shows that there is little difference in the prediction accuracies among the tests regarding the selection of prior probability assumptions and the inclusion of the nucleosome information. A comparison between the motif similarity scores provided by the three prior weight assumptions in <software>BayesPI</software> and those obtained by <software>MatrixREDUCE</software> is presented in Table 1. The results indicate that all poor predictions are caused by stress-induced transcription factors (e.g. ROX1, MSN2, and XBP1). Though <software>BayesPI</software> may provide some reasonable answers to TFs that are nonfunctional under certain growth conditions (e.g. Table 1, SKN7 and MSN2 in the YPD condition), its computational speed is much slower than that by the popular program <software>MatrixREDUCE</software> [8]. Nevertheless, the CPU hours used by <software>BayesPI</software> among the three prior weight assumptions differ significantly (e.g. comparing the distribution of used CPU hours without considering the nucleosome information by Wilcoxon rank-sum test: Gaussian vs. Laplace, p &lt; 4.2e-5; Gaussian vs. Cauchy, p &lt; 4.2e-5; Cauchy vs. Laplace, p &lt; 2.9e-4). Furthermore, we found that the cost of the CPU hours for the computation was slightly reduced with regard to the nucleosome information. Taken together, it can be concluded that the computational efficiency of the Laplace prior assumption over the weights in the Bayesian neural networks clearly surpasses that of the other two weight priors, and that the Laplace prior may be suitable for the further improvement of BayesPI algorithm.<br>
<br>
Performance comparisons from human ChIP-Seq datasets<br>
After the successful application of the earlier tests on ChIP-chip datasets, we tried the new <software>BayesPI</software> program on three human ChIP-Seq datasets [9] by applying three different prior assumptions (e.g. Gaussian, Lasso, and Cauchy) over the weights with predefined four groups of regularization constants ?. Here, the inputs to <software>BayesPI</software> were pre-processed raw ChIP-Seq measurements, which are a set of putative protein binding sites (e.g. there are 5814, 26815, and 73957 putative TF binding sites for NRFS, CTCF, and STAT1, respectively.), and the corresponding tag densities obtained from SISSRs method [9]. The results of these tests are shown in Figure 3, which demonstrates that the Laplace prior requires much less CPU hours when compared with that required by the other two assumptions. For instance, to complete the same calculation, Laplace approximation needs only 50 percent to 25 percent of the CPU hours that is used by either Cauchy or Gaussian approximation. However, interestingly, the accuracy of the predictions does not differ significantly among various prior assumptions similar to the previous tests that employed the ChIP-chip datasets: tuning of the prior assumption over the weights does not seem to affect the quality of the predictions, but is rather observed to bring strong impact on the CPU requirement. Thus, a careful design of the weight priors in a Bayesian model may significantly reduce the computational cost for the calculation.<br>
<br>
<br>
Discussion<br>
Nowadays, chromatin immunoprecipitation followed by massively paralleled sequencing (ChIP-Seq) is being used widely in various molecular biological researches such as investigating genome-wide protein-DNA interactions [7] and histone modification studies [10]. It is possible that the ChIP-Seq experiment may replace ChIP-chip technology completely [11] in future. That is because the ChIP-Seq experiment produces higher quality and higher resolution data than the ChIP-chip, which also avoids several pitfalls that accompany with the ChIP-chip technology: for example, array probe-specific behavior and dye bias [12]. In this work, we studied the effect of prior assumptions over the weight in <software>BayesPI</software> to predict the protein binding energy matrices from ChIP-based high-throughput datasets. The results on both synthetic and real experimental datasets were consistent: in general, the prior assumptions over the weights and the classification of regularization constants (e.g. hyperparameters ?) into several classes did not strongly affect the final outcome of <software>BayesPI</software> (e.g. Figures 1, 2, and 3) if sufficient training datasets were provided; particularly, a change in the number of classes over the regularization constants had a much weaker impact on the requirement of computational resource than a change in the weight prior in <software>BayesPI</software>; nevertheless, the selection of prior approximation over the weights had the most significant influence on the CPU hours that were used for calculation (e.g. by using a Laplace prior, the computational time was reduced by more than 50 percent when compared with that utilized by the old <software>BayesPI</software> [1], a Gaussian prior.) Thus, the present study reveals the importance of defining a right weight prior to a Bayesian hierarchical model, which may dramatically speed up the calculation when the program is applied to a large dataset.<br>
In addition to the above-mentioned findings that the computation efficiency of <software>BayesPI</software> is highly associated with prior assumptions over the weights, we also provided a detailed illustration of the hyperparameter re-estimation technique by using the evidence approximation method. We presume that the evidence method may become a popular approximate method for computational implementation of Bayesian hierarchical model (a deterministic algorithm), as well as become an alternative to Monte Carlo methods that are currently being widely used in bioinformatics research fields [13]. Particularly, the evidence method can overcome some of the inherent limitations of the sampling approaches, such as nonreproducible results, long burning period, and unknown stopping time.<br>
<br>
Conclusions<br>
The present study has clarified several doubts in the early implementation of <software>BayesPI</software>: 1) prediction accuracy of <software>BayesPI</software> is robust against dividing the hyperparameters (e.g. regularization constants ?) into multiple distinct groups; 2) there is a minor effect on the quality of predictions by selecting alternative prior assumptions over the weights in <software>BayesPI</software>; 3) however, there is a strong impact on the computational requirement for calculation when a proper weight prior is chosen. Overall, we have derived the new re-estimation formulas for both Laplace prior and Cauchy prior over the weights in the Bayesian neural networks, and the new implements have been tested successfully in both synthetic and real ChIP-based high-throughput datasets.<br>
<br>
Methods<br>
Computational modeling of protein-DNA interactions in <software>BayesPI</software><br>
In this study, we have only focused on the effect of prior assumption over the parameters in Bayesian neural networks. The descriptions of the biophysical background behind <software>BayesPI</software> and the implementation of the Bayesian predicative model to estimate the protein binding parameters by combining ChIP-based datasets with DNA sequence information will not be repeated, because they are available in the previous paper [1]. First, we regularized the objective function<br>
(1)M(w)=??ED(D|w,?,?)+??Ew(w|?,?)<br>
which can be used by the Bayesian neural networks [4] to determine the parameters (e.g. w, ?, ?). In the above-mentioned equation, ED, Ew, D, and ??, ?, ?? are the model error function (data error), the model regularizer (a penalty term to the error function), the input data, and the hypothesis model space (e.g. ? is the protein binding probability and ? is the regularization function), respectively; ? and ? are the two unknown hyperparameters (e.g. weight decay rate and model noise level) that must be determined from the input data; and w indicates the model parameters (e.g. weights in the Bayesian neural networks), which represents the inferred the protein binding energy matrix and the chemical potentials from ChIP-based high-throughput data [1].<br>
Here, a Gaussian model error function ED is assumed throughout as<br>
(2)ED=12?i=1g(ti?Yi)2<br>
where ti is the measured ChIP-based data to gene i, and Yi is the predicted TF occupancy data for that gene, according to a predefined TF binding probability (e.g. either inclusion or exclusion of nucleosome binding information in the protein-binding probability [1]). For the model regularizer Ew, three types of weight prior assumptions were selected [3]: 1) Gaussian prior assumption over the weights,<br>
(3)Ew=12?q=1Qwq2<br>
in which Q is the number of parameters in the model, such as w; 2) Laplace prior distribution as the model regularization function,<br>
(4)Ew=?q=1Q|wq|<br>
3) Cauchy prior assumption over the model parameters w in <software>BayesPI</software><br>
(5)Ew=1??q=1Qlog(1+?2wq2)<br>
Based on the above-mentioned three weight priors, we applied the evidence approximation method [4] to determine the corresponding re-estimation formulas for both ? and ?, which can be used by Bayesian neural networks to fit the model (e.g. to learn the model parameters w from the data).<br>
<br>
Bayesian choice of ? and ? through the evidence approximation<br>
Evidence approximation<br>
Based on Bayes' theorem, a posterior distribution of the model parameters, can be given as<br>
(6)P(w|D,?,?,?,?,?)=exp(?M(w))ZM(?,?)<br>
where M is a probability framework of the objective function described in equation (1) and ZM is a normalization factor [4]. By employing a Gaussian approximation of the posterior probability, we have<br>
(7)P(w|D,?,?,?,?,?)?exp(?MMP)exp(?12(w?wMP)TA(w?wMP))ZM'<br>
in which ZM'=?dwexp(?MMP(w)) and A is the Hessian of M (e.g. A = ???Ew + ???ED) evaluated at wMP. Here, we first assume that the most probable model parameters wMP are known (integrating out the model parameters), and then infer the hyperparameters through Bayes' rule<br>
(8)P(?,?|D,?,?,?)=P(D|?,?,?,?,?)P(?,?|?,?,?)P(D|?,?,?)<br>
where we also assume equal priors P(?, ?|?, ?, ?) to the alternative models and a constant term to the P(D|?, ?, ?). Thus<br>
(9)P(?,?|D,?,?,?)?P(D|?,?,?,?,?)<br>
where P(D|?, ?, ?, ?, ?) is the evidence for the overall model,<br>
(10)P(D|?,?,?,?,?)=ZM(?,?)Zw(?)ZD(?)<br>
including both the model architecture and the regularizing parameters [4], where Zw(?) and ZD(?) are the normalization factors given by Zw(?) = ? dwexp(-?Ew) and ZD(?) = ? dDexp(-?ED), respectively. By maximizing the log evidence of equation (10), we can determine the re-estimation formulas for hyperparameters ? and ? according to the weight assumptions Ew in <software>BayesPI</software>.<br>
<br>
Gaussian prior<br>
The log evidence for hyperparameters is<br>
(11)log(P(D|?,?,?,?,?))=logZM?logZw?logZD<br>
where a Gaussian prior, equation (3), is used for Ew and<br>
(12)ZM?exp(?MMP)(2?)k/2?detA?1/2<br>
(13)Zw?(2??)k/2<br>
(14)ZD?(2??)N/2<br>
After replacing ZM, Zw, ZD by equations (12), (13), and (14), respectively, equation (11) becomes<br>
(15)log(P(D|?,?,?,?,?))??MMP?12logdetA+k2log??N2log2?+N2log?<br>
To determine the conditions that are satisfied at the maximum log evidence, we differentiated equation (15) with respect to ? and ?, and then set the derivative to zero from which we can obtain the re-estimation formulas for both ? and ? as follows<br>
(16)?=k??Trace(A?1I)2Ew<br>
(17)?=N?k+?Trace(A?1)2ED<br>
Let<br>
(18)?=k??Trace(A?1)<br>
The equations (16) and (17) can be rewritten as<br>
(19)?=?2Ew<br>
(20)?=N??2ED<br>
where ? are eigenvectors of A. For example, equation (18) can be transformed to<br>
(21)?=?q?q?q+?<br>
Where ?q are the eigenvalues of the ???ED and the negative ?q are omitted from the sum. Thus, for a Gaussian weight prior, we used equation (21) to update the hyperparameters ? and ? through equations (19) and (20).<br>
<br>
Laplace prior<br>
By using equation (4) as a prior assumption over the weights, the Hessian of M becomes<br>
(22)A=???ED<br>
The log evidence for the hyperparameters is<br>
(23)log(P(D|?,?,A,?,R))??MMP+k2log2??12logdetA?logZw?logZD<br>
where<br>
(24)Zw?(2?)k<br>
(25)ZD?(2??)N/2<br>
After inserting equations (24) and (25) into the log evidence, we get<br>
(26)log(P(D|?,?,A,?,R))??MMP?12logdetA+k2log2??klog2+klog??N2log2?+N2log?<br>
To maximize the log evidence over the hyperparameters, we differentiated equation (26) with respect to ? and ?, and the derivative was set to zero, then we obtained the following re-estimation formulas<br>
(27)?=kEw<br>
(28)?=N?k2ED<br>
for the hyperparameters, when assuming a Laplace prior over the weights.<br>
<br>
Cauchy prior<br>
Here, we used equation (5) as the prior assumption over the weights, and the log evidence for the model can be given as<br>
(29)log(P(D|?,?,A,?,R))??MMP+k2log2??12logdetA?logZw?logZD<br>
where<br>
(30)Zw?(?|?|)k<br>
(31)ZD?(2??)N/2<br>
After inserting equations (30) and (31) into equation (29), the log evidence becomes<br>
(32)L(?,?)=?MMP?12logdetA+k2log2??klog?+klog|?|?N2log2?+N2log?<br>
where we assume that ? is known for Ew, ?Ew, and ??Ew. To determine the conditions suitable for the maximum log evidence, equation (32) was differentiated with respect to hyperparameters, and the derivative was set to zero. Then the re-estimation formulas for ? and ? are<br>
(33)|?|=k+?2Ew<br>
(34)?=N??2ED<br>
where<br>
(35)?=?q?q?q+???Ew<br>
where ?q are the eigenvalues of data error ???ED. Thus, for a Cauchy prior, equation (35) can be used to compute hyperparameters ? and ? through equations (33) and (34). Detailed derivations of hyperparameters update functions for above three priors are available in [Additional file 1: Supplemental Methods].<br>
<br>
Application of R-propagation algorithm<br>
In equation (35), there is a second derivative ??Ew, which can be estimated from an efficient R-propagation algorithm of Pearlmutter [14]. The algorithm applies a differential operator R() on the Back-propagation neural networks. For example, let us assume that equation (5) is used by the model regularizer Ew. Then<br>
(36)Ew=1?a2<br>
where ?2 is the node of the output layer<br>
(37)a2=?qlog(1+Hq2)<br>
in which Hq is the node of the hidden layer<br>
(38)Hq=?a1q<br>
and ?1q is the node of the input layer<br>
(39)a1q=wq<br>
After completing the above-mentioned forward computation of the neural networks, a backward pass can be subsequently obtained as<br>
(40)?E?a2=1??E?Hq=1?2Hq1+Hq2?E?a1q=2Hq1+Hq2?E?wq=2Hq1+Hq2<br>
and R-forward computation can be carried out as follows<br>
(41)R(a1q)=VwqR(Hq)=?R(a1q)R(a2)=?q2Hq1+Hq2R(Hq)<br>
Furthermore, the R-backward computation can be carried out as follows<br>
(42)R(?E?wq)=?2(Hq2?1)(Hq2+1)2R(Hq)<br>
By following the above-mentioned R-back-propagation procedures, R(?E?wq) can be estimated, which is equivalent to computing the second derivative ??Ew [14]. Detailed description of application of R-propagation algorithm is available in [Additional file 1: Supplemental Methods]. The source code of <software>BayesPI2</software> is public available http://folk.uio.no/junbaiw/bayesPI2.<br>
<br>
<br>
Multiple regularization constants ?<br>
For simplicity, we assumed that there is only one class of weights in <software>BayesPI</software> [1]. For example, the weights are modeled as coming from a single Gaussian prior (e.g. equation (3)). However, in a real study, weights may fall into multiple distinct groups [4]. Therefore, it is desirable to divide the weights into several classes c, with independent regularization constants ?c. In the new version of <software>BayesPI</software>, there are five types of assignment of weight decay rate ? to each of the three weight priors (e.g. Gaussian, Laplace, and Cauchy). The term ?Ew in equation (1) is replaced by ?c?cEwc, in which c is the number of classes to the regularization constants ?: 1) if c equals 1, then all the weights have the same regularization constant ?; 2) if c equals 2, then we can divide the weights into two groups, namely the weights in the hidden layer and the weights in the output layer; 3) if c equals 3, then it suggests that there are two distinct weight classes in the hidden layer (e.g. weights from the motif energy matrix and weight from the chemical potential), but only a single weight class in the output layer [1]); 4) if c equals 4, then it suggests that there are two independent weight classes in both the hidden layer and output layer; 5) if c is greater than 5, then it suggests that each binding position of the motif energy matrix has its own regularization constant ?c as well as the chemical potential, and that the two weights in the output layer have their own regularization constants, respectively (e.g. if TF motif length equals 8, then the regularization constant ? has 11 classes).<br>
<br>
Motif similarity score and Microarray datasets<br>
To access the quality of the predicted motif binding sites, we used a published method (motif similarity score [15]) to estimate the similarity between the predicted motif energy matrices and the corresponding consensus sequences from the <database>SGD</database> database [16]. Detailed description of these calculations can be found in the previous publication [1]. Synthetic ChIP-chip datasets and real ChIP-chip experiments for nine yeast transcription factors were adopted from the earlier works [1,7]. ChIP-Seq datasets for three human TFs (STAT1, NRSF, and CTCF) were obtained from Jothi et al. [9]. More information about the preprocessing of both ChIP-chip and ChIP-Seq datasets are available in [1].<br>
<br>
<br>
Competing interests<br>
The author declares that they have no competing interests.<br>
<br>
Authors' contributions<br>
JW conceived and designed the study, implemented program, performed data analysis and drafted manuscript.<br>
<br>
Supplementary Material<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1562452</b><br>
Transient Calcium and Dopamine Increase PKA Activity and DARPP-32 Phosphorylation<br>
Reinforcement learning theorizes that strengthening of synaptic connections in medium spiny neurons of the striatum occurs when glutamatergic input (from cortex) and dopaminergic input (from substantia nigra) are received simultaneously. Subsequent to learning, medium spiny neurons with strengthened synapses are more likely to fire in response to cortical input alone. This synaptic plasticity is produced by phosphorylation of AMPA receptors, caused by phosphorylation of various signalling molecules. A key signalling molecule is the phosphoprotein DARPP-32, highly expressed in striatal medium spiny neurons. DARPP-32 is regulated by several neurotransmitters through a complex network of intracellular signalling pathways involving cAMP (increased through dopamine stimulation) and calcium (increased through glutamate stimulation). Since DARPP-32 controls several kinases and phosphatases involved in striatal synaptic plasticity, understanding the interactions between cAMP and calcium, in particular the effect of transient stimuli on DARPP-32 phosphorylation, has major implications for understanding reinforcement learning. We developed a computer model of the biochemical reaction pathways involved in the phosphorylation of DARPP-32 on Thr34 and Thr75. Ordinary differential equations describing the biochemical reactions were implemented in a single compartment model using the software <software>XPPAUT</software>. Reaction rate constants were obtained from the biochemical literature. The first set of simulations using sustained elevations of dopamine and calcium produced phosphorylation levels of DARPP-32 similar to that measured experimentally, thereby validating the model. The second set of simulations, using the validated model, showed that transient dopamine elevations increased the phosphorylation of Thr34 as expected, but transient calcium elevations also increased the phosphorylation of Thr34, contrary to what is believed. When transient calcium and dopamine stimuli were paired, PKA activation and Thr34 phosphorylation increased compared with dopamine alone. This result, which is robust to variation in model parameters, supports reinforcement learning theories in which activity-dependent long-term synaptic plasticity requires paired glutamate and dopamine inputs.<br>
<br>
Introduction<br>
The basal ganglia play an important role in reinforcement learning, in which an animal learns that performing a particular action in response to a neutral (e.g., visual) stimulus will be rewarded [1,2]. In vivo recordings during learning reveal that the reward elicits an increase in dopamine neuron firing [3], and an increase in dopamine release in the striatum [4,5], whereas the visual stimulus and the motor action produce cortical activity, which is transmitted to the striatum as an increase in glutamate release. Thus, during reinforcement learning, the medium spiny neurons of the striatum receive paired glutamate and dopamine input. Numerous studies show that glutamate input from cortex combined with dopamine produces a persistent increase in the size of the glutamatergic EPSC of medium spiny neurons, which is known as long-term potentiation (LTP). These observations support the hypothesis that synaptic plasticity of medium spiny neurons underlies reinforcement learning: the cortico?striatal synapses that are active simultaneously with dopamine input are potentiated during reinforcement learning [6,7]. Equally important to theoretical models of reinforcement learning is that synaptic potentiation does not occur in response to dopamine or glutamate signals alone [8]. Nonetheless, the subcellular mechanisms within medium spiny neurons underlying the requirement for paired stimuli are not completely understood.<br>
Synaptic plasticity is controlled by the state of phosphorylation of various components in the intracellular signalling network [9,10]. Phosphorylation of Ser 845 on the GluR1 subunit by cAMP-dependent kinase (PKA) increases insertion of AMPA receptors into the membrane, whereas dephosphorylation by protein phosphatase 1 (PP1) has the opposite effect [11,12]. In the medium spiny neurons of the striatum, the balance between PKA and PP1 is heavily regulated by the phosphoprotein DARPP-32 (dopamine- and cAMP-regulated phosphoprotein of 32 kDa) [13].<br>
The activity of DARPP-32 is regulated mainly by two phosphorylation sites: threonine (Thr) 34 and Thr 75 (rat sequence). When phosphorylated at Thr34, DARPP-32 is a potent inhibitor of PP1 [14], whereas when phosphorylated at Thr75 DARPP-32 inhibits protein kinase A (PKA) [15]. The state of phosphorylation of DARPP-32 has been shown to be regulated by dopamine and several other neurotransmitters, including glutamate [16,17], adenosine [18,19], and opioids [20]. Thr34 is phosphorylated by PKA and dephosphorylated by protein phosphatase 2B (PP2B or calcineurin) [16,21] whereas Thr75 is phosphorylated by the cyclin-dependent kinase 5 (cdk5) and dephosphorylated mainly by PP2A [15,17]. A vast amount of data regarding the regulation of phosphorylation of DARPP-32, as well as its effect on other intracellular proteins, has been generated in the last few years (for review see [13]). Nonetheless, it is not evident how co-activation of multiple neurotransmitters modulates the signalling network.<br>
To gain a better understanding of the mechanisms regulating DARPP-32 phosphorylation and its effect on kinase and phosphatase activity and subsequent plasticity, we developed a computer model of this signalling network, based on the available experimental data. The model allows us to investigate the effect of transient glutamate and dopamine stimuli on levels of signalling molecules and the phosphorylation of DARPP-32; and to identify critical reactions within the network. We find that transient stimulation produces different enzyme activation compared with the prolonged treatments typically used in biochemical experiments. For example, short pulses of calcium influx increase DARPP-32 phosphorylation at Thr34 compared with the decrease seen with prolonged stimulation. Another finding is that the feedback loop PKA?PP2A?phosphoThr75 does not exclusively reinforce the PKA pathway, as previously hypothesized, but at times acts as a sink for catalytic PKA, dampening the stimulatory effect of dopamine.<br>
<br>
Results<br>
Model Verification: Response to Steady-State Inputs<br>
The model, illustrated in Figure 1, consists of ordinary differential equations describing mass action kinetics of biochemical reactions known to occur in striatal medium spiny neurons. Tables 1?4 list these reactions, as well as their rate constants and the quantities of molecules used in the model. Equations are derived assuming all reactions are in a single compartment (i.e., a spine).<br>
Several simulations were designed to replicate biochemical experiments in order to compare the model results with ?steady-state? experiments, in which the phosphorylation levels were measured after a few minutes. Some simulations were performed for the final adjustment of parameters, and others were performed for model verification. The simulations included 1) a prolonged increase in dopamine concentration, replicating bath application [18,21,22]; 2) a prolonged increase in intracellular calcium concentration, replicating bath application of NMDA and AMPA agonists [16,17]; and 3) a prolonged increase in both dopamine and calcium concentration, replicating the study of Snyder et al. [23].<br>
A simulated increase in dopamine from 10 nM (the basal level [24]) to 10 ?M increases the levels of phosphoThr34 9-fold within 2 min, comparable with the experimental observation of a 6?9-fold increase within 2 to 4 min [17] (Figure 2A1, solid line). In parallel, simulated phosphoThr75 decreases to 60% of basal (Figure 2A2, solid line), in accordance with published data [15]. The mechanism behind the downregulation of phosphoThr75 by dopamine is not known; however, evidence suggests that dopamine acts by increasing the dephosphorylation of phosphoThr75 by PP2A [17]. Other experiments show that PP2A activity is enhanced when it is phosphorylated by PKA [25]. Additional model simulations show that enhanced activation of PP2A via phosphorylation by PKA is required to produce the decrease in phosphoThr75. If this pathway is omitted, a sustained stimulation of the D1 receptor instead leads to a small increase in phosphoThr75 (Figure 2A2, dashed line) in contrast to experimental data, though the increase in phosphoThr34 qualitatively remains unchanged. Furthermore, a total removal of PKAc (e.g., by setting the basal dopamine level to zero) causes a significant elevation of phosphoThr75 in the model (unpublished data). This is in good agreement with experimental data, where dopamine depletion causes an increase in phosphoThr75 [26].<br>
A sustained increase in intracellular calcium concentration, as results from prolonged activation of NMDA or voltage-dependent calcium channels, leads to a decrease in the levels of phosphoThr34. In response to a 300-nM calcium signal, both phosphoThr34 and phosphoThr75 decrease to approximately half the basal level within 1 min (Figure 2B, solid lines), in agreement with experimental data [16,17,27]. The decrease in phospoThr34 is due to the calcium-dependent activation of PP2B. In contrast, the dephosphorylation of Thr75 is dependent on calcium activation of PP2A. If calcium activation of PP2A is eliminated, a calcium elevation does not affect the levels of phosphoThr75 much (Figure 2B2, dotted line). It is important to note that this modulation of PP2A by calcium has a minimal effect on the level of phosphoThr34 (Figure 2A1 and 2B1, dotted lines).<br>
The model is verified by evaluating the change in phosphoThr75 and phosphoThr34 in response to paired dopamine and calcium elevations. As demonstrated experimentally [23], the calcium increase inhibits the increase in phosphoThr34 caused by dopamine (Figure 3A, solid line), and enhances the decrease in phosphoThr75 (Figure 3B, solid line). The decrease in phosphoThr34 caused by elevated calcium is due to PP2B both in the model (Figure 3, dashed lines) and experimentally. If the level of PP2B is decreased to 10% of control (simulating the effect of cyclosporin A for 20 min [27]), the effect of calcium on Thr34 phosphorylation is eliminated.<br>
The steady-state experiments of either dopamine stimulation or calcium stimulation provided values for unconstrained rate constants; however, no rate constants in the model were adjusted while simulating the experiments with paired dopamine and calcium inputs. Thus, comparison of these simulations with experiments represents an authentic verification of the model. Taken together, these results confirm that our model, based on available biochemical data from many independent studies of signalling pathways in the striatum, reproduces the changes in the phosphorylation levels of DARPP-32 at Thr34 and Thr75 produced by activation of dopamine-regulated second messenger pathways and changes in intracellular calcium concentration.<br>
<br>
Response to Transient Inputs Differs from Response to Steady-State Inputs<br>
In vivo, physiological elevations in calcium and dopamine are transient. Dopamine is transiently elevated subsequent to burst firing of nigral or ventral tegmental neurons in response to reward or expectation of reward [3]. Calcium is transiently elevated during striatal up-states [28?30], which are caused by periods of high frequency cortical glutamatergic inputs [31,32]. Since the changes in concentration of most signalling molecules are impossible to measure with this high time-resolution, computer simulations may help elucidate the function of complex second messenger pathways. Thus, the verified model is used to investigate the change in phosphoThr34 and phosphoThr75 in response to transient inputs.<br>
A brief dopamine pulse (Figure 4A, solid line; peak = 1 ?M, half-width = 0.6?s), comparable to release evoked by a dopamine neuron burst [24,33,34] leads to a small, but sustained increase in free catalytic PKA (PKAc) (Figure 4A3) and a slightly longer increase in phosphoThr34 (Figure 4A4). If the same amount of dopamine is presented as a slow, low concentration signal (peak = 100 nM, half-width = 7 s; Figure 4A1, dashed line), the total amount of cAMP production is similar to that for the brief, high concentration input, although with a different time course that matches the dopamine signal (Figure 4A2). The changes in PKAc level (Figure 4A3) and in phosphoThr34 (Figure 4A4) are almost identical in the brief, high concentration dopamine input: not only are the amounts of free catalytic PKA and phosphoThr34 quite similar, but also the time courses are the same. Thus, PKAc activation acts as a temporal integrator of the cAMP signal.<br>
A transient calcium elevation produces changes in several downstream enzymes (Figure 4B). PP2B is activated by Ca4CaM and dephosphorylates DARPP-32 at Thr34. This is the dominant effect of a slow, low concentration calcium signal (peak = 0.2 ?M, half with 5 s; Figure 4B1, dashed line), leading to the expected and previously described decrease in Thr34 phoshorylation (Figure 4B4, dashed line). In addition, Ca4CaM also activates PDE1, and inhibits adenylate cyclase; both of these factors contribute to reduction of cAMP below basal levels (Figure 4B2). Though the decrease in cAMP concentration is expected to decrease the level of PKAc, this is not observed in the simulations because the calcium-dependent activation of PP2A and consequent dephosphorylation of phosphoThr75 releases PKAc from inhibition in a positive feedback loop involving PKA?PP2A?phosphoThr75 (Figure 1; further explained below). In the case of a fast and high-amplitude calcium signal (peak = 2 ?M, half-width = 250 ms; Figure 4B1), the free PKAc even increases transiently despite the decreased cAMP (Figure 4B3, solid line) because the positive feedback loop dominates over the cAMP decrease. The relative importance of this feedback loop is due to the large basal level of phosphoThr75 and thus significant PKAc inhibition. The resulting increase in free PKAc following its release from phosphoThr75 further contributes to an increase in phosphoThr34, with a higher peak value (though shorter half-life) than that caused by dopamine stimulation (Figure 4B4, solid line). These results show that a very brief calcium influx can actually lead to an increase in phosphorylation at Thr34, contrary to the results observed with steady-state calcium signals. This unexpected difference between transient and steady-state effects of calcium is further investigated below with paired transient dopamine and calcium stimuli.<br>
<br>
Repeated Paired Stimuli Produce Enhanced PKAc and phosphoThr34<br>
During reinforcement learning, striatal neurons receive paired glutamate and dopamine input. Since several-to-many trials are required for the animal to learn the association, the response of PKAc and phosphoThr34 to repeated paired presentations is of interest. In all the following simulations we have used the same input pulses as described above. Repeated transient dopamine pulses, with temporal properties consistent with reinforcement learning trials (e.g., eight brief pulses at 20-s intervals) produces an accumulation of PKAc (Figure 5A, green dashed line), and significantly increases the phosphorylation of DARPP-32 at Thr34 (Figure 5B). When a fast calcium pulse (as illustrated in Figure 4B1) is given with each dopamine elevation, as would happen with simultaneous release from the glutamatergic and dopaminergic terminals, the activation of PKA is greatly enhanced compared with the response to dopamine alone. The increase in free PKAc due to paired stimuli is 40% greater than the increase observed with dopamine alone, whereas the peak increase in phosphoThr34 is more than doubled (Figure 5A and 5B, solid red lines). The increase in Thr34 phosphorylation in its turn decreases PP1 (Figure 5C), and thus the ratio of PKAc to PP1 is greatly enhanced (Figure 5D) compared with dopamine alone. Since the phosphorylation state depends on the balance between kinases and phosphatases, the increase in PKAc to PP1 is likely to produce a significant change in several target molecules, as has been demonstrated for AMPA receptors [35].<br>
This calcium enhancement of the response to dopamine is in contrast to the effect seen with steady-state inputs in which steady-state calcium inhibits the steady-state dopamine response (compare Figure 3). The calcium pulse given simultaneously with the dopamine input also significantly enhances the decrease in phosphoThr75 (Figure 5E) due to the additional calcium-dependent activation of PP2A, which further elevates PP2A activity. This reduction in phosphoThr75, due to transient dopamine alone, is much smaller than the reduction observed with the high-amplitude (10 ?M) steady-state dopamine input used in some experiments (compare Figure 2A2).<br>
In reinforcement learning paradigms, the reward is given after the visual and motor stimuli; thus the dopamine stimulus occurs after the glutamate stimulus. If the reward occurs prior to the visual and motor stimuli, the animal does not learn. Thus, if the second messenger pathways in the model completely explain synaptic plasticity underlying reward learning, the model should be sensitive to interstimulus interval (ISI). Simulations were performed with the dopamine signal occurring prior to calcium (negative ISI) and after the calcium pulse (positive ISI). Large negative ISIs produce an elevation in PKAc comparable with a positive ISI (unpublished data), which is not consistent with behavior. Thus, the model's sensitivity to ISI does not match that of the behavior. This leads to the prediction that although the model reproduces the cooperativity between glutamate and dopamine, additional mechanisms are required to provide temporal sensitivity.<br>
Despite the larger increase in free PKAc with combined calcium and dopamine inputs, the total amount of PKAc produced (both free and bound to phosphoThr75) is still larger with dopamine alone (Figure 5F). That less PKAc becomes activated in the presence of increased levels of calcium may be expected because of the calcium-dependent AC5 inhibition as well as calcium-dependent PDE1 activation. Free PKAc levels increase following combined brief calcium and dopamine inputs because less PKAc becomes bound to phosphoThr75. This is further explained by the presence of the PKA?PP2A?phosphoThr75 loop.<br>
<br>
Feedback Loop Modulates Dopamine-Stimulated PKAc and phosphoThr34<br>
PKAc enhancement of PP2A activity (reaction (i) in Figure 6E); PP2A dephosphorylation of phosphoThr75 (reaction (ii)); and subsequent decreased inhibition of PKAc (reaction (iii)) can act on PKAc as a positive feedback loop. This loop is believed to enhance the levels of free PKAc, which would then enhance phosphorylation of DARPP-32 on Thr34 in response to dopamine stimulation [13]. To test this hypothesis, model simulations, using the same dopamine and calcium pulses as described above, are repeated with this positive feedback loop opened, by setting to zero the rate constants for reactions (i) and (iii). Removing the phosphorylation of PP2A by PKAc should decrease the modulation of phosphoThr75, and reduce the disinhibition of PKAc.<br>
Surprisingly, opening the feedback loop increases the concentration of both PKAc (Figure 6A1, orange line) and phosphoThr34 (Figure 6A2, orange line) compared with the control condition (Figure 6A, black lines). Eliminating just the phosphorylation of PP2A by PKAc (reaction (i), magenta lines), or eliminating just the inhibition of PKA by phosphoThr75 (reaction (iii), light blue lines), produces a small increase in PKAc, but eliminating both reactions produces a dramatically larger increase in PKAc. The supralinear effect of these two feedback loop reactions suggest that the apparent inhibitory effect of the loop following a transient dopamine input is due to a change in equilibrium in the reactions involving PKAc. As more PKAc is formed, much of it is bound to PP2A in the enzyme-substrate complex, as well as to phosphoThr75 in the inhibitory complex (Figure S1A). Thus these two forms of bound PKA act as a sink for free PKAc, dampening the stimulatory effect of dopamine on the increase in the free PKAc levels.<br>
Eight transient calcium pulses cause an increase in both PKAc and phosphoThr34 (Figure 6B, black lines) that is comparable to the increase caused with eight pulses of dopamine alone (compare Figure 6A). Because calcium increases PP2A activity, it is possible that the feedback loop is involved in this response. Simulations show that when the feedback loop is partly or completely opened (Figure 6B), the response of PKAc and phosphoThr34 to a brief calcium input is dramatically reduced. Opening up both reactions in the loop actually transforms the effect of calcium on PKAc into inhibition (due to inhibition of AC5 and activation of PDE1). Thus, binding of calcium to PP2A has two important effects. First, it reduces the amount of unbound PP2A, and thus reduces the substrate available for PKAc binding. This decrease in PKAc?PP2A (Figure S1B) shifts the equilibrium of reaction (i), thus PKAc dissociates from PP2A. Second, the enhanced PP2Ac activity dephosphorylates phosphoThr75 and disinhibits PKAc. The quantity of PKAc?phosphoThr75 decreases (Figure S1B), reflecting a shift in the equilibrium of reaction (iii), as PKAc dissociates from phosphoThr75. The stimulatory effect of calcium on PKAc requires the activation of PP2Ac, dephosphorylation of Thr75, and disinhibition of PKAc. Opening reaction (i) eliminates the reduction of PKAc-PP2A by calcium, and opening reaction (iii) eliminates the reduction of PKAc-phosphoThr75 by calcium.<br>
When brief dopamine and calcium pulses are given together (eight paired pulses at 20-s intervals), the calcium-dependent enhancement through the PKA?PP2A?phosphoThr75 loop dominates (Figure 6C). The combination of dopamine and calcium produces a smaller quantity of the PKAc?PP2A complex than dopamine alone (Figure S1C). When reaction (i) and (iii) are set to zero, the reduction in PKAc?PP2A and PKAc?phosphoThr75 complexes are smaller than in the control case. Thus, when rates of reaction (i) and (iii) are set to zero (opening the loop), both PKAc and phosphoThr34 increase less compared with when the loop is fully active.<br>
In the Thr to Ala mutant mouse [36], in which Thr75 cannot be phosphorylated, increasing dopamine levels no longer affect downstream processes such as CREB phosphorylation. To replicate this experiment, phosphoThr75 is set to zero in the model. The new equilibrium has a different basal level of PKAc and phosphoThr34 (unpublished data); however, the increase in PKAc due to paired stimuli is similar to the case when reaction (iii) is eliminated, consistent with experimental results [36,37]. In summary, the PKA?PP2A?phosphoThr75 feedback loop is important for increasing levels of free PKAc and phosphoThr34, but only when transient calcium elevations are present, and not in response to transient inputs of dopamine alone.<br>
Calcium activation of the feedback loop is essential for the enhancement of PKAc and phosphoThr34 in response to paired stimuli. This implies that if calcium-dependent activation of PP2A is eliminated, but the PKA?PP2A?phosphoThr75 loop is kept intact, pairing calcium and dopamine will give smaller elevations of free PKAc and phosphoThr34, than dopamine alone. Furthermore, calcium stimulation alone will reduce both PKAc and phosphoThr34. This is partly due to calcium-dependent inhibition of AC5 and calcium-dependent activation of PDE1, but also because now the PKA?PP2A?phosphoThr75 loop instead works as a sink for PKAc. This prediction is tested with simulations in which the calcium-dependent enhancement of PP2A is eliminated (rate constants set to zero). Figure 6D shows that now calcium alone (blue lines) produces a decrease in both PKAc and phosphoThr34 (qualitatively similar to that observed with the feedback loop eliminated), and calcium paired with dopamine inhibits the production of PKAc and phosphoThr34 (red lines), again similar to that observed with the feedback loop eliminated. Thus, without the calcium-dependent PP2A augmentation, a transient dopamine input alone is predicted to increase both PKAc activation and phosphoThr34 more effectively than dopamine paired with calcium.<br>
<br>
Role of Calcium Dynamics<br>
Many different mechanisms control calcium dynamics in spiny projections neurons. Brief calcium elevations are produced by synaptic activation and backward propagating action potentials. A slower and lower calcium elevation might result from calcium release from intracellular stores. This is one proposed mechanism by which the postsynaptic dopamine D2 receptor acts [38]. Therefore, to evaluate the effect of calcium dynamics, simulations were repeated using a slower and lower calcium elevation (as in Figure 4B1, dashed line). When slow calcium inputs are paired with an increase in cAMP, the buildup of free PKAc is unchanged or minimally decreased compared with cAMP increase alone (Figure 7A), instead of enhanced as when faster, larger calcium pulses are used (compare Figure 5). In addition, the buildup of phosphoThr34 is decreased (Figure 7B).<br>
These differences between a brief, high-amplitude calcium input and a slower, low-amplitude calcium input are due to the different effects that these calcium signals have on the level of PP2B and PP2A activation. Both the slow and the fast calcium inputs activate PP2B, though with a more prolonged time course if a longer calcium elevation occurs (Figure 7C, dashed line). But only the briefer and high concentration calcium activates PP2A significantly (Figure 7D, solid line). The slow calcium pulse therefore does not enhance dephosphorylation at Thr75, and thus does not enhance PKAc.<br>
<br>
The Main Simulation Results Are Independent of Model Parameters<br>
The main findings from these simulations suggest that the presence of the PKA?PP2A?phosphoThr75 loop causes increased formation of free PKAc as well as phosphoThr34 if a dopamine input is paired with a transient calcium elevation. To investigate the robustness of this result, simulations are repeated with variations in the least constrained parameters. Though most rate constants are constrained by direct measurements of affinity, the dissociation constants of some reactions have not been reported. In particular, the dissociation rate for calcium binding to AC5, calcium?calmodulin binding to PDE1, and calcium-dependent activation of PP2A is not known. Thus, simulations are repeated using a range of dissociation rate constants for each of these reactions. Figure 8A and 8B shows that, over a large range of dissociation rates for calcium binding to AC5 or Ca4CaM binding to PDE1, the PKAc elevation and PP1 suppression in response to a dopamine signal is enhanced when dopamine is paired with calcium. The model is mildly sensitive to the rate of calcium-dependent enhancement of PP2A activity. If the activation and deactivation kinetics are much faster (100? as fast as control values), paired dopamine and calcium inputs are no more effective than dopamine alone (Figure 8C). With a fast dissociation rate, the deactivation of the calcium-activated PP2A (PP2Ac) occurs too rapidly to allow PP2Ac to dephosphorylate phosphoThr75 during brief calcium elevations. Also, if the calcium activation is very slow, then very little calcium-dependent activation of PP2A occurs during the brief duration of the calcium inputs, and the effect of paired stimulation decreases. A 100? slower kinetics of calcium-induced PP2A stimulation is, however, not in accordance with results from Nishi et al. [17], in which a significant dephosphorylation of phosphoThr75 occurs within 5 min.<br>
Though the quantity of DARPP-32 has been estimated [37], enzyme quantities are typically measured from tissue volumes which include both neurons and non-neuronal cells. The actions of anchoring proteins for molecules such as PKA suggest that the effective quantities of these enzymes in the synapse may be much higher than published estimates [39]. Thus, simulations are repeated using both higher and lower quantities of the enzymes (PKA, cdk5, PP2A, PP2B) to determine to what extent the results are sensitive to these parameters. Figure 9A1 shows that, even with a 10-fold increase or decrease in enzyme quantity, calcium paired with dopamine produces a larger PKAc-to-PP1 ratio than dopamine alone. Simulations in which the quantity of single enzymes or pairs of enzymes was changed did not appreciably change the results. Since the ratio PKAc:PP1 controls AMPA channel phosphorylation, this larger increase in PKAc:PP1 would translate into more LTP due to paired stimulation.<br>
Nonetheless, the signalling network exhibits changes in activity due to the change in enzyme quantity. Specifically, both the basal and stimulated levels of PKAc are altered. Figure 9A2 (dashed lines) shows that both basal and stimulated PKAc decrease when enzyme quantities are decreased to 10% of the control values, and that both basal and stimulated PKAc increase when enzyme quantities are increased. In contrast, the basal and stimulated levels of phosphoThr34 increase with a decrease in enzyme concentration (Figure 9A2, dotted lines). In summary, though the basal level of PKAc:PP1 changes with a change in enzyme quantity, the basic principle, that paired stimulation produces a larger increase than either calcium or dopamine alone, is robust to enzyme quantity.<br>
To further demonstrate the importance of DARPP-32 in the integration of signalling pathways, simulations were repeated with parameter variations designed to make DARPP-32 independent of its regulatory molecules. The total concentration of PKA, PP2A, PP2B, and cdk5 were set to 10% of control values, and the dynamics of the PKA system were accelerated, i.e., the rate constants for all reactions involving PKA were increased 100-fold. These changes decreased the signal integration capability of PKA, and exposed the signal integration capability of DARPP-32.<br>
As seen in Figure 9B1, the amplitude of the PKAc response increases since it is activated more efficiently by cAMP. In addition, PKAc decreases almost to basal level between each pair of dopamine pulses. In contrast, phosphoThr34 accumulates during the eight pulse stimulus (Figure 9B2), just as in the control model. More importantly, though calcium paired with dopamine produces a slight decrease in free PKAc, it produces an increase in phosphoThr34 (Figure 9B2). Thus, independent of the ability of PKAc to integrate dopamine and calcium signals, phosphoThr34 also integrates dopamine and calcium signals.<br>
Last, we evaluated whether the results were contingent upon the high DARPP-32 concentration. Reducing DARPP-32 to 10 ?M or 1 ?M produced a change in the basal level of PKAc, phosphoThr34, and PP1, but produced very little change in the ratio PKAc:PP1. More importantly, the increase in PKAc:PP1 in response to paired stimulation remains greater than the response to dopamine alone (Figure S2A), similar to simulations in which other enzymes were increased. The relative increase due to paired stimulation compared with the sum of dopamine and calcium inputs alone is larger with the higher DARPP-32 concentration compared with the lower, suggesting that the high concentration of DARPP-32 in striatal neurons that has been measured may have a physiological relevance. Nonetheless, these simulations further confirm the robustness to parameter variations of the main result, namely that combined transient inputs are more effective.<br>
<br>
<br>
Discussion<br>
To better understand the complex intracellular signalling networks underlying synaptic plasticity and reinforcement learning, we have developed a model of the calcium and cAMP signalling cascades that regulate DARPP-32 phosphorylation. The model is based on published biochemical data: the simulated regulation of DARPP-32 phosphorylation by sustained G-protein receptor activation and calcium influx fits experimental data, increasing and decreasing phosphoThr34, respectively (Figures 2 and 3).<br>
Interestingly, our model suggests that the effect of transient calcium influx is very different from sustained calcium elevation and that short bursts of calcium influx actually increase DARPP-32 phosphorylation on Thr34 (Figure 4). More significantly, combined dopamine and glutamate stimulation produces a larger activation of PKA and inhibition of PP1 than dopamine alone (Figure 5). This is potentially important as both nigrostriatal dopaminergic input and corticostriatal glutamatergic input often terminate on heads and necks of dendritic spines [40,41]. Since phosphorylation state depends on the balance between kinases and phosphatases, the increase in PKAc to PP1 is likely to produce a significant change in several target molecules, such as GABA [42] receptors, as well as sodium [43] and calcium channels [44], which will modify the subsequent response to synaptic stimulation. In particular, D1 receptor activation, producing PKAc, phosphorylation of DARPP-32, and subsequent inhibition of PP1 activates kinases that phosphorylate AMPA channels [45], leading to synaptic potentiation. Simulations further demonstrated that the dephosphorylation of DARPP-32 at Thr75 can enhance PKA activity through a disinhibition loop, as has been suggested, but only during calcium influx alone or in combination with dopamine. This loop instead works as a sink when transient dopamine inputs are used and dampens the increase in free PKAc (Figure 6).<br>
The Role of DARPP-32 in Reinforcement Learning and LTP<br>
The concomitant increases in calcium and dopamine are behaviourally relevant stimuli in that they occur during reinforcement learning. Visual stimuli and motor action cause corticostriatal fibers to release glutamate; the subsequent reward is accompanied by dopamine release from nigrostriatal fibers. Thus, it is possible that during reinforcement learning, the calcium elevation produced by glutamate stimulation, together with cAMP produced by dopamine stimulation, induces synaptic plasticity, which underlies learning the association between the visual stimulus and reward.<br>
Theoretical models of reinforcement learning, e.g., temporal difference learning [8,46,47], posit that the striatum learns which among multiple actions produces the greatest future rewards. Memories of rewarded motor actions may be stored as potentiated corticostriatal synapses. During learning, those synapses that release glutamate when dopamine arrives are potentiated. Subsequently, glutamate stimulation alone evokes output from those neurons encoding the reinforced motor action.<br>
Experimental support for temporal difference learning is rather strong. Experiments from striatal slices show that release of glutamate from corticostriatal fibers produces LTP in the presence of dopamine. Neither dopamine alone nor cortical stimulation alone produces LTP [48?50], and corticostriatal LTP is blocked in animals lacking DARPP-32 [49]. Activated PKA is known to mediate phosphorylation of Ser845 on the AMPA receptor GluR1 subunit [51,52], leading to insertion of AMPA receptors on the cell surface [45,53,54].<br>
The model results presented here help explain the mechanisms underlying temporal difference learning. The prevailing view, based on biochemical experiments used to derive our model, has been that glutamate and dopamine have opposite effects on the PKA signalling cascade. Calcium decreases PKA activity, and increases PP1 activity, whereas dopamine does the opposite. In contrast, the model shows that transient calcium stimulation can enhance the PKA signalling cascade. Thus, when transient dopamine and glutamate are given together, the increase in the ratio of PKA to PP1 activity is enhanced compared with when dopamine is given alone (Figure 5). Thus, model simulations suggest that PKA activation, phosphorylation of DARPP-32 at Thr34, and subsequent PP1 inhibition, produced by conjunctive glutamate and D1 receptor stimulation, can produce LTP by increased phosphorylation of AMPA receptors. This suggests that the cellular mechanisms leading to temporal difference learning is the enhanced stimulation of PKA with paired calcium and dopamine stimuli. Thus, at a network level, when a subset of spiny projection neurons are activated by glutamate and dopamine simultaneously, corticostriatal synapses of those spiny projection neurons are strengthened; subsequently, glutamate stimulation alone evokes output from the correct neurons.<br>
The dynamics of the calcium transient controls the effect of calcium on the dopamine induced PKAc elevation. A persistent calcium elevation has a minimal effect on PKAc and decreases phosphoThr34, whereas a brief transient calcium elevation produces an increase in both PKAc and phosphoThr34. These results lend significance to the calcium imaging studies [29,30] which show that a backward propagating action potential enhances the calcium elevation during an up-state. This suggests that synaptic plasticity may require not only coincident glutamate and dopamine inputs, but also sufficient spiny projection neuron activity to produce a backward propagating action potential and the accompanying fast, high-amplitude calcium transient. This requirement for glutamate, dopamine, and a backward propagating action potential is the ?three-factor rule? [7,55].<br>
Nonetheless, the response to paired calcium and dopamine stimuli did not match the sensitivity to ISI as seen with behavior. Several other mechanisms within spiny projection neurons may be responsible for this sensitivity.<br>
1) G protein coupled receptors such as mGluR and mAChR produce diacylglycerol and inositol triphosphate leading to calcium release from intracellular stores. Calcium release plays a role in sensitivity to ISI in cerebellar Purkinje cells [56,57]; and diacylglycerol activates protein kinase C, which releases calmodulin (CaM) from neurogranin [58]. When included in future versions of the medium spiny neuron model, these mechanisms may produce sensitivity to ISI.<br>
2) In medium spiny neurons, various receptors and enzymes are localized to different compartments. For example, calcium influx through NMDA receptors occurs at the postsynaptic density, and dopamine receptors are distributed not only on the spines, but also on the dendritic shaft [59]. Anchoring of molecules such as CaMKII and PKA at the postsynaptic density [39,60] and diffuse distribution of other molecules such as PP2A implies that diffusion plays a role in the interaction between dopamine and calcium-activated pathways. This diffusion, by introducing temporal delays, may produce sensitivity to ISI. In addition to these intraneuronal mechanisms, it is possible that network interactions or presynaptic dopamine receptors may produce the ISI sensitivity observed behaviourally.<br>
<br>
Transient and Sustained Calcium Have Different Effects on DARPP-32 Phosphorylation at Thr34<br>
The nature of biochemical experiments makes it difficult to measure phosphorylation state with high time-resolution. Thus, the simulation result that transient calcium enhances phosphoThr34 is a prediction of the model (Figures 4?6). Recently, Nishi et al. used glutamate for brief stimulations (15 s) and did see an increase in phosphorylation at Thr34 [61]. Although they suggest this increase is mediated indirectly, via release of nitrous oxide and production of cGMP, model simulations suggest that it can also be an effect of calcium increase through NMDA receptors.<br>
The mechanism of action of D2 receptors in the striatum is unclear, since both inhibition of adenylate cyclase [62] and release of intracellular calcium [38,63] has been proposed. We show here that slow calcium transients, as occurs with calcium release from intracellular stores, are able to dephosphorylate Thr34. This may explain observations that activation of D2 receptors decreases phosphoThr34 and decreases the phosphorylation of PP1 substrates [21,64].<br>
The dual role of Thr75 phosphorylation.<br>
The disinhibition of PKA when phosphoThr75 is dephosphorylated has been suggested to be an important part of the potentiating effect of DARPP-32 on PKA activity [65]: PKA enhances PP2A activity via phosphorylation; then PP2Ap dephosphorylates phosphoThr75 at an accelerated rate and the decrease in phosphoThr75 leads to more active PKA. Nonetheless, during transient dopamine inputs alone, the loop acts as a sink, in which binding of PKAc to PP2A or phosphoThr75 actually decreases the amount of free PKAc, and thereby decreases the ability of PKAc to phosphorylate DARPP-32 on Thr34. In contrast, the stimulation of PP2A through calcium shifts the balance of the loop, increasing dephosphorylation of phosphoThr75 and thus disinhibiting PKA. Figure 10 illustrates these two alternative modes of operation of this loop. The relative changes in the reaction flows are dependent on the biochemical reactions involving PKAc (Figure S1 further illustrates the quantitative role that PP2A substrate depletion and phosphoThr75 inhibition of free PKAc play in the feedback loop). An important difference between transient and steady-state calcium inputs is predicted by the model, due to the relative activity of PP2A versus PP2B. Steady-state calcium produces a large increase in PP2B, dephosphorylating phosphoThr34; in contrast, large, transient calcium elevations produce an increase in PP2A activation and consequent disinhibition of free PKAc due to phosphoThr75 dephosphorylation. The net effects of these different calcium inputs on the PKAc:PP1 balance are summarized in Figure S2B. If calcium is prevented from binding to and enhancing PP2A activity, then a transient calcium stimulation does not increase PKAc activity, but instead decreases PKAc. Similarly, if PP2Ac is prevented from dephosphorylating DARPP-32 on Thr75, then instead of an increase in PKAc activation following calcium stimulation, the amount of activated PKAc is decreased (compare Figure 6D).<br>
One validation of the importance of phosphoThr75 in the action of calcium can be found in the study of different psychotomimetics in mice in which DARPP-32 has been genetically modified at either Thr34 or Thr75 so that these sites cannot be phosphorylated [36]. Phencyclidine (PCP), a drug that affects glutamate transmission and calcium influx, is the only drug whose effects are modified in the mice where Thr75 could not be phosphorylated, whereas lysergic acid diethylamide (LSD), which alters serotonergic transmission, and amphetamine, which alters dopamine transmission, have the usual effect on behavior in these animals.<br>
<br>
<br>
Regulation of Phosphatases Shape the Response of the Signalling Networks<br>
The regulation of kinases such as PKA and CaMKII has been studied in great detail, whereas much less attention has been given to the regulation of phosphatase activity. Nonetheless, our model, together with other modeling work [66?68], clearly suggests that the regulation of protein phosphatases is very important to shape the response in signalling networks. In the DARPP-32 signalling pathway, dynamic regulation of the level of phosphoThr75 by PP2A is essential. To reproduce experimental data on DARPP-32 phosphorylation, the regulation of PP2A by both calcium and dopamine is required. Although only PKA regulation has been directly proven, experiments show that glutamate inhibits phosphoThr75 [17] via a PP2A and calcium-dependent pathway. The importance of this pathway for mediating the cooperative action of calcium and dopamine suggests that future experimental demonstration and kinetic characterization of calcium-activated PP2A is critical.<br>
<br>
Results Are Robust to Variations in Parameters<br>
A common criticism of this type of model is that simulation results are sensitive to parameters, and that insufficient data is available to constrain the parameters. To preempt this criticism, several simulations demonstrate that the present model is robust to variation in the unconstrained parameters.<br>
The result that a transient calcium elevation can increase phosphoThr34 was shown with a previous model [69]. This earlier result was critically dependent on the presence of a Ca4CaM-stimulated form of AC; however, the predominant form of AC in striatum is type 5 [70,71], which is not stimulated by calcium. Incorporating the effect of phosphoThr75 and the regulation of PP2A in our model made the stimulatory effect of calcium robust to parameter variations (Figure 8).<br>
A potential source of error in this model is the concentration of various components. Although the concentration of many of the enzymes has been published, the data usually comes from crude homogenates, and does not take into account compartmentalization or scaffolding of molecules. As shown in Figure 9, however, the most important conclusions from this model are qualitatively robust, even when enzyme concentrations are varied 10-fold.<br>
Experiments have not shown directly whether the phosphorylation of Thr34 influences the rate of phosphorylation of Thr75. It is known, however, that phosphorylation of Ser 102 and Ser 137 influences the rate of phosphorylation of Thr34 [72,73], indicating that intramolecular mechanisms are important. Model simulations justified the constraint that DARPP-32 can only be phosphorylated at one threonine residue at a time. An alternative model, in which phosphorylation rate constants at one threonine site are lower (1/5) when the other threonine is phosphorylated gave very similar results (unpublished data). Nonetheless, this is an issue that needs to be addressed experimentally.<br>
In conclusion, our results support the idea that subcellular processing, such as coincidence detection, signal amplification, and decision making, based on the interactions within the intracellular networks, are essential for control of neuronal activity [74]. Some of these critical interactions control whether learning and synaptic plasticity will occur in different brain regions (see, e.g., [57,75]). Since it is exceedingly difficult to visualize the dynamics of intracellular signalling pathways, modeling is an important adjunct method for formulating hypotheses regarding the critical steps in these pathways [69,76]. New imaging techniques such as Fluorescence Resonance Energy Transfer (FRET) [77,78], rather than removing the need for dynamic modeling, instead provide essential constraints that improve the veracity of such models. Future modeling approaches that incorporate both this data and compartmentalization of enzymes due to anchoring proteins [35,79?81] will have an increased ability to formulate hypotheses regarding key molecules controlling neuronal dynamics and plasticity in the basal ganglia.<br>
<br>
<br>
Materials and Methods<br>
All reactions in the model are described as protein?protein interactions:<br>
				or as enzymatic reactions written in the Michaelis?Menten form which assumes a nonreversible catalytic step:<br>
				<br>
			<br>
The concentration of the substrate A in reaction 1 is described using a first-order differential equation of the form:<br>
				<br>
			<br>
Solution of this equation requires initial concentrations and the forward (kNf) and backward (kNb) rate constants for reaction N. For cascades of reactions, equations are derived by summing terms describing all reaction pathways leading toward or away from a particular molecule. For example, the rate of change of concentration of AE in Equation 2 is given by:<br>
				[76,82].<br>
			<br>
For protein?protein interactions, the equilibrium constant, Kd, is defined as the ratio of backward-to-forward rate constant: kNb/kNf. For enzymatic reactions, kNcat defining the last, catalytic step, is the rate at which product appears (sometimes called Vmax), and the affinity, KM = (kNcat + kNb)/kNf. When kNb is not known explicitly, kNb is defined as 4 kcat [76,83]. As explained below, all parameters are derived from experimentally measured constants found in publications, or carefully inferred from indirect biochemical studies.<br>
The reactions and rate constants in the model are summarized in Tables 1?3 and Figure 1. The total concentrations of molecules are summarized in Table 4. The equations were programmed in <software>XPPAUT</software> (http://www.math.pitt.edu/~bard/xpp/xpp.html) and run under the UNIX operating system. Simulations used the numerical integration method called ?stiff? with a time step of 0.01?0.1 s.<br>
Dopamine and the G-protein coupled receptor.<br>
The model has a tonic dopamine level of 10 nM, and stimulated or phasic dopamine pulses reach a concentration of 1 ?M. The tonic dopamine maintains both the phosphoThr34 and phosphoThr75 at the basal level observed experimentally (via a PP2A-dependent mechanism, see below). In striatum the postsynaptic dopamine D1 type receptor (D1R) is coupled to the Golf type of GTP-binding protein [84]. The D1R can bind to either the inactive G protein first, and then dopamine, or dopamine first and then the inactive G protein [85?87]. Once the complex is formed, it rapidly dissociates into ligand-receptor complex, inactive G?? subunit, and the active Golf?GTP. Golf?GTP binds to adenylate cyclase (see below), and also autohydrolyzes into Golf?GDP which then binds to G?? to regenerate inactive G protein. Compared to Gs?, Golf? binds to AC with a higher Kd and has a faster rate of hydrolysis [88].<br>
<br>
Cyclic AMP formation and PKA activation.<br>
Active Golf? binds to and activates adenylate cyclase type V (AC5) in the striatum [70,71,87], which produces cyclic AMP (cAMP). A simplified model of the AC5 enzyme reaction was created by combining several intermediate steps from a detailed AC model [89]. The rate constants for the simplified AC5 model were adjusted, using the nonlinear least-squares regression algorithm of <software>Dynafit</software> [90] (http://www.biokin.com/dynafit/). Figure S3 shows that the time course of cAMP production is almost identical to that of the Dessaur model for our conditions [89].<br>
Calcium causes a 50% reduction in the activity of AC5 [91], though it is not known whether calcium binds to AC5 before or after binding to Golf?. In the model, calcium is allowed to bind to the inactive AC5, which then can be activated by Golf?GTP. The results do not differ if calcium also is allowed to bind to active AC5. A step of regeneration of ATP is included that allows for a steady-state concentration of ATP in the absence of stimulation. ATP is only modeled explicitly in the formation of cAMP and not in any other enzymatic phosphorylation step below where it is not assumed to be a rate-limiting substrate.<br>
Several types of phosphodiesterases (PDE) degrade cAMP in the striatum. The model includes the calcium-activated PDE1 [92] and a constitutively active form (referred to as PDE4) that represents PDE4B, PDE10A, and PDE7 [93?96].<br>
cAMP binds to and activates PKA by binding to the regulatory subunit of PKA, releasing two catalytic subunits (PKAc). The regulatory subunit has two binding sites for cAMP, one with high affinity and one with low affinity [97]. These parameters were adjusted in the model to reproduce the activation curve for the holoenzyme [98,99] and the activation rate observed using Fluorescence Resonance Energy Transfer imaging [100].<br>
<br>
Glutamate and calcium-activated enzymes.<br>
Glutamate is modeled by its effect on calcium; thus, glutamate is simulated as a calcium elevation. Calcium binds to CaM, which has four calcium-binding sites: two N sites (fast, high affinity), and two C sites (slow, low affinity). Calcium binds to CaM in pairs, producing the active Ca4CaM in two consecutive steps. The rate constants are obtained from [101,102]. Ca4CaM binds to and activates several molecules in the striatum. The rate constants for PDE1 activation were adjusted to produce 50% activation of PDE1 when model calcium concentration is 400 nM [92,103]. PP2B has a very high affinity for Ca4CaM [104,105]. PP2B has a lower affinity for CaM than for Ca4CaM, and the calcium dissociation rate of PP2B-CaM and PP2B-Ca2CaM is slower than for PP2B-Ca4CaM [106].<br>
CaMKII was included in the model as a buffer for CaM; therefore only three reactions are included: reversible binding of CaM to CaMKII; a slow autophosphorylation into CaMKIIp, in which CaM is trapped; and dephosphorylation of CaMKIIp by PP1. Details of autophosphorylation, e.g., [68], were specifically excluded.<br>
<br>
Phosphorylation of DARPP-32.<br>
DARPP-32 activity is regulated by phosphorylation on two sites: Thr34 and Thr75. The catalytic subunit of PKA catalyses phosphorylation of DARPP-32 at Thr34 [73,107]; Cdk5 catalyses phosphorylation of DARPP-32 at Thr75 [15]. Little is known about the regulation of Cdk5, but experiments show that there is a basal level of phosphorylation at Thr75 due to Cdk5 activity [108]. Thus, a constant amount of active Cdk5 is included in the model, to produce the level of phosphoThr75 (25%) that has been experimentally measured [15]. It is known that phosphorylation of one site can affect phosphorylation rates on other sites of the molecule [72,73]; it therefore seems unlikely that the phosphorylation of Thr34 is independent of phosphorylation of Thr75 [15]. When these sites are assumed independent, allowing simultaneous phosphorylation at Thr34 and Thr75 on the same individual molecule, the results are not compatible with experimental data (unpublished data). Thus, for simplicity, simultaneous phosphorylation on both Thr34 and Thr75 are excluded. We have also chosen not to include phosphorylation at Ser102 and Ser137 since these sites are not known to be directly regulated by dopamine or glutamate.<br>
<br>
Dephosphorylation of DARPP-32 and regulation of PP2A.<br>
Thr34 is dephosphorylated by PP2B when this phosphatase is bound to Ca4CaM [16]; in contrast, Thr75 is dephosphorylated by PP2A [65]. Though PP2A is constitutively active, its activity is enhanced by phosphorylation [25] and calcium [17]. Therefore, the model has three active forms of PP2A: the basal form with a low kcat, a phosphorylated form (PP2Ap, phosphorylated by PKA) with a higher kcat [25], and a form that is activated directly by calcium, PP2Ac. Lacking experimental measurements of specific activity of calcium-activated PP2A, the activity of PP2Ac is set the same as that for phosphorylated PP2A. Furthermore, the calcium-dependent dephosphorylation of Thr34 (by PP2B) and Thr75 (by PP2A) are balanced to give approximately equal relative dephosphorylation levels as well as dephosphorylation dynamics following moderate increases of calcium as in [17,27].<br>
<br>
Activity of phosphorylated DARPP-32.<br>
When DARPP-32 is phosphorylated at Thr75, it binds to and inactivates PKA [15] with an affinity of 2.7 ?M [15]. When DARPP-32 is phosphorylated at Thr34 it binds to and inactivates PP1, with an affinity of 1 nM [14]. Since the Thr34 phosphorylated site of DARPP-32 binds to the active zone of PP1 [109], the rate of dephosphorylation of phosphoThr34 by PP2B is probably decreased when bound to PP1. Nonetheless, the very high affinity of phosphoThr34 for PP1 implies that almost all phosphoThr34 is bound to PP1, and if Thr34 can be dephosphorylated only when unbound, phosphoThr34 increases to the amount of PP1. This result is contrary to experimental data which reveals that the amount of DARPP-32 phosphorylated on Thr34 is less than 1% [27,110]. Thus the model uses the same dephosphorylation rates for PP1 bound and unbound phosphoThr34 to reproduce experimental measurements. No qualitative changes in the results are seen if the dephosphorylation rate is decreased 50% when phosphoThr34 is bound to PP1. In contrast, due to the much lower affinity of phosphoThr75 for PKA as well as the higher amount of phosphoThr75 in the system (basal level 13 ?M), results do not change significantly by assuming that phosphoThr75 bound to PKAc can be dephosphorylated by the different PP2A forms. Thus, for simplicity, only unbound phosphoThr75 can be dephosphorylated in the model.<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1570177</b><br>
Mathematical Modeling Identifies Inhibitors of Apoptosis as Mediators of Positive Feedback and Bistability<br>
The intrinsic, or mitochondrial, pathway of caspase activation is essential for apoptosis induction by various stimuli including cytotoxic stress. It depends on the cellular context, whether cytochrome c released from mitochondria induces caspase activation gradually or in an all-or-none fashion, and whether caspase activation irreversibly commits cells to apoptosis. By analyzing a quantitative kinetic model, we show that inhibition of caspase-3 (Casp3) and Casp9 by inhibitors of apoptosis (IAPs) results in an implicit positive feedback, since cleaved Casp3 augments its own activation by sequestering IAPs away from Casp9. We demonstrate that this positive feedback brings about bistability (i.e., all-or-none behaviour), and that it cooperates with Casp3-mediated feedback cleavage of Casp9 to generate irreversibility in caspase activation. Our calculations also unravel how cell-specific protein expression brings about the observed qualitative differences in caspase activation (gradual versus all-or-none and reversible versus irreversible). Finally, known regulators of the pathway are shown to efficiently shift the apoptotic threshold stimulus, suggesting that the bistable caspase cascade computes multiple inputs into an all-or-none caspase output. As cellular inhibitory proteins (e.g., IAPs) frequently inhibit consecutive intermediates in cellular signaling cascades (e.g., Casp3 and Casp9), the feedback mechanism described in this paper is likely to be a widespread principle on how cells achieve ultrasensitivity, bistability, and irreversibility.<br>
<br>
Introduction<br>
Apoptosis, an evolutionary conserved form of cell suicide, allows multicellular organisms to eliminate damaged or excess cells in order to maintain tissue homeostasis. Dysregulation of apoptosis is associated with various pathological conditions, including cancer and neurodegenerative disorders. Aspartate-specific cysteine proteases, also known as caspases, are the central executioners of apoptosis. In most cases, apoptotic stimuli activate initiator caspases, whose substrates, the effector caspases, ultimatively cause cellular demise by cleaving various cellular substrates [1].<br>
Figure 1A schematically depicts the so-called extrinsic and intrinsic apoptotic pathways that elicit apoptosis by cleaving and thereby activating caspase-3 (Casp3), the major cellular effector caspase. The extrinsic pathway is initiated by ligand-binding to death receptors (e.g., CD95), which then oligomerize and recruit various proteins, including pro-Casp8, into the so-called death-inducing signaling complex. Formation of the death-inducing signaling complex leads to autoprocessing of pro-Casp8 into active (initiator) Casp8, which then cleaves (effector) Casp3. Cytotoxic stress or death-receptor?stimulated Casp8 engage the intrinsic, or mitochondrial, apoptosis pathway by inducing the translocation of proapoptotic Bcl-2 family members such as Bax and Bid to mitochondria. This event, which is negatively regulated by antiapoptotic Bcl-2 family members (e.g., Bcl-2), results in the release of proapoptotic proteins (cytochrome c [cyto c] and Smac) from mitochondria into the cytosol. Cytosolic cyto c then elicits the oligomerization of Apaf-1 into an active high-molecular-weight complex, the apoptosome, which recruits and stimulates (initiator) Casp9, and thereby allows activation of effector caspases such as Casp3. Smac and inhibitors of apoptosis (IAPs) such as X-linked IAP (XIAP) establish an additional layer of regulation in the intrinsic pathway: XIAP inhibits the catalytic activities of Casp9 and Casp3 through reversible binding, and cytosolic Smac relieves this inhibition by sequestering XIAP away from caspases [2].<br>
Experimental studies revealed that the qualitative behaviour of caspase activation in the intrinsic pathway depends on the cellular context. Cyto c added to cytosolic extracts activates Casp3 in an all-or-none fashion in some cells [3?7], while gradual activation was observed in other systems [8?10]. As cyto-c release from mitochondria can be a reversible event [11], which does not affect mitochondrial function [12?14], it has been suggested that downstream caspase activation irreversibly commits cells to apoptosis [15,16]. Accordingly, cyto c?induced Casp3 activation remained elevated even after a strong decline in cytosolic cyto c [17] or after apo?cyto c, an inhibitor of apoptosome formation, was added [18]. Furthermore, the time course of caspase activation via the intrinsic pathway equals that for irreversible commitment to apoptosis [15,16], and caspase-inhibition allows for long-term cellular recovery and/or proliferation after removal of apoptotic stimuli [15,16,19?22]. Finally, Fas-treated Jurkat T cells, which enter apoptosis by the intrinsic pathway, escaped commitment to death as judged by maintenance of clonogenic potential if Casp3 was inhibited [23]. On the contrary, Casp3 activation was found to be a reversible event in glycochenodeoxycholate-treated hepatocytes [24].<br>
These qualitative differences in caspase activation suggest that the intrinsic pathway is bistable in some cells, but monostable in others. While simple monostable systems respond in a gradual and reversible manner, complex bistable systems exhibit true all-or-none responses and in some cases irreversibility. Bistability is thought to require a positive circuit, which may be established either by positive feedback or by double-negative feedback. Once a threshold stimulus is exceeded, such positive circuits allow bistable systems to switch from low activation levels (off state) to high activation levels (on state) in an all-or-none fashion. Bistable systems display hysteresis, meaning that different stimulus-response curves are obtained depending upon whether the system began in its off or its on state. In some cases, the on state is maintained indefinitely after the stimulus is removed, so that the system shows irreversible activation [25]. Experimental studies confirmed that bistability indeed occurs in natural and artificial biological networks [25?29].<br>
Recent mathematical modeling demonstrated that bistability can arise from ?hidden,? or implicit, feedback loops that are usually not explicitly drawn in biochemical reaction schemes [30,31]. Similarily, we present a model showing that inhibition of Casp3 and Casp9 by IAPs results in an implicit positive feedback and in bistability. As cellular inhibitory proteins (e.g., IAPs) frequently inhibit consecutive intermediates in cellular signaling cascades (e.g., Casp3 and Casp9), the mechanism described in this paper is likely to be a widespread principle on how cells achieve ultrasensitivity, bistability, and irreversibility (Protocol S5).<br>
<br>
Results<br>
Model Derivation<br>
Based on the published literature, we derived a core model of the intrinsic apoptosis pathway, which includes general regulatory mechanisms, while cell-type?specific events were not taken into account. The gray-shaded area in Figure 1A indicates the regulatory interactions considered in the model: active Apaf-1, which was taken as the input in most simulations, recruits and thereby stimulates (initiator) Casp9. Casp9 then in turn activates the output species, (effector) Casp3, by proteolytic processing. In addition, Casp3-mediated cleavage of Casp9 results in positive feedback amplification. Finally, both Casp3 and Casp9 are subject to stoichiometric inhibition by IAPs. For simplicity, we considered only the most potent caspase inhibitor among the IAP family of proteins, XIAP. The corresponding kinetic scheme is depicted in Figure 1B.<br>
Cyto c released from mitochondria is known to elicit heptamerization of Apaf-1 into active apoptosomes. As detailed kinetic measurements of apoptosome formation are currently lacking, apoptotic stimulation was modeled by altering the total concentration of activated Apaf-1 molecules assembled in apoptosomes (A*tot = A* + A*C9 + A*C9X + A*C9* + A*C9*X). Each active Apaf-1 monomer assembled in apoptosomes was shown to reversibly bind to a single Casp9 molecule [32], and Casp9 is then autoproteolytically processed at amino acid Asp-315 [33]. Importantly, Casp9 autoproteolysis does not affect either enzymatic activity of Casp9 [34] or its recruitment to apoptosomes [35,36]. Because of these data, we did not distinguish between autoproteolytically processed and unprocessed Casp9 in the model.<br>
The enzymatic activity of Casp9 is thought to be determined mainly by apoptosome recruitment, as apoptosome-bound Casp9 was shown to be much more active than free Casp9 [37,38]. Therefore, we assumed in the model that reversible association of Casp9 (C9) and Apaf-1 (A*) (reaction 1) yields a highly active Apaf1?Casp9 complex (A*C9), which cleaves pro-Casp3 (C3) much more efficiently (reaction 3) than free Casp9 (reaction 2; see Table 1). The latter reaction was nevertheless taken into account, since free Casp9 was shown to have significant basal activity towards pro-Casp3 [32].<br>
Processing of pro-Casp3 into mature Casp3 by upstream initiator caspases such as Casp9 was reported to occur by a sequential two-step mechanism: Pro-Casp3, which has negligible enzymatic activity [39], is initially processed by Casp9 into active p12-p20-Casp3, and this intermediate is subsequently autocatalytically cleaved into active p12-p17-Casp3 [40]. As shown in Figure 1B, we modeled Casp3 activation by a single-step mechanism (C3 ? C3*). This seems justified, as the p12-p20-Casp3 intermediate and mature p12-p17-Casp3 exhibit similar catalytic activities [41], and as they are both subject to inhibition by XIAP (see below).<br>
Casp3 is known to cleave its own activator, Casp9, at amino acid Asp-330 in vitro [34,42], and in cytosolic extracts treated with cyto c [5,33]. As Casp9 processing by Casp3 was shown to significantly enhance Casp9 activity [34], feedback cleavage by Casp3 results in autoamplification of the apoptotic signal. The physiological relevance of this positive feedback loop was confirmed in several studies, which showed that inhibition of Casp3-mediated cleavage of Casp9 prevented full activation of both Casp3 and Casp9 in response to cyto c [34,43,44].<br>
Casp3-mediated feedback processing of Casp9 was modeled by assuming that active Casp3 (C3*) cleaves both free and Apaf1-associated Casp9 (reactions 4 and 5), thereby generating the Asp330-cleaved Casp9 species, C9* and A*C9*. These feedback-cleaved Casp9 species in turn cleave pro-Casp3 more efficiently (reactions 6 and 7; see Table 1) when compared with their precursors, C9 and A*C9*, thus establishing a feedback amplification loop. Feedback-processed Casp9 (cleaved at Asp330) was shown to be associated with apoptosomes [34,36], much like its precursors that are not cleaved at Asp330 (see above). Therefore, we assumed in the model that the kinetics of Casp9-binding to Apaf-1 (reactions 1 and 8) are unaffected by Casp3-mediated feedback cleavage (see Table 1).<br>
IAPs such as XIAP act as stoichiometric inhibitors of Casp3 and Casp9 [2], and accordingly caspase inhibition can be described by simple reversible binding [45,46]. Experimental evidence suggests that XIAP can bind to and inhibit Casp9, even if the latter is associated with apoptosomes [34]. Accordingly, we assumed in the model that active Apaf-1 (A*) and XIAP (X) bind to Casp9 in a noncompetitive manner so that Apaf1-bound Casp9 intermediates (A*C9 and A*C9*) recruit XIAP with the same kinetics as free Casp9 (C9 and C9*). In addition, we modeled XIAP binding to Casp9 such that it is not affected by either Casp9 autocleavage (at Asp-315) or Casp3-mediated feedback cleavage (at Asp-330). As contradictory experimental results were obtained on how Casp9 cleavage modulates inhibition by XIAP, the impact of the latter assumption will be stressed in the Discussion.<br>
Because of the assumptions made in the previous paragraph, there is reversible recruitment of XIAP to all Casp9 species in the model (reactions 9?12), and also free exchange of Apaf1 between the resulting Casp9?XIAP complexes (reactions 13?14). All Casp9?XIAP complexes were assumed to be catalytically inactive, which is in accordance with experimental studies [47,48]. Furthermore, Casp3-mediated feedback processing of XIAP-bound Casp9 was neglected in the model, as the Casp9?XIAP binding interface is nearby the corresponding cleavage site (Asp-330) [48].<br>
It is well established that XIAP binds both to partially processed Casp3 (p12-p20) and to mature Casp3 (p12-p17), but not to its inactive precursor pro-Casp3 [46,49]. In accordance with experimental data [45,46], reversible association between Casp3 and XIAP (reaction 15) was modeled to result in a catalytically inactive complex (C3*X). Due to the enzymatic inactivity of pro-Casp3 (C3) [39] and of the Casp3?XIAP complex (C3*X), free active Casp3 (C3*) was taken as the response in our simulations.<br>
Finally, we included protein synthesis and degradation in the model (reactions 16?28). More specifically, the unmodified proteins A*, C9, X, and C3 are produced with a constant rate, and all molecular species in Figure 1B are subject to first-order degradation. While the total cellular concentrations of Apaf-1, Casp9, Casp3, and XIAP (i.e., the ratio of protein synthesis and degradation rates) were measured [35,50?53], the kinetics of synthesis and degradation were not known. For simplicity, we assumed the same degradation rate for all molecular species in the model, and adjusted the synthesis rates in order to obtain previously measured protein concentrations (Table 1). This implies that the total concentrations of Apaf-1, Casp9, Casp3, and XIAP remained constant throughout our simulations.<br>
From the model described above (Figure 1B), which will be referred to as the ?wild-type model? in the following, molecular balances could be derived for each considered molecular species resulting in a system of 13 ordinary differential equations (Protocol S1). In general, protein?protein association (reactions 1, 4?6, 7, and 10?13 in Figure 1B) was modeled as a reversible second-order process, and caspase-mediated cleavage (reactions 2, 3, 8, 9, 14, and 15 in Figure 1B) was modeled as an irreversible second-order process. As many similar reactions (e.g., 1 and 13 in Figure 1B) were assumed to proceed with the same kinetics (see Table 1), the model comprises 16 kinetic parameters. The unknown kinetic parameters were set to reasonable values (Table 1) in order to reproduce the previously reported time courses of caspase activation (see ?Time Course of Casp3 Activation?).<br>
Besides the wild-type model, we also analyzed two modified models in order to get insights into the mechanisms that are responsible for bistability in caspase activation. 1) In the ?Casp9-mutant model,? which comprises only the black reactions in Figure 1B, we eliminated Casp3-mediated feedback cleavage of Casp9 (reactions 8 and 9 in Figure 1B) from the wild-type model. 2) Based on available experimental data (see Discussion), we assumed competitive (i.e., mutually exclusive) binding of Casp3 and Casp9 to XIAP in the wild-type model. By contrast, Casp3 and Casp9 were allowed to bind XIAP simultaneously in the ?noncompetitive model?; that is, the wild-type model was extended by four ternary Casp9-XIAP-Casp3 complexes (Protocol S1).<br>
<br>
Time Course of Casp3 Activation<br>
Experiments in cytosolic extracts revealed that exogenously added cyto c induces maximal Casp3 cleavage within ~15 min in some cells [36], while completion takes longer (up to ~60 min) in other systems [5,6,33,54]. More specifically, the Casp3 cleavage seems to be fast upon strong stimulation, but slower if stimulation is weak [6,53,55].<br>
We were interested whether the model was able reproduce these observations if previously measured protein concentrations of Apaf-1 (20 nM), Casp9 (20 nM), Casp3 (200 nM), and XIAP (40 nM) were assumed [35,50?53]. Exogenous addition of cyto c was simulated by a step-like increase in the total amount of active Apaf-1 monomers, A*tot, as cyto c?induced apoptosome formation was reported to be a very rapid process [36,38]. Such a step-input is also expected to reflect input characteristics within living cells reasonably well, since cyto c release from mitochondria was shown to complete within 5 min [56,57].<br>
The results shown in Figure 2A reveal that the simulated time courses of caspase activation agree well with those measured experimentally, and that simulated response time is indeed inversely related to the stimulus strength (Figure 2A). Full activation of all cellular Apaf-1 molecules (A*tot = 20 nM) elicits fast Casp3 activation, while a critical slowing down is observed near the threshold (A*tot ~ 3 nM; see Figure 2B), as expected for a bistable system [28,50]. Notably, the slope of the time courses shown in Figure 2A is only marginally affected by the onset time of caspase activation (i.e., by the stimulus level). This is in accordance with experimental results obtained in cyto c?treated cytosolic extracts [5,36,54] and in single living cells [58], which showed that, once initiated, Casp3 activation is rapidly completed within less than 15 min.<br>
<br>
Bistability in Caspase Activation<br>
Experimental evidence suggests that cyto c?induced caspase activation can be bistable and irreversible (see Introduction). The simulated steady-state Casp3 activity (C3*) was indeed bistable and irreversible (Figure 2B, black line). The system exhibits three steady states, two stable (solid black lines) and one unstable (dashed black line), for A*tot between 0 and ~3 nM, and shows hysteretic behaviour: starting from the resting state (point 1), the system retains low Casp3 activity even for increasing stimuli, A*tot, until a threshold (point 2) is reached, whereby Casp3 activity switches to the higher steady state (point 3) in an all-or-none fashion. The system remains at this higher steady state even if the stimulus is removed (point 4), so that caspase activation is irreversible, and thus represents the point of no return for apoptosis.<br>
We next addressed the mechanism of bistability, and hypothesized that Casp3-mediated feedback cleavage of Casp9 was responsible, since bistability is thought to require a positive circuit [25]. Therefore, reactions 8 and 9 in Figure 1B were blocked to simulate a mutant Casp9 (D330A), which is refractory to cleavage by Casp3 (?Casp9-mutant model?). Unexpectedly, bistability was retained (Figure 2B, gray line), which suggests that a hidden positive feedback loop operates in the Casp9-mutant model.<br>
<br>
XIAP Establishes an Implicit Positive Feedback in Caspase Activation<br>
More detailed simulations revealed that XIAP establishes an implicit positive feedback in the Casp9-mutant model, and Figure 3 schematically depicts how this mechanism contributes to irreversibility in the wild-type model: Upon weak stimulation (point 1 in Figure 2B) the vast majority of Apaf1-associated, highly active Casp9 molecules is inhibited by excess XIAP, so that cleavage of pro-Casp3 is negligible (top left in Figure 3). As the stimulus strength is increased above the threshold (point 2 in Figure 2B), active Apaf-1 also recruits some free Casp9 that is not subject to inhibition by XIAP, so that Casp3 activation is initiated (top right in Figure 3). Active Casp3 then further promotes its own activation by sequestering XIAP away from Apaf-1-associated Casp9 (?redistribution?), so that finally the vast majority of XIAP is bound to Casp3 (bottom right in Figure 3). This XIAP redistribution results a positive feedback loop, which, together with Casp3-mediated Casp9 feedback cleavage, suddenly switches the system from low to high Casp3 activity (transition from point 2 to point 3 in Figure 2B). Caspase activity is maintained even if the stimulus is removed, as Casp3, once activated, retains XIAP, and thereby prevents full Casp9 deactivation (bottom left in Figure 3). Additional simulations, which corroborate our conclusions regarding XIAP-mediated feedback can be found in Protocol S2.<br>
In order to determine how the protein concentrations in the caspase cascade affect bistability, we analyzed the stimulus?response curves (similar to those in Figure 2) for varying total Casp3 and Casp9 concentrations. Five types of qualitative behaviour in caspase activation could be distinguished in the physiological range of stimulus concentrations (A*tot = 0?200 nM): 1) the system is essentially devoid of any Casp3 activation (monostable?no activation [MN], Figure 4A); 2) Casp3 activation occurs in a gradual manner (monostable?gradual activation [MG], Figure 4B); 3) the caspase cascade is bistable-reversible (BR, Figure 4C); 4) Casp3 activation is bistable-irreversible (BI, Figure 4D); and 5) constitutive Casp3 activity is observed (monostable?basal activation [MB], Figure 4E). The corresponding bifurcation diagram (Figure 4F) reveals that bistability in the Casp9 mutant model can only be observed if the total Casp9 concentration is lower than that of XIAP (40 nM), which ensures that the system is in the off-state as long as Casp3 is inactive. In addition, Casp3 must be significantly more abundant than XIAP to sequester it away from Casp9 (i.e., to establish positive feedback).<br>
<br>
Determinants for Bistability and Irreversibility<br>
We next sought to determine the relative contribution of XIAP-mediated feedback and that of Casp3-mediated feedback cleavage (of Casp9) to bistability and irreversibility in caspase activation. To this end, we compared the bifurcation plot of the wild-type model (Figure 4G) with those of mutant models, where we selectively blocked either XIAP-mediated feedback (Figure 4H; ?noncompetitive model?) or Casp3-mediated feedback cleavage (Figure 4F, ?Casp9-mutant model?). XIAP-mediated feedback is abolished in the noncompetitive model (Protocol S1), since XIAP was assumed to be capable of simultaneous binding to Casp3 and Casp9 in these simulations. As schematically depicted above Figure 4H, this corresponds to a caspase cascade, which is controlled by the XIAP fragments BIR1-BIR2 (specific for Casp3) and BIR3-RING (specific for Casp9) rather than by full-length XIAP. Figure 4F and 4H demonstrate that each feedback mechanism alone can bring about bistability for experimentally measured caspase expression levels (interception of dashed lines in Figure 4F?4H). By contrast, irreversibility is restricted to a narrow range of caspase concentrations in both mutant models, and is never observed in the vicinity of experimentally measured caspase expression levels. Importantly, the wild-type model exhibits robust irreversibility in the physiological range of caspase expression levels, which suggests that irreversibility in caspase activation requires coordinated action of both XIAP- and cleavage-mediated feedbacks.<br>
The computational results shown in Figure 4G also explain why various cell types show qualitatively different patterns of caspase activation and unravel the underlying mechanisms: Casp3 activation is efficiently inhibited in cells, where the total XIAP concentration exceeds those of Casp3 and Casp9 (MN; Figure 4A). Gradual Casp3 activation is predicted to occur in cells, where Casp9 expression is high compared with XIAP and Casp3 expression (MG, Figure 4B). In this situation XIAP is effectively sequestered by excess Casp9, and the remaining free Casp9 molecules efficiently cleave Casp3 as if XIAP was not present. In case that both caspases are expressed at intermediate levels, the feedback loops discussed above cooperate to reversibly switch on the system in an all-or-none fashion (BR, Figure 4D). Even higher caspase expression levels relieve the cascade from XIAP-mediated inhibition so that Casp3 can be highly active even in the absence of stimulation. Such constitutive activation either arises spontaneously (MB, Figure 4C) or requires previous suprathreshold Casp3 activation (BI, Figure 4E).<br>
The preceding conclusions could be confirmed by analyzing the qualitative behaviour of caspase activation as a function of the competition ratio ?, and of XIAP expression (Figure 5). The competition ratio ? equals the fold-change in XIAP's affinity for Casp9 brought about by Casp3 binding to XIAP (and vice versa), and thereby quantifies the degree of competitive caspase binding to XIAP (Protocol S1). Figure 5 demonstrates that the range of bistability is significantly broadened even if the Casp3-binding to XIAP reduces XIAP's affinity for Casp9 (and vice versa) less than 5-fold (? &gt; 0.2). By contrast, reliable irreversibility requires significant competition of caspases for XIAP, at least with the default protein concentrations (Table 1) we assumed here. As shown in Figure 5, high XIAP levels completely abolish caspase activation (MN), bistability is observed for intermediate XIAP concentrations (BR, BI), and low XIAP levels fail to prevent caspase activation even in the absence of external stimulation (MB).<br>
Our conclusions regarding the qualitative behaviour of caspase activation are supported by experimental data. 1) Overexpression of XIAP abolishes apoptosis and Casp3 activation in response to microinjection of cyto c (type MN) [59]. 2) overexpression of Casp3 [60,61] or Casp9 [33,62,63] results in caspase activation and/or apoptosis (type MB). In contrast, Casp3 overexpression failed to elicit its own activation in another study [64], and the model suggests that this may be due to low Casp9 expression (see Figure 4G). 3) High levels of IAP antagonists such as Smac were shown to activate the Casp9 ? Casp3 pathway [65,66] and to elicit spontaneous apoptosis [67], even in cell types that are devoid of basal cyto c release or Casp8 activation (type MB). The inability of others to reproduce Casp3 activation by XIAP depletion or Smac addition [68] is probably due to the fact that the threshold BI ? MB (Figure 5) was not exceeded in these studies (e.g., due to the expression of Smac-resistant IAP proteins such as NAIP [69]). 4) Gradual Casp3 activation (type MG) was observed in cyto c?treated cytosolic extracts [8?10], and also in flow cytometric analyses of living cells [70,71]. 5) The existence of bistable states (types BI and BR) is supported by all-or-none Casp3 activation in response to cyto c, and by the fact that Casp3 activation can irreversibly commit cells to death (see Introduction), although definitive proof for these types of behaviour is lacking (see Discussion).<br>
<br>
The Mitochondrial Pathway Acts as an Efficient Binary Integrator<br>
In the previous section, we demonstrated that excess of XIAP over Casp3 and Casp9 abolishes cyto c?induced caspase activation even if high concentrations (200 nM) of the stimulus, active Apaf-1, were assumed (type MN). However, various experimental studies in cells, where Casp3 activation was inhibited downstream of cyto c release, have shown that caspase activation can be rescued by relatively moderate Apaf-1 overexpression (see [72] and references therein). This suggests that Casp3 activation does not occur if the concentration of the bottleneck, active Apaf-1, is below the threshold stimulus concentration, where the bistable system switches from the lower to the upper steady state (point 2 in Figure 2B). In support for such a threshold model, it was recently shown that a minor (~2-fold) decrease in Apaf-1 expression dramatically decreases caspase activation in response to cyto c microinjection [72]. These studies also suggest that the apoptotic threshold can be regulated downstream of Apaf-1, as Smac, an inhibitor of XIAP action, rescued cyto c?induced caspase activation in Apaf1-knockdown cells [72]. Therefore, we were interested how the threshold of the bistable cascade is affected by transcriptional and post-transcriptional regulation of Casp3, Casp9, and/or XIAP.<br>
The corresponding results are shown in Figure 6: starting from the default model (point of intersection), the predicted threshold stimuli, A*tot,T, of the bistable system were plotted as a function of Casp3 (gray dotted line), Casp9 (gray solid line), and XIAP (black solid line) expressions. In addition, we also considered simultaneous alterations of Casp3 and Casp9 to the same relative extent (black solid line) in order to understand how the apoptotic threshold is affected by nitric oxide, a covalent inhibitor of Casp3 and Casp9 active sites [1]. These simulations demonstrate that decreasing levels of Casp3 moderately increase the threshold, A*tot,T, while alterations in Casp9 shift the threshold more efficiently. Regulation of XIAP levels is predicted to allow even more effective control over the apoptotic threshold, and similar arguments also hold for nitric oxide?mediated inhibition of both Casp3 and Casp9 [1].<br>
Our results regarding XIAP as an efficient modulator of an all-or-none threshold are in accordance with experimental studies, as a 2-fold drop in XIAP expression was sufficient to allow cyto c?induced Casp3 activation [73,74]. Moreover, increasing amounts of Smac, a high-affinity inhibitor of XIAP (Figure 1B), elicited all-or-none Casp3 activation in cyto c?treated HeLa cell cytosols [68]. Finally, the threshold cyto c concentration that is required to achieve switch-like Casp3 activation was shown to be cell-type dependent, and low thresholds correlated with low IAP expression levels [7]. Our simulations are also corroborated by the fact that PKB/Akt-mediated inhibitory phosphorylation of Casp9 completely abolished cyto c?induced Casp3 activation, even though Casp9 enzymatic activity was only partially suppressed [63]. In addition, Casp3 overexpression sensitizes cells to apoptosis in response to cytotoxic stress [64], which is also in accordance with the simulations shown in Figure 6.<br>
Thus, we can conclude that bistable behaviour in the mitochondrial caspase cascade serves to compute multiple regulatory inputs into a binary decision whether caspase activation occurs or not (?binary integrator?). Further calculations, where relative changes in protein expression were related to relative changes in the threshold stimulus, A*tot, suggest that the following order of input potency holds in general: Regulation of active Apaf-1 &lt; Casp3 regulation &lt; Casp9 regulation &lt; XIAP regulation ? simultaneous regulation of Casp3 and Casp9. The simulations also predict that the apoptotic threshold is essentially constant if all components (i.e., Casp3, Casp9 and XIAP) are simultaneously changed to the same relative extent (Figure 6; black dash-dotted line). Hence, the life-or-death decision appears to be remarkably insensitive towards random fluctuations in gene expression, which are thought to result in correlated changes in cellular protein levels [75]. In addition, these simulations suggest that general inhibitors of protein synthesis or degradation, which are known to be inducers of apoptosis [56,64], do not affect the threshold of the Casp9 ? Casp3 cascade.<br>
<br>
<br>
Discussion<br>
In the present paper we showed that inhibition of Casp3 and Casp9 by IAPs results in an implicit positive feedback, since cleaved Casp3 augments its own activation by sequestering IAPs away from Casp9 (Figure 3). In addition, we demonstrated that XIAP-mediated feedback cooperates with Casp9 cleavage by Casp3 to bring about bistable and irreversible Casp3 activation in the range of experimentally measured kinetic parameters and protein concentrations (Figures 2, 4, and 5).<br>
Model Assumptions<br>
XIAP-mediated feedback can only be observed if Casp3 and Casp9 compete for binding to XIAP at least to some extent (Figures 3 and 5). Such competition is supported by the fact that Casp3 and Casp9 cannot be co-immunoprecipitated in cells [35]. Casp3 (and not only Casp9) is recruited to the apoptosome at least in some cells [35], and it is conceivable that this occurs by means of a sequential Apaf1-Casp9-XIAP-Casp3 complex. Even if such a complex exists, it seems to be rather instable, as Casp3 can be eluted from the apoptosome (i.e., from Apaf-1) by low ionic strength [39], while much higher ionic strength is required to elute Casp9 [32]. Recent co-immunoprecipitation experiments revealed the existence of a ternary Casp9-XIAP-Casp3 complex in vitro [76]. However, only minor amounts of Casp3 were found in the complex even if XIAP was incubated with excess Casp3 and Casp9. Taken together, these data suggest that Casp3 and Casp9 significantly compete for binding to XIAP.<br>
Co-immunoprecipitation studies with Casp3, Casp9, and XIAP might underestimate the degree of competition of caspases for a single XIAP molecule (i.e., XIAP-mediated feedback), as IAP family members are often homodimers. In case that each XIAP molecule in a dimer independently couples to caspases, a ternary Casp9?XIAP?Casp3 complex will be seen, even if Casp3 and Casp9 compete for a single XIAP molecule. Therefore, we propose to directly test for XIAP-mediated feedback in vitro. As further outlined in Protocol S3, a Casp9 mutant (D330A), which is refractory to Casp3-mediated feedback cleavage, should be incubated with active apoptosomes and XIAP either in the presence or in the absence of pro-Casp3. Coincubation with XIAP alone is expected to result in low Casp9 activity [34], but excess pro-Casp3 should reverse this inhibition by sequestering XIAP away from Casp9.<br>
We have also assumed in the model that XIAP inhibits all forms of Casp9 (i.e., that the affinity between Casp9 and XIAP is not affected by either Casp9 autocleavage [at Asp-315] or Casp3-mediated feedback cleavage of Casp9 [at Asp-330]). While it is clear that autoprocessed Casp9 (cleaved at Asp-315 only) is efficiently inhibited by XIAP [34,47,49,51,77], some authors reported that XIAP also binds to and inhibits uncleaved pro-Casp9 [34,47,78], at least partially [77,79], but others could not reproduce these results [49,51]. As explained in the context of Figure 3, bistability requires that XIAP binds to and inhibits Apaf1-activated Casp9 upon weak stimulation, so that low Casp3 activity can be maintained. Importantly, such XIAP-mediated control over Casp9 activity will be ensured even if XIAP does not associate with uncleaved pro-Casp9, since pro-Casp9 recruitment to the apoptosome was shown to result in its fast and complete autoprocessing (at Asp-315) [38,39]. Casp3-mediated feedback cleavage (at Asp-330) was reported to relieve Casp9 from inhibition by XIAP [51], and might thereby establish an additional positive feedback, which would further broaden the ranges of bistability and irreversibility. As other experimental studies do not support the existence of this additional feedback [34,49], we have made the conservative assumption that XIAP inhibits feedback-cleaved Casp9, too.<br>
In our core model of the intrinsic pathway we considered only Casp3 and XIAP, but not functionally redundant molecules. For example, Casp7, which is activated by Casp9 [5], also mediates XIAP-mediated feedback, since it efficiently binds to IAPs [46]. Likewise, molecules such as c-IAP1, c-IAP2, and NAIP are functionally redundant to XIAP, as they inhibit both Casp3 and Casp9 [2,69]. In case such functionally redundant proteins are expressed, the protein concentrations varied in our simulations (e.g., C3tot in Figure 3F?3H) represent combinations (e.g., sums) of functionally redundant protein concentrations (e.g., C3tot and C7tot), so that the results given in the paper continue to hold.<br>
<br>
Input Signals<br>
We used the concentration of active Apaf-1 assembled into apoptosomes as the varying input signal in our simulations, rather than the amount of cyto c released from mitochondria. This seems justified, as available experimental evidence suggests that apoptosome formation increases gradually with increasing cyto c concentration [10,37], and that signal amplification occurs in the caspase cascade considered in this paper [10]. Our model explains how cells reject erroneous cyto c release from single mitochondria, and also predicts that reversible cyto c release can elicit irreversible caspase activation. It should be noted that cyto c release upon apoptotic stimulation was reported to be all or none under many [56,57] but not all [80,81] circumstances. Importantly, dose-response curves using active Apaf-1 as the input (e.g., Figure 2B) are physiologically relevant even if cyto c release is all or none, as they help to explain why caspase activation is completely abolished for limiting Apaf-1 expression (see [72] and references therein). More generally, the model provides insights into how the intrinsic pathway integrates multiple regulatory inputs including including cyto c release, cyto c sequestration [39], transcriptional regulation of Apaf-1 [72], Apaf-1 sequestration [39], transcriptional regulation of IAPs [2], Smac-mediated IAP sequestration [2], Casp9 phosphorylation [63], and caspase S-nitrosylation [1]. As shown in Figure 6, the caspase cascade acts as a binary integrator in the range of bistability (BI and BR in Figures 4 and 5). In contrast, gradual integration will be seen if the system resides in the monostable?gradual activation range, and this is particularly relevant for apoptotic stimuli that directly regulate caspase cascade members (e.g., Apaf-1) in addition to releasing cyto c (?feedforward regulation?). For example, p53 is known to induce Apaf-1 expression [82], and thereby can elicit gradual Casp3 activation even if cyto c release is all or none. Alternatively, gradual Casp3 activation, which was seen in flow cytometric analyses of living cells [70,71], may be due to cell-to-cell variability in the intrinsic pathway. Such cellular heterogeneity seems to be significant, as cyto c injection alone or in combination with Smac does not elicit Casp3 activation [83] or cell death [59,84] in all cells of a population. Our model provides a reasonable basis for further studies that focus on cell-to-cell variability in the intrinsic pathway.<br>
In the Results section, we referred to experimental studies where Smac, a competitive, high-affinity inhibitor of IAP binding to caspases [85], was either added to cytosolic extracts or microinjected into living cells. In living cells, Smac is eventually released simultaneously with cyto c from mitochondria [2] (see Figure 1A). Importantly, such physiological release of Smac simply corresponds to decreasing XIAP levels in our model, as most experiments with caspase inhibitors have shown that Smac release does not require caspase-mediated feedback [57,86?88]. Thus, the results shown in Figure 6 explain why simultaneous release of cyto c and Smac is required to elicit Casp3 activation in many cell types (e.g., [83]), and predict that these two stimuli are integrated in an all-or-none manner.<br>
<br>
Upstream, Downstream, and Feedback Signaling<br>
In accordance with previous experimental studies (see Introduction), we showed that, depending on the protein expression levels in the intrinsic pathway, caspase activation irreversibly commits cells to apoptosis (BI regions in Figures 4 and 5). However, some cells die by a delayed and morphologically distinct form of cell death, so-called caspase-independent cell death, even if caspases are inhibited [89]. Because caspase-independent cell death is thought to be initiated at the level of mitochondria, our simulations do not unravel the determinants for commitment to death in these cells, but only those for commitment to the fastest death pathway (i.e., apoptosis). As the precise kinetics of cell death may, for example, be important in development [89], our results are likely to be relevant even in cells subject to caspase-independent cell death. The physiological importance of the caspase cascade considered in our model is further supported by the fact that Apaf-1, Casp9, and Casp3 knockout mice show morphological defects and die early in development [89]. In addition, caspase inhibition (e.g., due to IAP overexpression) allowed for long-term cellular survival and mitochondrial recovery in response to cytotoxic stress [19?23] and/or after cyto c was released [11,13,15].<br>
Other positive feedbacks than those included in the model have been described in the literature. For example, Casp3 was shown to induce processing of Casp6, which in turn cleaves Casp8, an activator of Casp3 [5] (feedback 1 in Figure 1A). In our opinion, this feedback is unlikely to account for bistable Casp3 activation via the intrinsic pathway, since Casp3 activation in response to cyto c is unaffected when the delayed Casp6 ? Casp8 pathway is abrogated [5]. This conclusion is likely to hold in general, as Casp8 cleavage alone is not sufficient to stimulate its catalytic activity, but recruitment to the death-inducing signaling complex (i.e., ligand-binding to death receptors) is required [62].<br>
It has been suggested that active Casp3 amplifies cyto c release from mitochondria by directly cleaving upstream regulators such as Bid and Bcl-2 (feedbacks 2 and 3 in Figure 1A), or by cleaving modulators of these Bcl2-family members such as Mekk1 [1]. However, the relevance of this feedback for the intrinsic pathway remains unclear, as experiments with caspase inhibitors revealed that cyto c release is caspase-independent in most cell types (e.g., [4,11,49,56,57,77]). Furthermore, the concept of Casp3-induced cyto c release is inconsistent with the fact that Casp3 activation fails in various cell types even though large amounts of cyto c were released from mitochondria (see [72] and references therein).<br>
XIAP was shown to be cleaved by Casp3 and/or Casp8 in response to apoptotic stimulation, and such XIAP processing may result in autoamplification of Casp3 activity (feedback 4 in Figure 1A) [90,91]. In line with a predominant role of Casp8, cleavage of XIAP seems to be especially pronounced when cells are subjected to death-receptor stimulation [90,91]. By contrast, moderate [91], minor [35,36], or even no XIAP processing [92,93] was seen in response to apoptotic stimuli that initiate apoptosis via the intrinsic pathway. In addition, Casp3 may also establish a positive feedback loop by cleaving inhibitors of XIAP auto-ubiquinitation and proteasomal degradation such as PKB/Akt (feedback 4 in Figure 1A) [94,95]. Accordingly, the total XIAP abundance was shown to decrease during apoptosis (e.g., [95]), but this seems to be a cell-type?specific phenomenon, as the total amount of full-length XIAP remains essentially unchanged [91?93] or even increases [96] in other models of apoptosis.<br>
Because of these data and due to the fact that most molecular species of the caspase cascade were shown to be continuously synthesized during apoptosis [96,97], we assumed constant total protein concentrations in our model. In order to get insight into how Casp3-mediated XIAP degradation affects the behaviour of our model, we also implemented an extended model, which takes such regulation into account (Protocol S4). Importantly, Casp3-mediated feedback cleavage of XIAP did not result in physiologically relevant bistability in a system devoid of other feedback amplification loops (Protocol S4). In addition, the qualitative conclusions drawn from Figures 2, 4, and 5 were still valid when XIAP-mediated feedback was included in the wild-type model (Figure 1B). However, these calculations also indicated that Casp3-mediated XIAP degradation may cooperate with the feedback loops discussed above, as it lowered the apoptotic threshold, A*tot,T, and significantly broadened the range of XIAP concentrations, where caspase activation is irreversible (BI in Figure 5).<br>
Active Casp3 cleaves a variety of cellular substrates, and thereby initiates the execution phase of apoptosis [1]. Experimental evidence suggests that Casp3 activates multiple execution pathways in parallel and not in a sequential, cascade-like manner, since mutational inactivation of Casp3 cleavage sites abrogates specific features of apoptosis depending on the target mutated [1]. Some Casp3 substrates (e.g., PARP) are cleaved almost simultaneously with Casp3, while the processing of others (e.g., Topo I) is delayed by several hours [98,99]. Taken together, these data suggest that transient activation of the branchpoint molecule, Casp3, elicits a partial apoptotic program, which might lead to potentially harmful cellular deregulation or tissue inflammation. Active Casp3 is known to be a rather unstable protein [100], which suggests that irreversible behaviour of the caspase cascade is required to maintain Casp3 activation if upstream stimuli are removed. Experimental evidence indeed suggests that such transient stimulation occurs in living cells.<br>
1) Cyto c release from mitochondria is thought to be a reversible as long as mitochondrial membrane potential is maintained. Because the mitochondrial membrane potential can remain unchanged long after caspases have been activated [3,4], cytosolic cyto c (i.e., the stimulus) will decline as soon as the apoptotic trigger is removed.<br>
2) Experiments with antibodies towards the caspase-activating form of cyto c, holo?cyto c, revealed that holo?cyto c is rapidly degraded after its release into the cytosol [17].<br>
The irreversibility mechanisms described in this paper ensure that apoptosis will fully proceed even after a decline in cyto c, and render apoptotic execution program insensitive towards survival signaling once apoptosis has been initiated. Such insensitivity is then further enhanced by delayed Casp3-mediated cleavage and thereby inactivation of various antiapoptotic signalling proteins [94].<br>
<br>
Proposed Experimental Verification of Bistability<br>
Our predictions regarding all-or-none and binary integration of multiple inputs behaviour in caspase activation (Figures 2?5) can be addressed experimentally by analyzing Casp3 activation in cytosolic extracts or on a single-cell level. In cytosolic extracts, depletion and readdition experiments with various Apaf-1, Casp3, Casp9, and/or XIAP concentrations should result in all-or-none caspase activation in the BR and BI ranges in Figure 4F, but the amount of fluorescent Casp3 substrates must be chosen carefully if enzymatic activity is used as a readout. Alternatively, such multivariate analyses can be performed by microinjecting these proteins together with cyto c and/or Smac into living cells. Caspase activation can then be determined using antibodies against active Casp3 either in flow cytometric measurements or in immunofluorescence microscopy. Bistability should be confirmed by adding cyto c in combination with appropriate antagonists such as anti?cyto c antibodies, apo?cyto c, or diarylureas, which are known to inhibit apoptosome activity [18,101]. In the range of bistability, simultaneous addition of suprathreshold cyto c levels and sufficient amounts of antagonist should yield low Casp3 activity, while strong caspase activation should be observed if the antagonist is added after cyto c. Subsequent addition of a Casp9 inhibitor would break the feedback loops discussed in the paper, and is therefore expected to reverse Casp3 activation. The bistability measurements described above can be done on a population level (i.e., by Western blotting) if caspase activation is irreversible, but require single-cell tracking methods (e.g., real-time Casp3 assays or flow-cytometric cell sorting) in the bistable-reversible range.<br>
<br>
Concluding Remarks<br>
In conclusion, we have presented a theoretical framework for quantitative experimental analyses of the intrinsic apoptosis pathway. Previous mathematical models differ from the present study in 1) the choice of apoptotic pathways, 2) the network properties focused on, and 3) the cell types analyzed. Bentele et al. [102] and Eissing et al. [50] concentrated on the extrinsic apoptosis pathway (see Figure 1A), and analyzed how switch-like behaviour arises due to stoichiometric inhibition [102] or due to positive feedback [50]. Fussenegger et al. [103] have implemented a large-scale model of both intrinsic and extrinsic pathways, and analyzed time course behaviour rather than bistability and apoptotic thresholds. Bagci et al. [104] focused on how Casp3-mediated feedback cleavage of Bcl2-family members (feedbacks 2 and 3 in Figure 1A) contributes to bistability in the intrinsic apoptosis pathway. As discussed above, these feedbacks appear to be restricted to particular cell types, where they might cooperate with those discussed here. Finally, Stucki and Simon [105] concentrated on the regulation of Casp3 degradation. The mechanisms proposed in the present paper may be combined with those discussed by Bagci et al. [104] and by Stucki and Simon [105] in order to implement more realistic models of the intrinsic apoptosis pathway.<br>
As summarized in Protocol S5, cellular inhibitory proteins such as stoichiometric inhibitors, phosphatases, and GTPase-activating proteins frequently inhibit consecutive intermediates in cellular signaling cascades. In general, positive feedback and bistability can arise in this ?shared inhibitor motif? if: 1) the signalling intermediates compete for binding to the inhibitor at least to some extent; 2) only the active form of the downstream intermediate (e.g., Casp3), but not its inactive precursor (pro-Casp3), binds to the inhibitor; and 3) the downstream intermediate (e.g., Casp3) is more abundant than the inhibitor (e.g., XIAP), which in turn needs to exceed the upstream intermediate (e.g., Casp9). As available experimental data are in accordance with these requirements, the feedback mechanism described in this paper is likely to be a widespread principle on how cells achieve ultrasensitivity, bistability, and irreversibility (Protocol S5).<br>
<br>
<br>
Materials and Methods<br>
All numerical simulations were done using the <software>MatCont Toolbox</software> within the <software>MATLAB</software> (The Mathworks, Natick, Massachusetts, United States) computing environment. SBML codes of the wild-type and noncompetitive models are available in Protocols S6 and S7).<br>
<br>
Supporting Information<br>
Accession Numbers<br>
The <database>Swiss-Prot</database> (http://www.ebi.ac.uk/swissprot) accession numbers for the proteins discussed in this paper are Apaf-1 (Q4VZG8), Bax (Q07812), Bcl-2 (P10415), Bid (P55957), CD95 (P25445), c-IAP1 (Q13490), c-IAP2 (Q13489), Casp3 (P42574), Casp7 (P55210), Casp8 (Q14790), Casp9 (P55211), cyto c (P99999), Fas (P48023), NAIP (Q13075), p53 (P04637), PARP (P09874), PKB/Akt (P31749), Smac (Q9NR28), Topo I (P11387), and XIAP (P98170).<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1664705</b><br>
Identification of the Proliferation/Differentiation Switch in the Cellular Network of Multicellular Organisms<br>
The protein?protein interaction networks, or interactome networks, have been shown to have dynamic modular structures, yet the functional connections between and among the modules are less well understood. Here, using a new pipeline to integrate the interactome and the transcriptome, we identified a pair of transcriptionally anticorrelated modules, each consisting of hundreds of genes in multicellular interactome networks across different individuals and populations. The two modules are associated with cellular proliferation and differentiation, respectively. The proliferation module is conserved among eukaryotic organisms, whereas the differentiation module is specific to multicellular organisms. Upon differentiation of various tissues and cell lines from different organisms, the expression of the proliferation module is more uniformly suppressed, while the differentiation module is upregulated in a tissue- and species-specific manner. Our results indicate that even at the tissue and organism levels, proliferation and differentiation modules may correspond to two alternative states of the molecular network and may reflect a universal symbiotic relationship in a multicellular organism. Our analyses further predict that the proteins mediating the interactions between these modules may serve as modulators at the proliferation/differentiation switch.<br>
<br>
Introduction<br>
How cells coordinate proliferation and differentiation has been one of the most important questions in developmental biology, cell biology, and cancer biology. The idea that growth and proliferation are poorly compatible with differentiation has wide currency, and explicit proliferation/differentiation switches have been demonstrated for many different cell types [1?4], but no general mechanism has been apparent. Due to the multifactor nature of this coordination process and the recent advances in gene networks, Waddington's theory of development as a canalization of the epigenetic landscape shaped by gene networks [5] has gained more popularity. As predicted by this theory, a breakthrough may be achieved through a systems approach. Recent production of various ?-omics? data has probed the gene networks from various aspects. Integrating the static interactome together with the expression and phenotypic profiles during a certain biological process can frequently reveal the dynamics of the gene network [6,7].<br>
The protein?protein interaction networks (PPI, or interactome networks) have been shown to have dynamic modular structures [7,8], yet the functional connections between and among the modules are less well-understood. Through examining the dynamics of the interactome network, we found that two major network modules, the ?P? (for proliferation) and ?D? (for differentiation) modules are anticorrelated transcriptionally over adulthood in both the human brain and the fruit fly. These modules are enriched in proliferation and differentiation genes, respectively, and display alternatively lower and higher expressions at the cellular proliferation/differentiation switch. Most P module genes are conserved between higher organisms and unicellular organisms such as yeast, but most D module genes are absent from unicellular organisms. Thus, these modules may correspond to alternative cellular states characteristic of higher organisms.<br>
<br>
Results<br>
Transcriptionally Anti-Correlated Modules in the Interactome Network<br>
To investigate the dynamic features of the human interactome network through changes in gene expression, we used as surrogates the expression profiles on 30 postmortem human brains from subjects ranging from 26 to 106 years old. This dataset was originally generated to examine the age-related changes in gene expression and the biological functions related to aging [9]. In this study, we only focus on the gene expression patterns across different individuals and try to dissect the modular structure of the interactome network by similar or opposite expression profiles between a pair of genes.<br>
As both transcriptional correlation and anti-correlation between a pair of genes are biologically relevant under specific conditions [10,11], we focused on the subnetwork that consists of only interactions between gene pairs that are transcriptionally correlated and anti-correlated (abbreviated as correlated and anti-correlated interactions, respectively) to examine the dynamic modular structure of the interactome. Such networks will be referred to as the NP network, where NP stands for negative and positive correlations. The expression correlations and anti-correlations between a pair of genes are commonly measured by a correlation coefficient. The Pearson correlation coefficient (PCC) is known to focus on the ?shape? of changes rather on than the intensity or amplitude of signals, and hence does not have bias for strong signals and has been shown to be best suited for oligo arrays [12,13]. It has a value of 1 for perfect correlation or ?1 for perfect anti-correlation.<br>
Our analysis pipeline includes the following steps (Figures 1 and S1): (1) obtain all the PPIs between genes that have similar expression profiles or opposite expression profiles determined by PCCs as correlated or anti-correlated interactions to arrive at the NP network; (2) identify network modules so that the expression profiles of genes within a module are similar, correlated interactions are maximally enclosed within a module, and anti-correlated interactions are optimally distributed between modules. The second step is approximated by first applying hierarchical clustering to the genes in the NP network, then manually dissecting the largest anti-correlated clusters or automatically scanning from the top of the hierarchical tree for clusters that have &lt;1% intracluster anti-correlated interactions and finding the largest anti-correlated clusters with an average expression of PCC &lt; ?0.7.<br>
Extracted from PPIs in the <database>Human Protein Reference Database</database> (<database>HPRD</database>) [14], the NP network across the human brain frontal cortex expression profiles [9] comprises 1,055 correlated and 395 anti-correlated interactions among 1,260 genes/proteins. We used PCC values of 0.4 and ?0.4 as cutoffs for positive and negative correlations, respectively. These cutoffs have been established in previous studies. However, as described later, the identities of the clusters are not dependent on PCC cutoffs.<br>
Using the hierarchical clustering algorithm implemented by <software>Cluster</software> [15,16] and visualizing the clusters with <software>Tree View</software> [15,17], we found that most of the anti-correlated interactions in the NP network bridge between two anti-correlated expression clusters among the genes (nodes) within the NP network. This is apparent if the samples are clustered on the other dimension (Figure 2A). We named the two anti-correlated clusters ?P? and ?D? based on their associations with proliferation and differentiation functions, respectively (see below). Another large cluster that is largely correlated with P, but not obviously anti-correlated with D is named the ?N? cluster for ?not determined? function. The smallest cluster is named the ?S? cluster for ?small? (Figure 2A). There are 457, 435, 260, and 108 nodes, and 220, 316, 111, and 27 interactions within the D, P, N, and S clusters, respectively (Figure 2B and 2C). Genes in each module are listed in Table S1.<br>
To examine the biological functions of each module, we first searched for overrepresented <database>Gene Ontology</database> (<database>GO</database>) categories among the genes within a module. To fully illustrate the preference of certain biological processes in one module versus others, we further grouped the related <database>GO</database> terms into a few broad categories, and performed a more comprehensive keyword search for genes potentially sharing the same process but not annotated by <database>GO</database>; these are listed at the end within each category in Table 1. The genes associated with the overrepresented <database>GO</database> terms and those found by the keyword search were also listed. According to Table 1, the D cluster is enriched in circulation/angiogenesis, apoptosis machinery, and ion and neurotransmitter channels, which are hallmarks of neural differentiation, cell cycle regulators, cell surface receptors, and steroid receptors. The P cluster is enriched in transcription, nuclear and intracellular transport, cell cycle, and cell motility genes. These enriched <database>GO</database> terms suggest that the P and D modules might be associated with cell proliferation and differentiation processes, respectively. The N cluster is enriched in genes involved in proteolysis, translation activity, intracellular transport, and energy metabolism. The S cluster is related to immunity (Table 1 and Figure 2B). A subset of the D genes (253 genes from D and one gene from N), the ?SD? cluster, also anti-correlates with a subset of the N genes (250 genes from N and 40 genes from D), the ?SN? cluster, across different subgroups of human subjects (Figure S2A and S2B).<br>
The anti-correlated expressions of the D and P clusters are even more evident when the average gene expression levels of these clusters in each sample are compared across different human subjects, with a PCC reaching ?0.867; that is, in almost each case when the expression of D is up, that of P is down, and vice versa.<br>
Since genes and the functional relationships among the genes in a coexpressed cluster are frequently called a coexpressed module in a network [18], we will refer to these clusters as modules in the context of a network. The D module can be divided into four smaller submodules, DS1 to DS4, by reclustering only the D genes so that no anti-correlated interactions are within any submodules (Figure 2B). These submodules are, however, connected by more correlated than anti-correlated interactions (Figure S2C).<br>
<br>
The P and D Modules Reflect the Dynamics of the Cellular Network<br>
If the P?D partition is a feature of the dynamic transcriptional regulation in the adult brain, we would expect that the partition should not be dependent on whether or not the PPI network is integrated and should also be independent of the PCC cutoffs used to extract the NP network.<br>
To test whether the physiological transcriptome is necessary for the P?D partition, or if network topology alone may give rise to such partitions, we permuted the expression values of each gene among different samples in the <database>HPRD</database> network, calculated the PCC of each <database>HPRD</database> interaction, and identified coexpressed modules using the automated pipeline (Figure 1). Among 100 such permutations, none of them gives rise to a pair of anti-correlated modules of more than 100 nodes per module (empirical p &lt; 0.01; Control 1 in Table 2). Permuting gene expression intensities within each sample (Control 2 in Table 2), or permuting PCC values among different PPIs (Control 3 in Table 2) also renders the anti-correlated modules undetectable or barely detectable (p &lt; 0.01 and p = 0.08, respectively). These randomization controls verify that the P?D partition is a true nature of the expression patterns, and cannot be derived by randomized expression patterns or pairwise relationship of expression profiles. As the networks in these controls have exactly the same network topology as the <database>HPRD</database> network, they also demonstrate that <database>HPRD</database> topology alone is not sufficient to give rise to the P?D partition. In other words, the P?D partition is not an artifact of network topology. In fact, it does not depend on any particular network at all.<br>
We created a nonoverlapping PPI dataset to the original early version of <database>HPRD</database> interactions that consists of the incrementally updated interactions to <database>HPRD</database> since the earlier version (the interactions added to <database>HPRD</database> between November 22, 2004, and September 13, 2005) and two recently generated Yeast Two-Hybrid (Y2H) datasets [19,20]. With this different PPI dataset, similar P, D, and N modules can be extracted from the NP network (Experiment 2 in Table 2 and Figure S3) that indicate the presence of P, D, and N modules is not a bias introduced by the original collection of <database>HPRD</database> interactions. Loosely aggregated coexpression clusters can be also derived from all genes on the microarray to significantly overlap with P (783 nodes) and D (1,276 nodes) modules (Experiment 3 in Table 2). The P?D partitions can actually be detected in almost any network of the same number of nodes (Experiment 4 in Table 2) or the same number of edges (Experiment 5 in Table 2) as the <database>HPRD</database> network but randomly sampled from an extended PPI network. The extended PPI network contains the updated <database>HPRD</database> plus two yeast two-hybrid interactome maps [19,20] and covers 7,568 proteins, 3,973 of which have expression profiles on the Affymetrix U95 array.<br>
The P?D partition also does not depend on any particular PCC cutoff used to extract the NP network. In the extended interactome, using the automated module dissection pipeline at various PCC cutoffs or even without a PCC cutoff (|PCC| &gt; 0), we could identify large clusters of genes that share significant overlaps with the P, D, and N modules. Although they correspond to smaller and smaller fractions of the total genes available on the microarray when the |PCC| cutoff increases, the fraction of genes corresponding to P and D modules maximizes around |PCC| cutoffs of 0.45 and 0.5 (Text S1 and Figure S4). Furthermore, relationships among the P, D, and N clusters did not change; only the number of genes and interactions varied to some extent without altering most of the enriched functions in each clusters (unpublished data). Therefore, although P and D modules are identified as predominate modules in the NP network of |PCC| &gt; 0.4, the two modules reflect the dynamics of the whole cellular network, which is not limited to those covered by the NP network.<br>
<br>
Stable Module Detection by Integrating the Interactome with the Transcriptome<br>
As demonstrated above, the presence of P, D, and N modules are not dependent on a particular PPI network being examined or a particular PCC cutoff. Why then do we need to integrate the PPI network and extract the NP network? The answer lies in the difference in the stability of detecting these network modules.<br>
Unlike the gene clusters in the NP network, where 71% of the genes fall into P and D clusters, when all the genes in the full <database>HPRD</database> network are clustered, a pair of loosely aggregating anti-correlated gene clusters covers only 24% of the <database>HPRD</database> genes (Experiment 3 in Table 2, Text S1, and Figure S4). Similarly, the anti-correlation between the SD and SN was not visually clear when all the genes in the NP network were clustered (Figure 2A). Only after we examined the distribution of the anti-correlated interactions, of which a large number between the D and the N modules are evident (see below), we decided to further cluster only the genes in the D and N clusters. Then, an obvious anti-correlation between the SD and SN clusters became visible (compare Figure 2A and Figure S2A). These suggest that by concentrating on only the correlated and anti-correlated interactions, we enriched for the genes of the P, D, and N clusters.<br>
To more rigorously test the stability of finding the P?D partition, we compared the chance of finding anti-correlated network modules with 100 or more nodes in each module and their overlap to the P and D modules with or without extracting NP networks prior to clustering the genes in the network. The chance of detecting anti-correlated modules is 99%, or 98% when an NP network with a |PCC| &gt; 0.4 was extracted from a randomly chosen partial PPI network with the same number of nodes or edges as the early version of the <database>HPRD</database> (Experiments 4 and 5 in Table 2). By omitting the step of extracting the NP network, the chances are reduced to 66% and 62%, respectively. In addition, the fraction of P and D module genes among all the input genes also reduces from an average of 29% of total input genes to 12% (compare Experiments 4 and 5 with Controls 4 and 5, respectively, in Table 2).<br>
Choosing a different PCC cutoff also reduces the chance to 75% (Experiment 6 in Table 2, Text S1, and Figure S4), with the fraction of genes identified as P and D module genes reaching maximal levels at |PCC| &gt; 0.45 or |PCC| &gt; 0.5 (Text S1 and Figure S4). If the PPI network is replaced by a randomly generated network with the same degree of distribution as <database>HPRD</database> or a sampled PPI network, using |PCC| &gt; 0.4 as a cutoff to extracting the NP network, although the chance of identifying anti-correlated modules are still high, these anti-correlated modules are of smaller average sizes and display low overlap (18%?19%) with P and D modules (compare Controls 6?8 with Experiments 1, 4, and 5, respectively, in Table 2). These reductions in the probability of finding anti-correlated modules and further reductions in the identification of P and D modules among the input genes (or all modules) point to a role of using the appropriate PCC cutoff and integrating true PPIs on the stability of P and D module identification. However, the contribution of integrating the PPI network is not limited to the module identification, but more importantly is linked to the identification of the large PPI interface between the P and D modules that potentially coordinate the cellular proliferation and differentiation processes (see below).<br>
These controls demonstrate that integrating the interactome, extracting the NP network, and applying an appropriate PCC cutoff ensured a high probability of stably detecting the P and D modules and improved their homogeneity, probably by filtering out most gene pairs that function in irrelevant tissue or cell types or under irrelevant physiological conditions.<br>
<br>
Conservation of P?D Anti-Correlation in Other Species<br>
The P?D partitions and their transcriptional anti-correlation can also be seen in the fruit fly. We used the adult whole-fly expression profiles to probe the dynamic gene relationships in the network. In the original publication [21], the expression profiles were used to study the effect of diet restriction on aging, and consisted of two sets of profiles: one for flies fed with a large amount of food, and one for those fed with a small amount of food, called ?high-food? and ?low-food? conditions, respectively. Here, we used these profiles to extract the network modules based on anti-correlated and correlated interactions across different fly populations using the automated analysis pipeline described in Figure 1. The modules derived under high-food and low-food conditions are more than 50% identical. While the composition of the P module is largely conserved between the human brain and the fly, that of the D module is quite different between the two species. In particular, the apoptosis pathways are only enriched in the human brain D module. The enriched differentiation markers in D modules are also different; in the human brain, there are the neuronal markers; in the fly, there are genes involved in eye development (unpublished data), which is consistent with their tissue- and organism-specific requirements for differentiation.<br>
We examined the percentages of human gene orthologs that can be found in yeast, worm, fly, and mouse. We found that 60% of D genes are specific to mouse and human and only 8% have yeast origin, whereas 35% of P genes have yeast homologs, and less than 30% are mammalian-specific (Figure 2D). Similar evolutionary patterns can be seen for fly P and D modules (Figure 2D).<br>
The above observations indicate that the P module is more conserved from the single-cellular organism yeast to the multicellular organisms C. elegans, Drosophila, mouse, and human, while the D is multicellular-specific and is subjected to species-specific and probably also tissue-specific modifications.<br>
The conservation of the P?D partition, their relationship, and the similar evolutionary profiles between fly and human also indicate that these observations cannot be due to sample variations introduced by sample preparations or other technical factors, but instead reflect true biological features of the gene networks of different multicellular organisms.<br>
<br>
The P/D Temporal Switch Corresponds to the Proliferation and Differentiation Switches<br>
A switch between differentiation and proliferation has been demonstrated in myoblast C2C12 cells [1]. Inhibition of the P?D interface protein HDAC4 has been shown to promote differentiation and inhibit proliferation, whereas inhibition to another interface protein, SRF, does the reverse [1]. Both the HDAC4 and SRF proteins are downregulated upon differentiation with a concurrent increase in differentiation markers and their antagonizing microRNAs [1]. The levels of ?5?1 integrin bound to fibronectin have also been shown to control the switching between proliferation and differentiation of C2C12 cells [4]. A proliferation/differentiation switch has also been observed in neural progenitor cells, and PI3K, cyclic AMP, raf, and MAPK pathways, which are all present at the P?D protein interaction interface, have all been implicated in regulating the switch [2,3]. These findings and many others collectively point to the existence of the switch between proliferation and differentiation at the cellular level.<br>
If the P and D modules are indeed associated with the proliferation and differentiation processes as suggested by the enriched <database>GO</database> annotations, we expect they might correspond to the cellular proliferation/differentiation switch in the tissue and organism we examined. Indeed, we found a decrease of P expression and an increase of D expression when fly, rat, mouse, or human cells of various cell types are switched from the proliferation to the differentiation state upon induction by various external stimuli. In this analysis, we used previously published data on human endometrial stromal cell differentiation induced by cyclic AMP, mouse C2C12 myoblast differentiation upon shifting to differentiation medium, mouse smooth muscle cell differentiation induced by retinoic acid, the inhibition of proliferation and induction of differentiation by the FGF of rat chondrocytes, and fruit fly neural progenitor cell differentiation (detailed sample information is available in Table S2). Consistent with the conservativeness of the P module, P is more uniformly suppressed upon differentiation of various different tissues in various different organisms. For example, the expression of fly P genes, especially of those derived under diet restrictions or low-food conditions, is suppressed in all cell types (Figure 3A, and middle and bottom rows in Figure 3B). In contrast, the expression of the human brain D is strongly induced during human endometrial stromal cell differentiation, and less so during mouse and fly cell differentiation; the expression of fly D genes is only most strongly induced in fly cells, but less so in cells of other organisms (Figure 3). Furthermore, detailed time courses of the proliferation/differentiation switch revealed that the P/D transition occurs only at the exact short window of the switch and are not observed before or after the switch (human endometrial stromal cell in Figure 3B), which accounts for some weak signals when the expression levels of all timepoints before or after the switch are averaged (Figure 3A). In addition to the association to proliferation and differentiation processes suggested by the overrepresented functional annotations in the P and D modules and transcriptional anti-correlations between the two modules, the correspondence to cellular level proliferation/differentiation switch more unequivocally supports the P/D temporal switch as the switch between proliferation and differentiation.<br>
However, except in the development of compound fly eye [22], it is not known if cellular proliferation/differentiation switches are coordinated at the tissue or individual levels, especially in postmitotic tissues or among post-developmental adult animals. One way the systems level controls are achieved might be through circulating hormones and growth factors, as many of them and their downstream regulation molecules are present at the P?D interface.<br>
<br>
Anti-Correlated Interactions Bridging the P?D Modules<br>
In addition to facilitating the module detection, integrating the interactome and the transcriptome also revealed a large number of PPIs between a limited number of proteins forming a PPI interface between P and D modules. The high degree of interactions at the P?D interface cannot be obtained from randomly generated PPI networks of the same degree distribution as <database>HPRD</database> (compare Control 6 to Experiment 1? in Table 2; empirical p &lt; 0.01, one-tail normal distribution p = 1.17 ? 10?8). The degrees of the P?D interface proteins in sampled PPI networks are also significantly higher than those in the artificially created control networks of the matched degree distributions (compare Controls 7 and 8 with Experiments 4 and 5 in Table 2; one-tail Student t-test p = 1.66 ? 10?41 and 1.93 ? 10?63, respectively).<br>
As expected, anti-correlated interactions preferentially bridge between the transcriptionally anti-correlated P and D modules. More than half (58%) of the correlated interactions are within the coexpressed modules, and 22% are between the P and N modules, whereas 57% of the anti-correlated interactions bridge between the P and D modules, and 22% bridge between the D and N modules (Figure 2C). The probability of the anti-correlated interactions bridging the P and D modules compared with bridging any modules is 1.8 times that of the uncorrelated interactions (PCC between ?0.4 and 0.4), which is a very significant difference (Fisher exact test p = 2.399 ? 10?20).<br>
In principle, the anti-correlated interactions can occur among different subjects or in different developmental stages and, as a consequence, bridge various coexpressed modules as small as one- or two-gene modules; therefore, it is surprising to see that two major coexpressed modules comprised 71% of the genes in the network, anti-correlated with each other across 77% of the samples, and were connected by the majority of the anti-correlated interactions in the network.<br>
<br>
Regulatory Role of the P?D Interface<br>
From the <database>GO</database> terms overrepresented at the interfaces, the P interface is enriched for transcription factors and the D interface is enriched in cell-cycle checkpoint, DNA repair, and receptor signaling genes (Table 3). All these processes are important regulatory mechanisms in the proliferation/differentiation switch. In particular, the D interface proteins include many of the well-known tumor suppressor genes, such as BRCA-1 and p53, and many receptors and transcription regulators known to be required for neuron differentiation, such as MYC, TOP2B, integrin, estrogen, FGF, PDGF, and TSH receptors, and many A kinase?anchoring proteins (AKAPs), etc. The P interface contains genes promoting cell proliferation, such as K-RAS, HDACs, SRF, CREB, CREBBP, IL4R, and INSR (the insulin receptor gene). It also contains genes that inhibit p53 and BRCA-1 functions such as PARC and LMO4 (Table 3 and Figure S5). We further evaluated the potential regulatory role of the P?D interface by three well-known network and biological properties of regulatory genes: (1) genes playing crucial regulatory roles are often hubs in the network, and vice versa; (2) if the interface plays crucial regulatory roles in the proliferation/differentiation switch, malfunction of these genes may lead to cancer, and thus these genes are on average more likely to be oncogenes or tumor suppressor genes; and (3) regulatory genes function in regulatory pathways, where feedback control is a dominant network feature. We therefore compared the protein interaction degrees, the percentage of oncogenes and tumor suppressor genes, and the percentage of genes in the feedback loops between the interface and the non-interface, or core genes. The results of all three tests are consistent with a crucial regulatory role in the P/D switch: compared with the core of the P and D modules, the P and D interfaces have a much higher average interaction degree (Figure 4A; p = 2.27 ? 10?12), percentage of known oncogenes and tumor suppressor genes (Figure 4B; p = 3.28 ? 10?2 and 2.08 ? 10?4 for P and D interfaces, respectively), and percentage of proteins/genes located in feedback loops (Figure 4C; p = 1.07 ? 10?2 and 3.45 ? 10?4 for P and D interfaces, respectively). Even though all the feedback loops are still of very limited coverage, it is already evident that most of these feedback controls are between the P and D modules and mediated by anti-correlated interactions (Figure 4D). Nearly all the proteins involved in these feedback loops are transcription regulators, and many of the loops are formed by both PPI and transcriptional regulations (Figure 4D). These special features of the P?D interface proteins make them potential key regulators for the proliferation/differentiation switch.<br>
Alternating expression of genes can be brought upon by the ?toggle switch? network circuit, which is a feedback loop consisting of two mutually inhibitory interactions between the nodes [23]. If we treat the P and D modules as single nodes in a module network, the P and D expression pattern can be also achieved through a simple toggle switch design between them (Xia et al., unpublished data). However, in a complex system involved in differentiation and proliferation control, much more redundancy and fine-tuning than one feedback control might be implemented. As examples of potential feedback controls of the differentiation and proliferation in the adult human brain, we can list at least ten interesting signaling pathways of three or more nodes that traverse the P?D interface (Figure 4E). Highlighting a unique advantage of such integrative systems analysis, these pathways are not just a collection of known pathways preexisting in the literature. Although all the interactions are derived from the literature, and most genes in these pathways have been shown to affect differentiation, proliferation, or growth, the pathways themselves have not been reported previously and are not known as control circuits for coordinating differentiation and proliferation processes.<br>
Altogether, the P and D modules are not only transcriptionally anti-correlated across different individuals, they are also functionally associated with basic cellular proliferation and differentiation functions, evolutionarily represent cell-autonomous and multicellular-specific modules, and are respectively suppressed and induced at the cellular proliferation/differentiation switch in various cells of various multicellular organisms. The functional interdependence, antiphase temporal compartmentalization, and different evolutionary origins of the two modules suggest that the P and D modules are two counterparts in a symbiotic relationship that need to be tightly controlled and coordinated at the cellular, tissue, and organism levels by switching temporally between the two phases?proliferation and differentiation.<br>
<br>
<br>
Discussion<br>
In this study, we describe a new integration analysis of the interactome and the transcriptome, which, even though rather straightforward, is very effective at removing the analysis noise of a conventional gene clustering process and allowed us to robustly find the P and D modules and their transcriptional anti-correlation. Moreover, the most important contribution of PPI integration is to reveal the P?D interface, which has a potential regulatory role in coordinating the proliferation and differentiation processes.<br>
We found that anti-correlation goes beyond the individual gene pairs but between the gene populations?a pair of transcriptionally anti-correlated gene groups. The P and D modules seem to be associated with cellular proliferation and differentiation and are suppressed and induced at the cellular proliferation/differentiation switch, respectively, therefore corresponding to alternative states of the cellular network. This indicates that logical relationships also exist at the modular level in the cellular networks. A possible scenario for anti-correlation at the modular level is that it might reflect a temporal separation of the biological functions in the cellular network [24]. The metabolic cycle has been suggested to fulfill such a role of temporal compartmentalization of oxidative and reductive metabolism in eukaryotes [24]. The antiphase temporal compartmentalization of proliferation and differentiation has been demonstrated over and over for single molecules upon switching from proliferation to differentiation at the cellular level [1?4], but it is surprising that such relationships also exist at the tissue or organism level during adulthood or might be brought upon by transcriptionally anti-correlated modules through complex feedback mechanisms between the two modules. Our result is therefore consistent with Waddington's [5] view of the development, in which differentiation and proliferation correspond to two states of the network where a balance between them is achieved at a systems level. More importantly, the tissue-level and organism-level coordination during adulthood and the evolutionary conservation level of the P and D modules imply the balance is not restricted to the single-cell level during early development, but rather exists during the whole life of an organism.<br>
Although such an expression pattern can be achieved by a simple toggle switch between them (Xia et al., unpublished data), in a complex system, redundancy is often implemented to ensure robustness; therefore, multiple toggle switches may exist between the two modules, and the switches must be connected with each other to transfer information. The exact molecular mechanisms giving rise to the transcriptional correlated and anti-correlated modules at the systems level during adulthood remain to be determined; we expect that many signaling pathways involved in cancer formation and aging will be part of the control mechanisms. But due to methodology limitations in the past, most of these pathways have been only studied individually; a general and comprehensive mechanism is still lacking. Our identification of the P and D modules at the systems level has provided an entry point for arriving at such a general mechanism. It is possible that the genes in each module share a few common immediate upstream transcription regulators. For example, we have found that the 5? untranslated regions of the fly P module genes are clearly enriched for Dref-targeting sites among a few other less-well characterized sequence motifs (Xue et al., unpublished data). Ectopic overexpression of Dref has been shown to block the proliferation/differentiation switch in the fruit fly eye imaginal disc [22].<br>
As the P module is concentrated at transcription-level activities and the N module is concentrated at protein-level activities, a temporal delay between transcription and translation might account for the lack of complete synchronization between the two clusters. Even though the samples are mostly from subjects of different ages, the timescale reflected by these samples may not be restricted to age differences; a delay between translation and transcription may well be reflected as individual differences.<br>
Although the current coverage of the interactome comprising both the literature and large-scale yeast two-hybrid interactions is still limited [25], the conservation of the P?D pattern in the human brain and fruit fly across different datasets indicates that the coverage is sufficient at the current level to detect, annotate, and analyze the P and D modules. In the NP network, we only focused on the strongly correlated and strongly anti-correlated interactions, but the genes excluded this way may also play important regulating functions toward the temporal compartmentalization between P and D modules or in the proliferation/differentiation control. Nevertheless, our identification of the large interconnected P?D modules for the first time revealed a proliferation/differentiation switch and their interrelationship at a systems level. It opens a new avenue to examine differentiation and proliferation at the systems and network levels, and provides a channel to connect physiological level events, such as hormone secretions, to the underlying cellular and molecular changes. It will help to elucidate many complex biological processes.<br>
<br>
Materials and Methods<br>
Datasets.<br>
The <database>HPRD</database> dataset [14] was downloaded from www.hprd.org on November 22, 2004; a later version of the <database>HPRD</database> dataset was obtained on September 13, 2005; two human Y2H datasets were included in the extended PPI network [19,20]. Two Y2H screens were combined as the fly protein interaction dataset [26,27].<br>
Microarray expression profiles were obtained from previously published studies on postmortem human brains of subjects between 26 and 106 years of age [9] and on adult Drosophila melanogaster of various ages [21].<br>
<database>GO</database> annotations were downloaded from ftp://ftp.ncbi.nlm.nih.gov/gene/DATA on March 10, 2005.<br>
Lists of proto-oncogene and tumor suppressors were obtained at http://ca.expasy.org/cgi-bin/get-entries?KW=Anti-oncogene and http://ca.expasy.org/cgi-bin/get-entries?KW=proto-oncogene on July 4, 2005.<br>
<br>
Orthologs.<br>
Human orthologs in mouse, fly, worm, and yeast were identified as the best reciprocal <software>BlastP</software> hits with an e-value cutoff of 10?6 based on <database>RefSeq</database> protein sequences downloaded on December 9, 2004, from http://www.ncbi.nlm.nih.gov/RefSeq.<br>
<br>
Filtering <database>GO</database> terms.<br>
To filter out the <database>GO</database> terms [28] that are broadly associated with many proteins, we calculated the number of proteins each <database>GO</database> term associated with and used only the <database>GO</database> terms that have a detection probability (p-value) among randomly paired genes less than the significance value of 0.05 after Bonferroni correction for multiple hypothesis testing on the total number of <database>GO</database> terms in each species. The p-value for a <database>GO</database> term is defined as p = (n/t)2, where n is the number of genes associated with the <database>GO</database> and t is the total number of genes in the species; a <database>GO</database> term was used only if p &lt; (0.05/g), where g is the total number of <database>GO</database> terms associated with all the genes in a species.<br>
<database>GO</database>-term enrichment was determined by Fisher exact test followed by Benjamini-Hochberg correction [29] for multiple hypothesis testing on all the <database>GO</database> terms tested in each gene set.<br>
<br>
Feedback loops.<br>
The NP network was searched with the breadth first-search algorithm for pathways that have the same start and end nodes with lengths between two (minimum, one protein and one regulatory edge between two nodes) and ten based on all the directed protein and regulatory interactions we annotated or extracted. Protein interaction annotation was based on the <database>PubMed</database> abstracts of the references listed by <database>HPRD</database>. Regulatory relationships between PPI partners within the NP network were extracted from <database>PubMed</database> by the Pathway Assist text-mining function (Ariadne Genomics, http://www.ariadnegenomics.com/products/pscentral). Other regulatory relationships were predicted from the transcription factor motifs annotated by the <database>TRANSFAC</database> database or from the p53 chromatin immunoprecipitation experiment [30], then filtered with pairwise expression |PCC| &gt; 0.4.<br>
<br>
Expression clustering and visualization.<br>
Unsupervised agglomerative hierarchical clustering of genes was performed in <software>Cluster</software> 3.0 [15,16]. The expression values were first adjusted by the following operations: log transform, median center genes, normalize genes, median center arrays, and normalize arrays. Then, hierarchical uncentered correlation and centroid linkage were used for clustering in both dimensions. The clustering results were visualized in <software>JavaTreeView</software> 1.0.12 [15,17].<br>
The layout of the reorganized NP network to visualize the intercluster interactions was created with a new visualization tool we developed in Java (Hou et al., unpublished data) and imported into <software>Cytoscape</software> 2.1 [31]. All other network visualization was achieved directly with <software>Cytoscape</software> 2.1.<br>
Networks of a predefined degree distribution were generated by an algorithm used by Milo et al. [32].<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC1794322</b><br>
Robustness Can Evolve Gradually in Complex Regulatory Gene Networks with Varying Topology<br>
The topology of cellular circuits (the who-interacts-with-whom) is key to understand their robustness to both mutations and noise. The reason is that many biochemical parameters driving circuit behavior vary extensively and are thus not fine-tuned. Existing work in this area asks to what extent the function of any one given circuit is robust. But is high robustness truly remarkable, or would it be expected for many circuits of similar topology? And how can high robustness come about through gradual Darwinian evolution that changes circuit topology gradually, one interaction at a time? We here ask these questions for a model of transcriptional regulation networks, in which we explore millions of different network topologies. Robustness to mutations and noise are correlated in these networks. They show a skewed distribution, with a very small number of networks being vastly more robust than the rest. All networks that attain a given gene expression state can be organized into a graph whose nodes are networks that differ in their topology. Remarkably, this graph is connected and can be easily traversed by gradual changes of network topologies. Thus, robustness is an evolvable property. This connectedness and evolvability of robust networks may be a general organizational principle of biological networks. In addition, it exists also for RNA and protein structures, and may thus be a general organizational principle of all biological systems.<br>
<br>
Introduction<br>
The biochemical parameters that determine the behavior of cellular systems?from proteins to genome-scale regulatory networks?change continually. Such change has two principal sources. One of them is genetic and consists of mutations. The other is nongenetic; it is exemplified by noise internal to the organism and by environmental change. In contrast to mutations, which are relatively rare, internal noise is ubiquitous and substantial. Much of it consists of stochastic variation in gene expression and expression regulation [1?6]. Such noise makes all biochemical parameters affecting a circuit's behavior appear to fluctuate randomly. Environmental change, such as a change in temperature, salinity, or nutrient availability, can similarly affect many parameters at once. These observations suggest that biological circuits are not fine-tuned to exercise their functions only for precise values of their biochemical parameters. Instead, they must be able to function under a range of different parameters. In other words, they must be robust to parameter change. These insights have lead to explorations of circuit robustness in processes ranging from bacterial chemotaxis to embryonic development [7?16].<br>
Quantitative models of cellular circuits help us to understand processes as different as circadian rhythms [17?25], the cell cycle [26], organismal development [7,9,10,16,27?31], bacterial chemotaxis [8], and the behavior of synthetic circuitry [32?36]. Several classes of models are used to represent such biological networks. The first class comprises differential equation models. The continuous state variables in these equations correspond to the concentrations or activities of gene products. The interactions of these gene products are represented through biochemical parameters such as binding affinities of transcriptional regulators to DNA, dissociation constants of ligand-receptor complexes, or kinetic rate constants of enzymes. A nearly universal problem is that quantitative information about these biochemical parameters is absent, even for experimentally well-studied systems. In other words, some knowledge of the topology of a circuit?who interacts with whom?may exist, but the strengths of the interactions are usually unknown. Even where measurements of biochemical parameters are available, they are often order-of-magnitude estimates rather than quantitative measurements with known precision. This difficulty leads one naturally to a second class of models in which only the qualitative nature of the state variables (on?off, or low?high) is considered.<br>
Our focus here is not to consider any one circuit but many circuit architectures or topologies. Because of the incessant changes of biochemical parameters and the lack of quantitative information about their values, such an approach is appropriate for studying fundamental properties of cellular circuits; in particular, one may ask what features are responsible for the robustness of a circuit architecture or topology [7,9,29,37,38]. In this work, we carry out an analysis for a model of transcriptional regulation networks with important functions in developmental processes. Despite its level of abstraction, this model has proven highly successful in explaining the regulatory dynamics of early developmental genes in the fruit fly Drosophila as well as in predicting mutant phenotypes [27,39?41]. It has also helped to elucidate why mutants often show a release of genetic variation that is cryptic in the wild-type, and how adaptive evolution of robustness occurs in genetic networks of a given topology [42?45]. Most recently, it has helped explain how sexual reproduction can enhance robustness to recombination [46].<br>
The model [42] is concerned with a regulatory network of N transcriptional regulators, which are represented by their expression patterns S(t) = (S1(t), S2(t), ?, SN(t)) at some time t during a developmental or cell-biological process and in one cell or domain of an embryo. The time scale of the model's expression dynamics is the time scale characteristic for transcriptional regulation, which is on the order of minutes, and not on the order of days, weeks, or months, as for complete development from zygote to adult. The model's transcriptional regulators can influence each other's expression through cross-regulatory and autoregulatory interactions, which are encapsulated in a matrix w = (wij). The elements wij of this matrix indicate the strength of the regulatory influence that gene j has on gene i (Figure 1A). This influence can be either activating (wij &gt; 0), repressing (wij &lt; 0), or absent. Put differently, the matrix w represents the (regulatory) genotype of this system, while the expression state is its phenotype. We model the change in the expression state S(t) of the network (hereafter also referred to as a circuit) as time t progresses according to the difference equation <br>
				 where ? is a constant, and ?(.) is a sigmoidal function whose values lie in the interval (?1, +1). This equation reflects the regulation of gene i's expression by other genes. We are here concerned with networks whose expression dynamics start from a prespecified initial state S(0) at some time t = 0 during development, and arrive at a prespecified stable equilibrium or ?target? expression state S?. We will call such networks viable networks. The initial state is determined by regulatory factors upstream of the network, which may represent signals from the cell's environment or from other domains of an embryo. Transcriptional regulators that are expressed in the stable equilibrium state S? affect the expression of genes downstream of the network. As a modeling assumption, we think of their expression as critical for the course of development. Thus, deviations from S? are highly deleterious. It is because our work starts from such a developmental framework that S(0) and S? play a central role; this is in contrast with most studies determining the generic properties of random Boolean networks [30,31,37,38,47?50].<br>
			<br>
We here examine the relationship between robustness and network topology for millions of networks with different topologies. Topology is synonymous with the ?structure? of the matrix w, because each of w's nonzero entries corresponds to one regulatory interaction among the circuit's genes (Figure 1A). Changes in topology correspond to the loss of a regulatory interaction (wij ? 0), or to the appearance of a new regulatory interaction that was previously absent. Such topological changes can occur on very short evolutionary time scales, in particular in higher eukaryotes with large regulatory regions [51]. This underscores the need to study their effects on network robustness. In our analysis, we first ask how robustness to mutations and noise varies within an ensemble of networks with different topologies. Subsequently, and more importantly, we also ask whether highly robust topologies can evolve from topologies with low robustness through gradual topological changes.<br>
<br>
Results<br>
Robustness to Noise and Robustness to Mutations Are Highly Correlated<br>
Robustness to mutations on one hand, and to environmental change and internal noise on the other hand, correspond to two different measures of robustness in the circuits we study. In both cases, the robust feature is the network's equilibrium gene expression pattern S?. Robustness to mutations corresponds to robustness of S? to changes in regulatory interactions, either as a change in network topology, or as a change in the strength of regulatory interaction. Specifically, for a given viable network, we define mutational robustness R? as the fraction of its one-mutant neighbors that are also viable. Robustness to noise corresponds to robustness of S? to changes in gene expression patterns. We use three complementary measures of robustness to noise. The first of them is the probability R?,1 that a change in one gene's expression state in the initial expression pattern S(0) leaves the network's equilibrium expression pattern S? unchanged. The second measure is the fraction R?* of genes whose expression needs to change in S(0), such that the probability of attaining the equilibrium state falls below ?. The third measure is the probability that changes in the gene expression trajectory from S(0) to S? preserve S? (see Text S1 for details). Importantly, robustness to noise and robustness to mutations are highly correlated. Figures 2 and S1A illustrate this for one example, a network of N = 20 genes (Spearman's s &gt; 0.56, p &lt; 10?15). Similar observations have been made for mutational robustness and thermodynamic stability in RNA secondary structures [52].<br>
We show in Text S1 that all important network properties depend only on the fraction of genes that differ in their expression state between S(0) and S?. We refer to this fraction as the distance d between the two states (0 ? d ? 1). We find highly significant associations between our four measures of robustness for wide ranges of values for this distance d, the number of genes N, and number of regulatory interactions. Examples are shown in Tables S1 and S2 for two of our measures of robustness to noise. A comparison of the tables shows that the significant correlations between robustness to mutations and to noise exist regardless of how robustness to noise is assessed. The data in these tables are for networks where regulatory interactions are discrete (wij = ?1), but the same conclusions hold for networks with continuous-valued regulatory interactions (Figure S1B; Tables S3 and S4).<br>
<br>
The Fraction of Viable Networks Is Tiny<br>
Consider all networks of a given number N of genes and total number M of regulatory interactions. The fraction vf of viable networks, that is, networks that arrive at a prespecified target expression state S? given an initial gene expression state S(0), is generally tiny. We first present a qualitative argument for why this should be so. Consider an equilibrium expression state S?. Now choose one network w at random out of the space of all possible networks. Because there are 2N possible equilibrium states, the probability that this network w arrives at S? should be at most on the order of 1/2N. This simple observation suggests that the fraction of viable networks should be exponentially small in N. A quantitative analysis for networks with wij = ?1 confirms this exponential scaling (Figure S2). Even for small networks, the fraction vf of viable networks is small. For example, we used exhaustive enumeration to show that for networks with N = 4 genes (M = 8 regulatory interactions, d = 0.5) less than 0.5% of networks are viable. For moderately sized networks of N = 20 genes (M = 200, d = 0.5), random sampling establishes that vf = 5.1 ? 10?9 ? 1.7 ? 10?10. That is, fewer than one in one hundred million networks are viable.<br>
<br>
A ?Metagraph? of Networks That Differ Greatly in Their Robustness<br>
Next, we focus on the set of viable networks with a given number N of genes and a number M of regulatory interactions within a narrow range. From the set of these networks, we define a graph whose nodes correspond to the networks: two networks (nodes) in this graph are connected if they differ in the value of only one regulatory interaction (Figure 1B). We call this graph a metagraph, because it is a graph whose nodes are networks?which could themselves be represented as (oriented) graphs. These nodes differ in the topology of their regulatory interactions. Neighboring networks in the metagraph arise from one another by genetic changes that affect only one regulatory interaction (Figure 1B). In the biological evolution of network topology, this metagraph could be traversed through a series of small genetic changes, each affecting one regulatory interaction.<br>
From here on we shall concentrate on mutational robustness. This is not much of a restriction since robustness to noise and to mutational robustness are highly correlated; thus, one can be used as a proxy for the other. Clearly, metagraphs are ideally suited to study how mutational robustness evolves. In fact, all questions about the evolution of mutationally robust regulatory network topologies are questions about the structure of the metagraph. We discuss most of our results for the case where regulatory interactions are discrete (wij = ?1), but nearly all of our results hold also for regulatory interactions that have continuous values.<br>
The higher a network's mutational robustness R? is, the larger the number of regulatory interactions one can change without affecting the network's equilibrium gene expression state S?. If all nodes in the metagraph had the same number of neighbors, all networks would be equally robust, and robustness could not change in biological evolution. However, this is not the case. Figure 3 shows the distribution of mutational robustness for networks with N = 20 genes and M ? 0.25 N2 regulatory interactions. There are clearly vast differences in robustness among networks. For example, the most robust network in Figure 3 has R? = 0.98 and is almost 300-fold more robust than the network with the lowest robustness (R? = 3.3 ? 10?3). Figure S3 shows that qualitatively the same observations hold for networks with varying regulatory interactions and varying distance between S(0) and S?. Networks with continuously valued regulatory interactions show a similarly broad distribution of robustness. All of these properties seem to be general, holding for mutational robustness and for our three measures of robustness (unpublished data). The distribution of robustness has no tendency to become more ?concentrated? at a pronounced peak with increasing N. We thus cannot restrict ourselves to studying a ?typical? R?. In sum, different networks show very different robustness to mutations or noise, and heterogeneity in robustness is the rule.<br>
<br>
Networks Can Evolve Gradually toward Robustness<br>
A key question regarding the evolvability of robust networks is whether one can reach highly robust networks starting from networks of low robustness through a series of small genetic changes. This is not self-evident. Recall that viable networks comprise a tiny fraction of all possible ones. They could be widely scattered in the space of all possible networks and occupy disconnected islands in this space. However, our analysis indicates precisely the opposite. The metagraph of viable networks has one ?giant? connected component that comprises most or all viable networks. Any two networks in this component can be reached from one another through gradual changes of one regulatory interaction at a time, changes that never leave the space of viable networks.<br>
We demonstrated these properties in the following manner. First, for networks with few genes, we can obtain all viable networks through exhaustive enumeration. In this case, we test whether the metagraph of viable networks is connected by comparing the whole list of viable networks to that associated with a connected component. This component is constructed by initiating a random walk on the metagraph, starting from an arbitrary viable starting network. The list of all distinct viable networks visited during this random walk is a lower bound on the size of the connected subpart of the metagraph that contains this starting network. This number usually is very close or equal to the total number of viable networks. For example, for networks of N = 5 genes, 6 ? M ? 7 regulatory interactions, and d = 0.4, there are a total of 37,338 viable networks (out of 6.3 ? 107 possible ones). A random walk visiting 107 networks finds all 37,338 of these. More generally, long random walks through the space of viable networks visit all but a very small fraction of the nodes of the metagraph, and this missing fraction decreases as N increases.<br>
Second, when N becomes too large to enumerate all viable networks, Monte Carlo sampling becomes necessary (see Text S1). For networks with few genes and few regulatory interactions (one to two interactions per gene), some randomly chosen viable networks are isolated points of the metagraph. We note that this situation is exceptional and results from our constraint that forbids more than a prespecified small total number of regulatory interactions. In the generic case, however, which becomes more and more prevalent as N increases, an overwhelming fraction of the whole metagraph is in one ?giant component? (Table S5). For instance, a fraction 0.998 of viable networks belong to the giant component of the metagraph for N as small N = 12 (M ? 0.25 N2, d = 0.5).<br>
We conclude that most or all viable networks are contained in one large connected component for the cases we examined here. This means that nearly all viable networks can evolve toward greater robustness through gradual changes in topology. This great cohesiveness of viable networks is not self-evident, as we show in Text S1. Specifically, it does not hold for a metagraph comprising the same number nv of nodes as the above metagraph of viable networks, where neighboring nodes (networks) differ in one regulatory interaction, but where the nodes need not be viable. In such a ?random metagraph,? the probability that an arbitrary node is isolated is bounded from below by [1 ? K/(n ? nv + 1)]nv?1 ? 1, where K is the number of neighbors of a network w. It follows immediately that only a negligible fraction of the nodes in a random metagraph is not isolated.<br>
The connectivity difference between metagraphs of viable and of random networks is already drastic for a small number of genes. For example, for N = 6, M ? 0.5 N2 regulatory interactions, and d = 0.5, there are a total of n = 8.59 ? 1013 networks. Using random sampling, we find that there are nv = 7.77 ? 106 viable networks, of which only a fraction, 0.0022, is isolated. For a random metagraph of the same size, the fraction of isolated nodes would be at least 0.988.<br>
For all of our previous analyses, we have considered only one pair of initial and target gene expression states. Regulatory gene networks, however, often function in more than one context inside the organism, each of which can be characterized by a different pair of initial and equilibrium states. A detailed analysis of such multiple gene expression states will be reported elsewhere. Here, we just note that our key results are unaffected by these additional constraints. Specifically, although for any given N and M, the metagraph of viable networks is more often disconnected than where there is a single initial-target pair, it is still dominated by a single connected giant component as N increases, and the networks in this component still show a broad distribution of robustness (Figure S5).<br>
<br>
Mutational Robustness and Natural Selection<br>
All evolution by natural selection takes place in populations of organisms. To find out to what extent natural selection can change mutational robustness, one thus has to take into account the dynamics of an evolving population of organisms (networks) on the metagraph. Specifically, the question is to what extent natural selection can shape the average mutational robustness (or the robustness to noise) of networks in the population [53]. We here briefly summarize a relevant result from earlier work [54] that was derived with biological macromolecules in mind but applies also to the networks considered here. This result pertains to populations that evolve under the influence of (regulatory) mutations and strong selection to maintain viability. For small population sizes P, small number of genes N, or small mutation rates ? (PN? ? 1), natural selection is not capable of increasing population robustness beyond the mean robustness of the networks in the metagraph. In contrast, for larger populations with sufficiently high mutation rates (PN? ? 1), the population becomes concentrated at nodes (genotypes) of higher mutational robustness. To understand the selective force driving the evolution of high mutational robustness, consider two hypothetical subpopulations of networks on a metagraph, one with low robustness, the other with high robustness. Mutations arrive at the two subpopulations with equal frequency. However, individuals in the subpopulation with low robustness are much more likely to lose viability than individuals with high robustness. Over many generations, this preferential elimination of individuals with low robustness drives the evolution of high robustness. In the long run, the average robustness <br>
					R?? in a population of networks will exceed the mean of R? when averaging uniformly over the metagraph. In fact, in the large population size limit, <br>
					R?? converges to an eigenvalue associated with the adjacency matrix [55] of the metagraph [54]. We here estimate <br>
					R?? numerically.<br>
				<br>
Figure 4 shows the mean population robustness <br>
					R?? for a large population subject to natural selection (black bars) and for a random sample of the metagraph (open bars) which represents the average robustness of networks in the metagraph. The difference is a measure of the extent to which natural selection can increase robustness. Figure 4 shows that although natural selection acts on viability alone, population robustness is enhanced compared with the metagraph average. Although this holds regardless of the number N of genes, the ratio of population robustness to average robustness increases with increasing N, rising to a factor of approximately three when N = 40. Larger values of N have greater potential to evolve increased robustness. The same holds for networks with continuously valued regulatory interactions (Figure S6) and for our different measures of robustness to noise (unpublished data).<br>
				<br>
<br>
Designing Robust Networks<br>
We here develop a ?minimum-frustration? [56] prescription for the design of a highly robust network. There are two key requirements for robust network design. The first of them is that the equilibrium gene expression state should be highly stable, such that noise or mutations leave it unchanged. In this regard, we note that the expression Si of each gene in the equilibrium state has to fulfill the equation<br>
					<br>
				<br>
The equilibrium expression state will be most stable if the sum above is large in absolute value, because changes in individual gene expression states or regulatory interactions will not affect the sign of the sum. In the discrete case (wij = ?1), the largest possible absolute value is achieved if one chooses wij = Si,?Sj,? for all nonzero regulatory interactions.<br>
The second key requirement for a robust network is that the equilibrium gene expression state can be reached quickly from the initial state. The longer the network's trajectory to the equilibrium state, the greater the chance that the trajectory veers off course due to gene expression noise, and the smaller the network's mutational robustness. For example, in a sample of 104 networks with N = 40 genes, M ? 100 regulatory interactions, and d = 0.5, R? is highly correlated with the reciprocal of the time t needed to reach the equilibrium state (R? ? 1/t: s = 0.88, p &lt;10?15).<br>
In the discrete-time model we consider, the shortest possible time from initial to equilibrium state is one time step. Which networks have this shortest possible trajectory? To find out, it is best to separate the genes of the network into two groups, those that have the same (?aligned?) expression state in the initial and equilibrium expression pattern (Si(0) = Si,?), and those that have a different (?misaligned?) expression state (Si(0) ? Si,?). To reach the equilibrium state in just one step (the shortest possible amount of time), a network has to obey the equation<br>
					<br>
				<br>
If one again chooses wij = Si,?Sj,? for all j belonging to ?aligned? genes, then the left sum will make a contribution that is most favorable. In addition, this choice also favors the stability of the equilibrium state. For the group of ?misaligned? genes, the opposite choice, e.g., wij = ?Si,?Sj,? might seem appropriate, because it has the correct sign to validate the equation; however, this choice would directly oppose the stability of the equilibrium state.<br>
Taken together, these observations suggest the following prescription for designing highly robust networks. For any gene j whose expression state is the same in the initial and the equilibrium states, choose wij = Si,?Sj,? whenever a regulatory interaction is present. For genes that are not of that type, we assign the magnitude of nonzero interactions wij such that the right-hand sum in the above expression is zero or close to zero for every i. (For a sufficiently large total number of regulatory interactions, i.e., M/N ? 1, choosing random values for these interactions will achieve this goal.)<br>
We note that although the fraction of networks designed to be highly robust may be tiny, their absolute number may be astronomical for any given number of genes, initial states, and equilibrium states, simply because of the different ways to choose which regulatory interactions are present. (Furthermore, our prescription also leaves some freedom for choosing the strengths of those interactions that are to cancel out.) We also note that our prescription resembles the Hebb rule for storing information in neural networks [57], an important difference being that our networks are asymmetric (wij ? wji). In Text S1, we demonstrate that the simple principles discussed here are sufficient to produce highly robust networks (Figure S4).<br>
<br>
<br>
Discussion<br>
To summarize, we find that networks of different topology vary by orders of magnitude in their robustness to mutations and noise. Highly robust networks can be ?designed? following a simple prescription for their regulatory interactions. Most importantly, highly robust networks can be reached from networks with lower robustness through gradual evolutionary change, one regulatory interaction at a time. Not only that, all or most networks with a given equilibrium gene expression state are connected in one large metagraph of network topologies. These findings hold for a wide range of numbers of genes, total numbers of regulatory interactions, and different initial and equilibrium gene expression patterns.<br>
Albeit the subject of some earlier work [50,58], the topology of regulatory networks has received more widespread attention since the realization that many biological systems keep functioning when faced with a wide spectrum of genetic and nongenetic change. Such change alters the biochemical parameters?concentrations, binding constants, half-lives, etc.?under which a network operates. It requires studying network topologies if one is to understand robustness, because much else about a network is in constant flux. Important earlier work has largely focused on the extent to which one or few network or circuit topologies supported by experimental data are robust [7,9,10]. The assertion that such networks are indeed robust has a major limitation: how do we know that their robustness is unusual or remarkable? This question can only be answered by studying many network topologies and their distribution of robustness. The same holds for our central question: how can robustness can be achieved through gradual Darwinian evolution, a process that does not create radical new network architectures in one step, but slowly modifies existing networks? An evolutionary perspective becomes important here: although circuits with a desirable feature may exist, it may be impossible to reach them through gradual evolution from other circuits. The difficulty in answering these questions is due to insufficient empirical information on topological variants of any one specific biological network.<br>
What is the value of our results, given that they are based on a general model of transcriptional regulation networks, and not on one specific network in one organism? Results from such an abstract model have the advantage that they may apply to all or most networks that share specific characteristics. In this regard, we note that our model is designed to capture the qualitative behavior of transcriptional regulation networks in which cooperative regulation of gene expression is important. Given how central such cooperative regulation is in eukaryotes, it is perhaps not surprising that variants of this model can correctly predict the gene expression dynamics of biological circuits such as the Drosophila gene regulation network [27,39?41]. Also, our key findings do not depend strongly on many details such as the number of genes or regulatory interactions. Finally, similar results?broad distribution and evolvability of robustness?have been recently reported for a small sample of circadian oscillator networks that are very different from our regulatory model [59], which suggests that robustness may be evolvable for a broad class of cellular networks. At the least, our results call for analysis of a wide range of experimentally well-understood circuits with partially known topology, in order to find out whether there are biologically important exceptions to our findings.<br>
The evolution of increased robustness by the mechanism discussed here is neutral evolution. This does not mean that all mutations that occur in the process are neutral. Some mutations in regulatory interactions?those that cause a network to leave the metagraph?may be deleterious. However, such mutations are eliminated by natural selection, and only the neutral mutations survive. If we had sufficient data to study the evolution of transcriptional regulation networks over long times, for example by following changes in transcription factor binding sites, then the deleterious mutations that inevitably occur during evolution would leave only one trace: conservation?to a greater or lesser extent?of individual binding sites. Limited conservation of regulatory interactions and binding sites [60,61] is thus no contradiction to neutral evolution. It just indicates that some mutations that occurred in the evolutionary history of a network have been eliminated by natural selection.<br>
We note intriguing parallels of our observations to the work of others both on artificial systems, such as ?digital organisms,? and on natural systems, such as the sequence?structure relationships of RNA and protein molecules [52,62?70]. The secondary or tertiary structure of a molecule can be viewed as its phenotype (analogous to the gene expression pattern of a regulatory network). Its RNA or amino acid sequence is its genotype (analogous to a regulatory network with a given topology of regulatory interactions). The set of all molecules (sequences) that adopt the same structure comprises both sequences of great and little robustness to mutations or thermal noise. Most importantly, a set of sequences adopting the same structure typically form a very large connected graph called a neutral network, where sequences differing only at one residue are neighbors in the graph; our notion of a metagraph for regulatory circuits mirrors the neutral network concept. Such topological properties show that gradual evolution changing one residue at a time?analogous to changes affecting one regulatory interaction at a time?can readily traverse such a graph and find highly robust sequences, or sequences that have any other desirable feature. The observation that robustness is evolvable in biological systems at two different levels of organization?molecule and circuit?with different architectures and purposes, further suggests that our findings reflect a general organizational principle of biological systems.<br>
<br>
Methods<br>
Random sampling of viable networks.<br>
To explore the space of random viable networks, we generate such networks numerically with uniform probability when this space is discrete (for instance, when the regulatory interaction strengths are either zero or ?1). A random network is easily generated by assigning to each of the N2 values wij a random value. However, in our study we also constrain the number M of nonzero wij to lie in a given range, (M?,M+). To meet this constraint, we first compute the fraction of networks that have each of the allowed values of M. This allows us to generate a probability distribution for M within the allowed range. For any one randomly chosen M, we then choose at random the M nonzero interactions. This procedure uniformly samples the space of all networks, satisfying the range constraint on M. Keeping only those networks that are viable then leads to a uniform sampling of the space of viable networks, allowing us to estimate parameters of interest, such as the distribution of robustness. This algorithm can be extended to continuous interaction strengths (see Text S1).<br>
<br>
Exploring the connectivity properties of the space of viable networks.<br>
To show that the space of viable networks is indeed dominated by one large connected component, we first start with some arbitrary viable reference network. Then we determine numerically the fraction of viable networks that can be connected to this reference network via some series of point changes in the interaction weights. To do this, we generate a random viable network; from it, we produce a long sequence of 106 point changes that are randomized but preferentially reduce the Hamming distance to the reference network. If during this sequence we reach the reference network, then the two networks are manifestly ?connected?; otherwise, we consider that the two of them are not connected to one another. We repeat this procedure for 1,000 random reference viable networks. In practice, we find that nearly all (more than 99%) of the networks are ?connected? to the reference one.<br>
<br>
<br>
Supplementary Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2065893</b><br>
Stochastic Noise and Synchronisation during Dictyostelium Aggregation Make cAMP Oscillations Robust<br>
Stable and robust oscillations in the concentration of adenosine 3?, 5?-cyclic monophosphate (cAMP) are observed during the aggregation phase of starvation-induced development in Dictyostelium discoideum. In this paper we use mathematical modelling together with ideas from robust control theory to identify two factors which appear to make crucial contributions to ensuring the robustness of these oscillations. Firstly, we show that stochastic fluctuations in the molecular interactions play an important role in preserving stable oscillations in the face of variations in the kinetics of the intracellular network. Secondly, we show that synchronisation of the aggregating cells through the diffusion of extracellular cAMP is a key factor in ensuring robustness of the oscillatory waves of cAMP observed in Dictyostelium cell cultures to cell-to-cell variations. A striking and quite general implication of the results is that the robustness analysis of models of oscillating biomolecular networks (circadian clocks, Ca2+ oscillations, etc.) can only be done reliably by using stochastic simulations, even in the case where molecular concentrations are very high.<br>
<br>
Introduction<br>
Dictyostelium discoideum are social amoebae which normally live in forest soil, where they feed on bacteria [1]. Under conditions of starvation, Dictyostelium cells begin a programme of development during which they aggregate to eventually form spores atop a stalk of vacuolated cells. At the beginning of this process the amoebae become chemotactically sensitive to cAMP, and after about six hours they acquire competence to relay cAMP signals. After eight hours, a few pacemaker cells start to emit cAMP periodically. Surrounding cells move toward the cAMP source and relay the cAMP signal to more distant cells. Eventually, the entire population collects into mound-shaped aggregates containing up to 105 cells ([2], p. 4350). The processes involved in cAMP signalling in Dictyostelium are mediated by a family of cell surface cAMP receptors (cARs) that act on a specific heterotrimeric G protein to stimulate actin polymerisation, activation of adenylyl and guanylyl cyclases, and a number of other responses [3]. Most of the components of these pathways have mammalian counterparts, and much effort has been devoted in recent years to the study of signal transduction mechanisms in these simple microorganisms, with the eventual aim of improving understanding of defects in these pathways which may lead to disease in humans [4].<br>
In [5], a model was proposed for the network of interacting proteins involved in generating cAMP oscillations during the early development stage of Dictyostelium.<br>
The model, which is written as a set of nonlinear ordinary differential equations, exhibits spontaneous cAMP oscillations of the correct period and amplitude, and also reproduces the experimentally observed interactions of the MAP kinase ERK2 and protein kinase PKA with the cAMP oscillations [6]. In addition to accurate reproduction of experimental data for one chosen set of parameter values, model robustness to parameter uncertainty in appropriate subsets of those parameters has been proposed by several authors in recent years as an important criterion for model validity [7,8]. The idea here is simply that the model's dynamics should not be highly sensitive to changes in parameters whose values either cannot be determined accurately, or are known to vary widely in vivo. In [5], the dynamics of the model are claimed to be highly robust when subjected to trial and error variations of one kinetic parameter at a time. More systematic robustness analyses of this model published in [9] and [10], however, revealed an extreme lack of robustness in the model's dynamics to a set of extremely small perturbations in its parameter space. Since the cAMP oscillations observed in vivo are clearly very robust to wide variations in these parameters, this result could be interpreted as casting some doubt on the validity of the model. On the other hand, there is strong experimental evidence to support each of the stages and interconnections in the proposed network, and the ?nominal? model's dynamics show an excellent match to the data.<br>
In this paper, we attempt to resolve this apparent paradox by showing how a stochastic representation of the deterministic model proposed in [5], together with the incorporation of synchronisation effects due to the diffusion of extracellular cAMP between aggregating cells, results in an extremely robust model for cAMP signalling in Dictyostelium. The effects of stochastic noise in biomolecular networks have been intensively studied from a number of points of view in recent years [11?16]. Efficient ways of calculating the magnitude of noise in biomolecular networks are described in [17,18]. In addition, the ability of noise to generate oscillations and the effect of noise on the resonant frequency are analysed in [19]. Similar synchronisation structures, i.e., coupled oscillators, are found in many biomolecular networks, for example, glycolytic oscillations in yeast cells [20], circadian oscillations [21], etc. Typically, however, analyses of oscillations in such systems are conducted in a deterministic framework [20,21]. A common feature of all such studies is that they emphasise the necessity of taking stochastic noise effects into account only for models of systems involving very low molecular copy numbers. In this paper, we have an example of a situation where it appears that, at least for the purposes of robustness analysis, stochastic noise effects must be taken into account even for very high intracellular molecular concentrations. In addition, most previous studies that have considered the issue of robustness have investigated robustness of the system to the effects of stochastic noise, see for example [12]. The possibility of beneficial effects arising from stochastic fluctuations in genetic and biochemical regulatory systems was first proposed in [22]. The results contained in this paper provide strong evidence that stochastic noise is actually an important source of robustness for this, and probably many other, oscillatory biological systems.<br>
<br>
Results<br>
Stochastic Noise Improves the Robustness of cAMP Oscillations in Individual Dictyostelium Cells<br>
The original model for cAMP oscillations given in [5] comprises the set of coupled nonlinear ordinary differential equations shown in Materials and Methods as in Equation 1. The stochastic version of the model is obtained by converting the ordinary differential equations into the corresponding fourteen chemical reactions, Equation 2. The interaction network described by both models is shown in Figure 1A. After external cAMP binds to the cell receptor CAR1, ligand-bound CAR1 activates adenylyl cyclase ACA and the mitogen activated protein kinase ERK2. ACA stimulates the production of cAMP and the cAMP activates the protein kinase PKA. PKA inhibits ACA and ERK2, which form two feedback loops around the internal cAMP. As shown in Figure 1B, a 2% perturbation from the nominal values of the kinetic parameters in the original deterministic model is sufficient to destroy the stability of the oscillation and make the system converge to a steady state in about 6 h [10]. On the other hand, Figure 1B shows that the stochastic model continues to exhibit a stable oscillation for this perturbation to the nominal model parameters. The distributions of the numbers of all molecular species are shown in Figure 2A?2F. For the deterministic model, the numbers of each molecular species are concentrated in a narrow region. On the other hand, for the stochastic case they are relatively widely spread, which shows that the magnitude of noise in the network has a dominant effect in terms of generating oscillations. The critical factor in terms of stochastic noise generating oscillations is the number of molecules in the cell. That is, the magnitude of the noise depends on the square root of the number of molecules. Moreover, the number of molecules is a function of the cell volume as shown in [23]. Hence, unless the cell volume was far larger than that which corresponds to biological reality, the stochastic effects considered here will remain dominant.<br>
To systematically compare the robustness properties of the two models, we generated 100 random samples of kinetic constants, the cell volume, and initial conditions from uniform distributions around the nominal values for several different uncertainty ranges. The period distributions of the deterministic model for three uncertainty ranges, i.e., 5%, 10%, and 20%, are shown in Figure 3A?3C. The same results for the stochastic model are shown in Figure 4A?4C. In the figures, the peak at the 20 min period denotes the total number of cases where the trajectories converged to some steady state value. Note that the proportion of non-oscillatory trajectories is already 2% for the deterministic model with just a 5% level of uncertainty. On the other hand, for a 5% level of uncertainty in the model parameters, the stochastic model shows perfect robustness, with not a single converging case discovered in the simulations. In fact, for perturbations of up to 20%, a significant majority of cases still displayed stable oscillations, with only 14% converging to a steady state. Similar improvements in the robustness of the amplitude distributions are shown in Figure 3D?3F and Figure 4D?4F. For a 5% level of uncertainty, the variation in the amplitude of the oscillation is much wider for the deterministic model, while for perturbations of up to 10% and 20% its amplitude distribution seems to become almost bimodal. For the stochastic model, on the other hand, the standard deviations of the amplitude for all cases are smaller than that for the deterministic cases.<br>
<br>
Synchronisation of Oscillations in Aggregating Dictyostelium Cells Provides Robustness to Cell-to-Cell Variations<br>
One important mechanism, which is missing in the model of [5], is the communication between neighbouring Dictyostelium cells through the diffusion of extracellular cAMP. During aggregation, Dictyostelium cells not only emit cAMP through the cell wall but also respond to changes in the concentration of the external signal which result from the diffusion of cAMP from large numbers of neighbouring cells. The authors in [24] clarified how cAMP diffusion between neighbouring cells is crucial in achieving the synchronization of the oscillations required to allow aggregation. Interestingly, similar synchronisation mechanisms have been observed in the context of circadian rhythms?the consequences and implications of such mechanisms are discussed in [25].<br>
To investigate the effect of synchronisation on the robustness of cAMP oscillations in Dictyostelium, we extended the stochastic version of the model of [5] to capture the interactions between cells as described in Materials and Methods. Figure 5A shows an example of the extended model for the case of three cells in close proximity to each other. Because each cell is not exactly the same, the kinetic constants and initial conditions are assumed to be different for each individual cell model. As shown in Figure 5B, with just a 10% level of variation among the different cells' kinetic parameters, the cell volume, and initial conditions, the oscillations generated by 20 non-interacting cells will be completely asynchronous with each other after only 10 min. On the other hand, the extended model which allows communication between the cells through the diffusion of cAMP provides synchronised and stable oscillations for variations of up to 20% in the parameters of the individual cells?Figure 5C and 5D. Thus the dynamics of the cAMP oscillations appear to depend strongly on the strength of synchronisation between the individual cells, as well as on the level of cell-to-cell variation. These factors may in fact be the critical mechanisms for developing morphogenetic shapes in Dictyostelium development?note that [26] showed that cell-to-cell variations desynchronise the developmental path and argued that they represent the key factor in the development of spiral patterns of cAMP waves during aggregation.<br>
Robustness analysis results for the extended model in the case of five and ten interacting cells are shown in Figures 6 and 7. Figure 6A?6C and Figure 7A?7C show that the variation in the period of the oscillations reduces as the number of synchronised cells in the extended model increases. The proportion of non-oscillating trajectories for the five-cell extended model with a level of variation between the cells of 20% is only 12% of the total. This proportion is further reduced as the number of synchronised cells increases. For the extended model with ten cells, the first non-oscillating cells appear with a 20% level of variation and these make up only 5% of the total. The mean values of the amplitude distributions, shown in Figures 6D?6F and 7D?7F, are more or less similar. However, it may be the case that greater effects on the amplitude distribution are produced for larger numbers of cells.<br>
Note that for computational reasons the number of interacting cells considered in the above analysis was limited to ten. In nature, some 105 Dictyostelium cells form aggregates leading to slug formation, and each cell potentially interacts with far more than ten other cells. The stochastic model here suggests how either direct or indirect interactions will lead to even stronger robustness of the cAMP oscillations as well as entrapment and synchronization of additional cells.<br>
<br>
<br>
Discussion<br>
As well as resolving an apparent paradox concerning the robustness of a proposed model for cAMP oscillations in Dictyostelium cells, the results of this study make some interesting contributions to the ?stochastic versus deterministic? modelling and simulation debate in Systems Biology. Generally speaking, the arguments in favour of employing a stochastic framework for the modelling of intracellular dynamics have focused on the case of systems involving small numbers of molecules, where large variabilities in molecular populations favour a stochastic representation. Of course, this immediately raises the question of what exactly is meant by ?small numbers??see [27] for an interesting discussion of this issue. In this paper, however, we have analysed a system in which molecular numbers are very large, but the choice of a deterministic or stochastic representation still makes an enormous difference to the robustness properties of the network model. The implications are clear?when using robustness analysis to check the validity of models for oscillating biomolecular networks, only stochastic models should be used. The reason for this is due to the second major result of the paper?intracellular stochastic noise can constitute an important source of robustness for oscillatory biomolecular networks, and therefore must be taken into account when analysing the robustness of any proposed model for such a system. Finally, we showed that biological systems that are composed of networks of individual stochastic oscillators (e.g., aggregating Dictyostelium cells) use diffusion and synchronisation to produce wave patterns which are highly robust to variations among the components of the network.<br>
<br>
 Materials and Methods<br>
The deterministic model.<br>
The deterministic model for cAMP oscillations used in this study is taken from [5] and is given by<br>
					where ACA is adenylyl cyclase, PKA is the protein kinase, ERK2 is the mitogen-activated protein kinase, RegA is the cAMP phosphodiesterase, cAMPi and cAMPe are the internal and the external cAMP concentrations, respectively, and CAR1 is the ligand-bound cell receptor.<br>
				<br>
<br>
The stochastic model.<br>
To transform the above ordinary differential equations into the corresponding stochastic model, the following fourteen chemical reactions are deduced [28]:<br>
					where ? represents some relatively abundant source of molecules or a non-interacting product, nA is Avogadro's number, 6.023 ? 1023, 10?6 is a multiplication factor due to the unit ?M, and V is the size of the volume where the reactions occur. In our computations, we chose V equal to 3.672 ? 10?14 l, to ensure that for the nominal kinetic parameter values the average number of ligand-bound CAR1 molecules corresponds to the average number of CAR1 receptors on the surface of a Dictyostelium cell 4 h after the initiation of development, which is around 40,000 [5]. The probability of each reaction occurring is defined by the rate of each reaction. For example, the probabilities during a small length of time, dt, that the first and the second reactions occur are given by k1 ? CAR1 and k2/nA/V/10?6 ? ACA ? PKA, respectively. The probabilities for all the other reactions are defined similarly. Based on these, the chemical master equation is obtained and solved using standard numerical routines [29].<br>
				<br>
To consider synchronisation between multiple cells, Equation 2 is extended under the assumption that the distance between cells is small enough that diffusion is fast and uniform. In this case, the above reactions for each individual cell just need to be augmented with one reaction that includes the effect of external cAMP emitted by all the other cells. Since the external cAMP diffuses fast and uniformly, the reaction involving k13 is modified as follows:<br>
					for i = 1, 2, ?, nc ? 1, nc, where cAMPe is the total number of external cAMP molecules emitted by all the interacting cells, nc is the total number of cells, ki13 is the i-th cell's kinetic constant for binding cAMP to CAR1, and CAR1i is the i-th cell's CAR1 number.<br>
				<br>
Note that the diffusion constant, D, of cAMP is equal to 4.0 ? 10?4 cm2/s [24]. At the stage in the aggregation process considered here, there will be ten cells in a 100 ?m ? 100 ?m rectangular region assuming a density of 105 cells/cm2 [26]. The diffusion time is given by r2/(6D), where r is the diffusion distance [30]. Hence, the diffusion time from one corner to the other corner of the rectangular region considered, i.e., the farthest possible distance, is approximately 0.083s. This is orders of magnitude faster than the usual period of cAMP oscillations, which is between 5 min and 10 min. Therefore, the effect of diffusion speed, i.e., the effect of cAMP spatial distributions on cAMP oscillations will be minor during this stage of aggregation. However, if the distance between cells is very large, as could be the case in the early stages of aggregation, then the spatial distribution will have a significant effect, and a corresponding wave of cAMP over the region is observed. On the other hand, if the distance between cells becomes very small, then most of the cAMP molecules will be almost immediately bound to the receptors before diffusion can occur. Indeed, these issues could be proposed as a possible explanation for the qualitative changes in Dictyostelium which occur after aggregation.<br>
<br>
Random sampling for Monte-Carlo simulations.<br>
To ensure a consistent procedure for checking the robustness of both the deterministic and stochastic models, the Monte-Carlo simulation technique is used. The kinetic constants are sampled uniformly from the following:<br>
					for i = 1, 2, ?, nc ? 1, nc and j = 1, 2, ?, 13, 14, where <br>
					 is the nominal value of kj (given in Figure 1), p? is the level of perturbation, i.e., 0.05, 0.1, or 0.2, and ? ij is a uniformly distributed random number between ?1 and +1. The initial condition for internal cAMP is randomly sampled from the following:<br>
					for i = 1, 2, ?, nc ? 1, nc, where cAMPii is the nominal initial value of cAMPi for the i-th cell and ? icAMPi is a uniformly distributed random number between ?1 and +1. The sampling for the other molecules is defined similarly. The nominal initial value for each molecule is given by [5] as: ACA = 7290, PKA = 7100, ERK2 = 2500, RegA = 3000, cAMPi = 4110, cAMPe = 1100, and CAR1 = 5960. Similarly, the cell volume is perturbed as follows:<br>
					for i = 1, 2, ?, nc ? 1, nc, where <br>
					 = 3.672 ? 10?14 l and<br>
					 is a uniformly distributed random number between ?1 and 1.<br>
				<br>
Although some of the nominal parameter values in the model were derived from (inherently noisy) biological data, others were tuned to values which generated the required oscillatory behaviour.<br>
Thus, we have very little a priori information on the likely distributions of the parameters as a result of environmental variations and modelling uncertainty. In such cases, the uniform distribution is the standard choice for the type of statistical robustness analysis performed in this paper. Indeed, this is the approach adopted in several previous studies of robustness in biomolecular networks, [31,32]. Even if the true distribution were in fact a normal distribution, unless the variance is very small the robustness analysis results obtained with the uniform distribution would not be significantly different.<br>
The simulations for the deterministic model and the stochastic model are performed using the Runge-Kutta 5th-order adaptive algorithm and the ?-leap complex algorithm [33], with the maximum allowed relative errors 1 ? 10?4 and 5 ? 10?5 respectively, which are implemented in the software <software>Dizzy</software>, version 1.11.4 [34].<br>
<br>
Calculating period and amplitude distributions.<br>
From the simulations, the time series of the internal cAMP concentration is obtained with a sampling interval of 0.01 min from 0 to 200 min. Taking the Fourier transform using the fast Fourier transform command in <software>MATLAB</software> [35], the maximum peak amplitude is checked and the period is calculated from the corresponding peak frequency. If the neighbourhood amplitudes around the peak amplitude are greater than 70% of the peak amplitude, i.e., the signal with the peak amplitude is not a significantly dominant one, then the signal is considered to be non-oscillatory.<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2585056</b><br>
Encoding of Naturalistic Stimuli by Local Field Potential Spectra in Networks of Excitatory and Inhibitory Neurons<br>
Recordings of local field potentials (LFPs) reveal that the sensory cortex displays rhythmic activity and fluctuations over a wide range of frequencies and amplitudes. Yet, the role of this kind of activity in encoding sensory information remains largely unknown. To understand the rules of translation between the structure of sensory stimuli and the fluctuations of cortical responses, we simulated a sparsely connected network of excitatory and inhibitory neurons modeling a local cortical population, and we determined how the LFPs generated by the network encode information about input stimuli. We first considered simple static and periodic stimuli and then naturalistic input stimuli based on electrophysiological recordings from the thalamus of anesthetized monkeys watching natural movie scenes. We found that the simulated network produced stimulus-related LFP changes that were in striking agreement with the LFPs obtained from the primary visual cortex. Moreover, our results demonstrate that the network encoded static input spike rates into gamma-range oscillations generated by inhibitory?excitatory neural interactions and encoded slow dynamic features of the input into slow LFP fluctuations mediated by stimulus?neural interactions. The model cortical network processed dynamic stimuli with naturalistic temporal structure by using low and high response frequencies as independent communication channels, again in agreement with recent reports from visual cortex responses to naturalistic movies. One potential function of this frequency decomposition into independent information channels operated by the cortical network may be that of enhancing the capacity of the cortical column to encode our complex sensory environment.<br>
<br>
Introduction<br>
Oscillations are a common and prominent feature of cortical sensory-evoked activity. Presentation of sensory stimuli elicits oscillations in Electro-Encephalogram (EEG) and Local Field Potential (LFP) recordings which span a very broad frequency spectrum, ranging from a fraction of a Hz to well over 100 Hz. Oscillations in the gamma band (30?100 Hz) have elicited a great deal of attention because they are robustly triggered and modulated by sensory stimuli in olfactory [1], auditory [2],[3] and visual cortices [4]?[9]. In addition, particular types of behaviorally relevant stimuli (such as stimuli with either rhythmic, complex, or naturalistic dynamics) elicit and modulate cortical oscillations at specific frequencies within the low-frequency (&lt;10?20 Hz) range [10]?[16]. The prominent presence of oscillations in sensory systems raises two sets of important questions: how are these oscillations generated? and why are they generated? In other words, what is the mechanism of the oscillations, and what is their function?<br>
The first question has motivated many recent theoretical studies. Theorists have proposed different mechanisms giving rise to oscillatory activity in models of recurrent networks of spiking neurons. In networks coupled through purely chemical synapses, oscillatory synchrony might emerge through mutual inhibitory interactions [17],[18], or due to a feedback loop between excitatory and inhibitory neurons [19],[20]. Recent studies have focused on a regime of high noise, due to the observed irregularity of firing of neurons in cortex [21]?[24]. These studies have demonstrated the existence of an oscillatory regime in which a population of cells fire rhythmically at high frequencies, while single cells fire stochastically at a rate that is much lower than the population frequency. The network frequency was shown to depend on synaptic time scales [25],[26], as well as on the balance between excitation and inhibition [26],[27]. In a large parameter range, the network frequency is in the gamma range [26],[27]. One of the interesting features of this oscillatory regime is that it strongly depends on external inputs. For weak external inputs, the network is typically in an asynchronous state, with small damped oscillations due to finite size effects [25]. As the inputs increase, the network becomes more synchronized, and the amplitude of the oscillation increases.<br>
In spite of the effort to understand the mechanism of generation of network oscillations, the role of such oscillations in information encoding has remained so far elusive, and several key questions have yet to be addressed. First, there is currently no theoretical framework that explains how, even in the same sensory area, different types of stimuli encode information in different frequency bands [8],[10],[14]. Second, although there is evidence that external stimuli with a rhythmic structure may entrain low frequency cortical oscillations [10],[16], it is not known how the combination of fluctuations generated by stimulus-neural interactions and the oscillations generated by neural-neural interactions shapes the network dynamics and serves sensory information encoding. Third, the potential computational advantages of the cortical encoding of stimuli by a diverse and broad range of frequencies have not been understood yet.<br>
Here, we hypothesize that stimulus-related changes of low-frequency cortical fluctuations originate directly from stimulus-neural interaction and encode information about slowly varying features in the sensory or thalamic input, whereas the stimulus-related changes of high-frequency cortical oscillations are mediated by neural-neural interactions and carry information about sensory features that provoke thalamic responses that differ only in terms of their total spike rate. We tested this hypothesis by simulating a network of excitatory and inhibitory neurons modeling a local population in primary visual cortex, and we determined how the LFPs and spiking activity generated by the network encode information about either simple or complex inputs, the latter simulating sensory-related thalamic signals. We found that the simulated network encodes dynamic stimulus features according to the hypothesis described above, and in particular encodes naturalistic stimuli by using low and high response frequencies as independent communication channels, in agreement with results from visual cortex [14].<br>
<br>
Results<br>
We used a model of cortical network composed of leaky integrate-and-fire neurons, similar to the one used in [26] (Figure 1A). In brief (see Methods for full details), the model network represents in a simplified way a local circuit in primary visual cortex, and was composed of two neuronal populations: 1000 inhibitory interneurons and 4000 pyramidal neurons. The network connectivity was random and sparse with a 0.2 probability of directed connection between any pair of neurons. Synaptic currents represented fast synaptic interactions, with time courses resembling experimentally measured AMPA currents (for excitatory currents) and GABA currents (for inhibitory currents). The strength of GABAergic connections was sufficient to ensure stable activity at low firing rates in the network. Both populations received a noisy excitatory external input taken to represent the activity from thalamocortical afferents, with interneurons receiving stronger inputs than pyramidal neurons [28].<br>
We quantified the network activity by monitoring the individual spike times of each neuron, the instantaneous population firing rate (obtained counting the number of spikes fired by neurons in a given population in a 1 ms bin), the average membrane potential of each population, and the average synaptic currents. Since the spiking activity of individual cortical neurons is irregular, oscillations are often monitored experimentally by recording LFPs. Thus, to better compare the oscillations of our model to those recorded in cortex, we computed a simulated LFP from our network (see Methods for a complete description).<br>
LFPs are experimentally obtained by low pass filtering the extracellularly recorded neural signal, and are thought to reflect primarily the current flow due to synaptic activity around the tip of the recording electrode [29], as well as some other type of slow activity such as voltage-dependent membrane oscillations [30] and spike afterpotentials [31]. Thus, we computed the LFP as the sum of the absolute values of AMPA currents and of GABA currents. Since pyramidal neurons contribute maximally to generation of LFPs in cortex because their apical dendrites are organized in an approximate open field configuration, we summed only currents from synapses of the pyramidal neurons population. This model, though much simpler than some previous models [32], proved to be an effective way to generate a realistic LFP signal that match many characteristics of LFPs in sensory cortex, as shown below.<br>
As a consequence of strong recurrent inhibition, single neurons fire in an irregular fashion at low rates [25], [33]?[37]; however population oscillations are clearly visible at the network level [25],[26] (as shown in Figure 2). Since they were present when the input to the network was constant in time, these oscillations must be generated within the network. As demonstrated previously, two features of the recurrent connectivity contribute to the generation of network oscillations: delayed interactions between interneurons, which tend to favor high frequency oscillations [25],[26], and the excitatory-inhibitory feedback loops, that tend to promote lower frequency oscillations [26]. The oscillation frequency depends on delays, synaptic time constants and the balance between excitation and inhbition [26]. For the model parameters chosen here (see Methods), the frequency of the oscillation was in the 30?100 Hz range, similar to experimental observations [14].<br>
One crucial property of such excitatory-inhibitory recurrent networks is that the strength of the population oscillation strongly depends on external inputs to the network. Typically, for low enough external inputs, the network is in an asynchronous state, with weak and strongly damped oscillations in the population activity due to finite size effects, while for strong external inputs, the network tends to settle in a pronounced oscillatory state [25],[26],[37]. The goal of the present paper is to analyze how the modulation of this internally generated oscillatory synchrony and the interaction between stimulus oscillations and neural oscillations allow the population activity to transmit information about the signals received by the network, and to compare the results with available experimental data [8],[14] in order to better understand the transformation between stimuli and cortical oscillatory activity. To study how stimuli modulate the activity of the model cortical network, we injected to the network three classes of inputs of increasing complexity (Figure 1B?D), composed by different kinds of signals to which we superimposed a noise (see Methods) that was different from simulated trial to simulated trial. We first considered inputs that are constant in time and vary only in rate; we then considered periodic inputs of different frequency and amplitude, and we finally considered complex broadband inputs with a statistics similar to that of geniculate neurons responding to naturalistic movies.<br>
How Gamma Oscillations Are Modulated by the Firing Rate of the Input Stimulus<br>
We started by examining the network response to 2 seconds long constant signals with different rates, superimposed to noise (Figure 1B). Figure 2 illustrates the dynamics of the system for different rates of the signal (1.2, 1.6 and 2.4 spikes/ms). Raster plots in Figure 2A?C show that the neuronal firing was sparse in all conditions: the average firing rate of individual pyramidal neurons was 0.19, 0.45, 0.92 spikes/sec respectively, whereas the average firing rate of individual interneurons was 0.75, 1.76 and 3.95 spikes/sec respectively. Though spiking activity of single cells was seemingly random, inspection of the total firing rate from the pyramidal and the interneuronal population (Figure 2D?F) showed that increasing the signal rate led to an increased average firing in the network and that population spikes occurred in a synchronous fashion, due to pronounced population oscillations [26] in the gamma band (30?100 Hz). The increase of gamma oscillations with stimulus rate was also clearly visible in the simulated LFPs displayed in Figure 2G?I.<br>
The trial-averaged power spectra of LFPs measured in response to a wide range of firing rate values of the signal are displayed in Figure 3A. The simulated LFPs obtained in responses to such stimuli are of interest because they can be compared directly to the cortical LFPs recorded experimentally in V1 of anesthetized monkeys in response to grating stimuli with different levels of contrast and reported in Ref [8]. This is because the increase in contrast in visual stimuli leads to an increase of average firing rate in LGN [38],[39]. Consistent with the results of [8], we found that the spectra of the simulated LFP showed the highest power at low frequencies, with a local peak in the gamma range (30?100 Hz). The height and the width of the gamma range spectral peak increased monotonically with the signal firing rate. To better visualize how signal rate modulates different frequency bands of the LFP, we defined (in analogy with [8]) the power modulation at given frequency and signal rate as the difference between the trial-averaged spectral power at that frequency in response to the considered input rate and the trial-averaged spectral power at that frequency in response to the smallest input rate tested (1.2 spikes/ms), normalized to the latter power. Modulation values are reported in Figure 3B. Frequencies below 30 Hz in the simulated LFP spectra were only weakly modulated. Frequencies that were more strongly modulated by the stimulus were in the gamma band, with a peak at ?70 Hz. The modulation reached a plateau at higher frequencies (&gt;100 Hz). All these results are fully consistent with the neurophysiological experiments reported in [8] (see their Figures 2 and 4), which report a strong modulation of the LFP power by visual contrast in the gamma band (30?100 Hz), a smaller modulation at higher frequencies, and very weak modulations at lower frequencies.<br>
We next examined the behavior of the total firing rate. Unsurprisingly, the power spectra of the instantaneous population firing rate varied with signal rate in a way which was very similar to the LFP (Figure 3C and 3D). The relative weight of gamma oscillations was stronger in the interneuron population than in pyramidal neurons; nonetheless the size of the modulation in the gamma band was very similar (Figure 4B).<br>
The above power modulation analysis reveals the frequencies at which the trial-averaged power is most modulated by the stimulus, but it does not tell how easy it is to gain information about the stimulus by observing the LFP in a single trial. To address single-trial discriminability, we used Shannon information (defined in Methods and abbreviated as ?information? in the following). We delivered to the network constant signals with 8 different rates ranging from 1.2 to 2.6 spikes/ms. Each stimulation lasted 2 seconds and was repeated for 20 trials, with noise generated independently from trial to trial (see Methods). From these simulated responses, we computed the information I(S; Rf) between the LFP power Rf at a given frequency f and the stimulus input rate S. The information I(S; Rf) is plotted as a function of frequency f in Figure 4A and 4B. LFP information was very small at frequencies below 30 Hz, and was high within the gamma range, where it reached a peak of 1.32 bits at a frequency of 70 Hz (to be compared with a stimulus entropy of 3 bits). Information then decreased to an average value of 0.85 bits at higher LFP frequencies (&gt;100 Hz). It is interesting to note that the information peak was reached at a higher frequency than the one at which gamma-range oscillation power was highest. This is consistent with the empirical observation of [7] and can be explained by the fact that the power is maximally modulated by the stimulus at frequencies higher than the peak, since when the input rate is increased the gamma peak is at the same time increasing in power and moving toward higher frequencies (see Figure 3A and 3C).<br>
After determining which LFP frequencies convey the most information about the stimulus, the next step is to investigate whether the information carried by different frequencies is redundant or independent. This can be done by computing the redundancy, defined as the difference between the sum of the information provided by each individual frequency I(S; Rf1)+I(S; Rf2) and the joint information I(S; Rf1Rf2) carried by the joint observation of power at frequency f1 and f2 (see Equation 14). Results are reported in Figure 4C and 4D. For any frequency above 50 Hz, the joint information I(S; Rf1Rf2) is on average 0.51 bits less than I(S; Rf1)+I(S; Rf2), and redundancy is highly positive: on average for this range the 60% of the information of the less informative frequency in the pair. Thus, the gamma-range representation of the input spike rate is highly redundant.<br>
Information redundancy can happen because the two frequencies are tuned in the same way to the stimulus features, or because they share correlated sources of noise. The correlation of the mean responses across different stimuli of two frequencies are called ?signal correlations? [40],[41] because they are entirely attributable to stimulus selectivity. Correlations manifested as covariations of the trial-by-trial fluctuation around the mean response to the stimulus are traditionally called ?noise correlations? [40],[41]. Since these noise covariations are measured at fixed stimulus, they ignore covariations effects attributable only to shared stimulation.<br>
To understand better the causes of redundancy, we therefore computed the amount of signal and noise correlation. Figure 4E reports the amount of signal correlation between frequencies f1 and f2 (computed, for each frequency pair, as the Pearson correlation across stimuli of the trial-averaged responses). Signal correlation was very high across all gamma frequencies (mean in the gamma range: 0.71), showing that all frequencies were tuned to the same stimuli. This is consistent with the above finding (Figure 3) that the height and the width of the gamma range spectral peak increased monotonically with the input firing rate. Since the presence of signal correlations always decreases the joint information and leads to redundancy, this explains the redundancy between frequencies. Figure 4F reports the amount of noise correlation (computed as the Pearson correlation coefficient across trials at fixed stimulus of the trial-average-subtracted powers at frequency f1 and f2, averaged over all stimulus windows). There was little noise correlation (mean in the gamma range: 0.05), which means that redundancy is due to signal correlation.<br>
In experimental conditions, it is typically possible to record spikes from a limited set of neurons, and not from all neurons in a local network. Since single neurons fire irregularly at rates which are much lower than gamma frequencies, it is essentially impossible to detect the gamma oscillation from single neuron spike trains. What is the minimum amount of neurons necessary to detect gamma band modulations driven by the stimulus rate? We address this question in Figure 5, where we plot the power spectrum of the average activity of ensembles of a small number of neurons (from 1 to 10), for two different input signal rates. We have considered only pyramidal neurons for this analysis because they have a larger soma, so they are more likely to be recorded extracellularly and offer hence a clearer comparison with experimental results. Figure 5A shows the power spectrum of the pyramidal neuron with highest average firing rate when the signal rate was varied from 1.2 to 2.6 spikes/ms. In both cases the power spectrum was flat, consistent with the neuron firing approximately as a Poisson process in all stimuli conditions. The only effect of changes in stimulus rate was to increase the power uniformly at all frequencies, proportionally to the mean firing rate (maximum firing rate of 10.6 spikes/sec). Pooling together the spikes of the 5 neurons with highest firing rates was still not enough to detect gamma oscillations (Figure 5B). It was necessary to pool together the 10 neurons with highest firing rates, to see a clear peak in the gamma band for the highest rate stimulations (Figure 5C). In this case, the compound firing rate of all pyramidal neurons in the set reached 85 spikes/sec.<br>
<br>
Response of the Network to Low Frequency Oscillatory Inputs<br>
Most previous model studies of oscillations in cortical networks consider, as we did above, the network dynamics in response to time-independent input stimulation. However, naturalistic stimuli are not static, but vary on time scales which are typically much slower than the time scales of network oscillations discussed above [42]. As a preliminary to the study of the network dynamics under natural stimulation conditions, we thus next examine the network dynamics in response to periodic input signals that oscillate at frequencies below 20 Hz. We stimulated the network with periodic signals (see Methods) characterized by their amplitude A (7 different amplitude values, ranging from 0.4 to 1.6 spikes/ms in 0.2 spikes/ms steps), and their frequency ? (7 different frequency values, ranging from 4 to 16 Hz in 2 Hz steps). Each signal was presented to the network superimposed to noise (Figure 1C) in 20 different trials, each one lasting for 2 seconds. Figure 6A shows the trial-averaged LFP spectra for different frequencies ? of the input signal, averaged over all presented amplitudes A. Input oscillations in this low frequency range are reproduced in the LFP, causing a peak in the spectrum at exactly the input frequency, with little effect on the rest of the spectrum, apart from a very small modulation of the power of the gamma range. This suggests that different low frequencies in the stimulus are represented by the LFP independently from each other and are almost entirely encoded as entrainment of the corresponding low-frequency LFP band. Figure 6B reports the trial-averaged LFP spectra for different signal amplitudes, averaged over all presented frequencies. The LFP spectral peaks originated by the different low frequencies in the input did not shift place when the amplitude was increased and only increased the height of their peak, again compatibly with an entrainment with the stimulus. When the amplitude of the oscillation was increased from 0.4 to 1.6 spikes/ms, the peak power corresponding to the input frequency increased linearly of a factor 4.7?0.4 for all considered frequencies. Very similar results were obtained analyzing the power spectrum of the total firing rate (data not shown).<br>
To investigate whether the low frequency band was more sensitive to amplitude or frequency modulations, we used again information theory. Figure 6C plots (blue line) the information that the LFP power at frequency f conveyed about both the frequency ? and the amplitude A of the input spike rate. The information contained in the peaks in the signal frequencies range was in between 0.55 and 0.71 bits for a total stimuli entropy of 5.6 bits. The information about ? and A was significant (p&lt;0.05; bootstrap test) only at the LFP frequencies corresponding to the input ones (with smaller peaks at their first harmonics). However, the LFP at a given frequency may actually represent only a smaller subset of stimulus parameters; for example, either A alone or ? alone. To reveal which parameter is encoded at each frequency, we used the stimulus grouping approach of [43]. In this approach, stimuli were grouped into classes of frequency or of amplitude. When this was done, the number of unique stimuli in the set was reduced. For frequency grouping, the 49 stimuli defined by joint values of ? and A were reduced to seven groups in which all stimuli within a group had the identical value of ?. Likewise, amplitude grouping yielded seven groups defined by identical values of A. Applying the ?data processing inequality? [44], it follows that the information about the frequency or amplitude-grouped stimuli must be less than or equal to the information about the full, ungrouped stimulus set made of amplitude and frequencies. Grouped and ungrouped information can be equal if, and only if, the LFP power responds only to the stimulus feature that characterizes the grouped responses [44]. We computed the grouped information carried by the LFP at frequency f about either A only (green line) or ? only (black line). The information about ? conveyed by low frequency LFPs was larger than the one conveyed about A. This means that low LFP frequencies are more sensitive to modulations in the signal frequency than in the signal amplitude.<br>
To characterize entrainment of network activity by the input signal, we measured the circular variance of the phase difference between the signals and the band-passed LFP (see Methods). The value of this measure ranges from zero (signal and LFP are perfectly phase locked for a given frequency window), to one (the phase difference changes randomly). Figure 6D displays the average of the circular variance over all trials and amplitudes and shows that periodic stimuli were able to entrain the LFP at the corresponding frequency. The effect was stronger for lower frequencies and for larger signal amplitudes (Figure S1).<br>
<br>
LFP Responses to Complex Input Stimuli with Naturalistic Dynamics<br>
The results obtained so far with constant and periodic input signals suggest that low LFP frequencies contain information about the corresponding low frequencies in the input signal, while gamma LFP frequencies contain information mostly about the spike rate of the input stimulation. To understand the implications of these coding rules, we now turn to the study of the LFP responses to inputs with a broadband naturalistic temporal structure.<br>
We aimed at simulating responses of visual cortex dynamics during the viewing of naturalistic movie stimuli, for which detailed neurophysiological data of LFP cortical responses are available [14],[15]. We thus built a naturalistic input that closely matched the time course of multiple-unit activity (MUA) recorded from LGN of anesthetized monkeys that were presented with natural color movies (see Methods for details).<br>
We started by analyzing how different frequencies of the LGN MUA signal encode information about which scene of the movie was presented to the animal. This analysis documents the characteristics of the information injected to the network. Figure 7A reports the information that the power of LGN MUA signal described above encodes about the movie scenes; the total entropy of the movie scene characterization was 4.3 bits. Almost all the information in the LGN MUA (which provided the naturalistic input to the simulated network) was carried by the power at frequencies below 5 Hz (peak at 1 Hz with a value 0.24 bits) and in the average spike rate (the DC component of the LGN MUA), carrying 1.27 bits of information. Interestingly, different low frequencies of the LGN MUA signal carried independent information about the visual stimulus: their information redundancy, as well as any noise or signal correlation, was very small: the mean redundancy between frequencies carrying significant information was 0.008 bits (see Figure S2). The average spike rate too conveyed independent information from that carried by modulations at low frequencies: the mean redundancy between frequencies carrying significant information and the spike rate was 0.007 bits.<br>
Once we documented the properties of the naturalistic input, we injected it in the network and we measured the information about the stimuli that was carried by the power of the simulated cortical LFP at each frequency f (Figure 7B). There were two frequency regions in which the simulated network LFP was highly informative about the stimuli. The first informative LFP region was in the range 1?5 Hz. The peak information value in this region was 0.21 bits, and was reached for the 3 Hz frequency (Figure 7B). The amount of information contained in the low frequencies of the LFP was similar to the one contained in the same band of the naturalistic input. The second highly informative LFP frequency range was inside the gamma band, in the range of 50?80 Hz (Figure 7B). The peak information value at high frequencies was 0.23 bits. Intermediate simulated LFP frequencies (in the range 6?30 Hz) carried no significant information about the naturalistic input (p&gt;0.05; bootstrap test). It is interesting to compare the information carried by the power of the simulated LFPs to the information carried by real visual cortical LFPs during stimulation with a color movie [14]. Figure 7B compares the information carried by the simulated LFPs with the information carried by real LFPs obtained from seven different electrodes from monkey V1 [14], which were recorded simultaneously with the very same LGN MUA data used to construct the input to the simulated network. The information about the stimuli were computed with exactly the same procedures on both simulated and real data, and are thus directly comparable. Figure 7B shows a very close agreement between simulated and real V1 LFPs.<br>
The agreement between model and data was measured with the reduced ?2 (see Methods). The model described correctly both the shape and the information content of the spectrum of the recorded LFP (Table 1). The only appreciable difference between simulated and real data is that the low frequency peak of simulated data decays to a non-significant value at lower frequency (5 Hz) than that of V1 LFPs (whose information drops to a non-significant value at 10 Hz). One potential explanation for this discrepancy is that the input information to our cortical network decayed to zero within 5 Hz, and that low frequency LFPs just follow this trend (see below for an explicit demonstration). However, the LGN MUA signal that we used as input represents only a part of the real inputs to V1, which may receive additional information in the 5?10 Hz frequency range from other sources.<br>
Simulations with periodic and constant signals suggest that the information contained in the low frequency and gamma peaks of Figure 7B corresponds respectively to information about the low frequency modulations of the signal and its rate. To evaluate in detail this hypothesis, we calculated the correlation across all stimuli and trials between the power of each frequency in the LFP spectrum and the average rate of the signal (Figure 7C). As expected, frequencies below 5 Hz are not significantly correlated with the signal rate, while LFP frequencies above 50 Hz are strongly correlated with it (with a peak at 70 Hz). We then calculated the correlation across all stimuli and trials between the power of each frequency in the LFP spectrum and the power associated to the same frequency in the signal spectrum (Figure 7D). Low-frequency LFPs up to 8 Hz were significantly correlated to the signal frequency modulations, while LFP frequencies above 50 Hz were not. Again, very similar results were obtained analyzing the power spectrum of the total firing rate (data not shown).<br>
The entrainment between signal and LFP was measured band-passing both for frequencies ranging from 2 to 15 Hz and then computing for all pairs of signal and LFP frequencies the circular variance of the phase difference (see Methods). The average over all trials and scenes of the phase circular variance was 0.12 between the signal and LFP when they were both bandpassed at frequencies below 4 Hz. It was larger than 0.5 for every other combination of frequencies. Hence, entrainment is restricted to very low frequencies. This could be due to the fact that only low frequency oscillations in the signal are strong enough to override the noise and that entrainment is stronger for low frequency oscillations (see Figure 6D).<br>
Therefore, results obtained with simple stimuli are still valid when stimuli are realistic, suggesting that the two ?information channels? (low frequencies and gamma band) could represent the sensory stimuli in a largely independent way.<br>
As a step toward gaining a more quantitative insight on how different LFP frequencies encode information about the naturalistic stimulation, we next considered the information  about the stimuli that can be extracted from the joint observation of the powers of two LFP frequencies f1 and f2. Figure 8A shows that the highest peak in joint information (0.43 bits) was reached when combining one low frequency (3 Hz) and one gamma range frequency (70 Hz), in nice agreement with results obtained from real V1 LFPs during movie stimulation [14]. The information  was smaller when both f1 and f2 were in the gamma range (&lt;0.4 bits; Figure 8A).<br>
Why is it more convenient to extract information about the naturalistic stimulus by considering one low and one high gamma frequency LFP? Figure 8B explains this finding by considering the redundancy of the information about the stimuli obtained from two different LFP frequencies. Gamma frequencies were all very redundant to each other: frequencies in the 50?80 Hz range shared on average 0.08 bits of redundancy. In contrast, low and high frequencies shared an information redundancy which was close to zero, again in agrement with [14]. This suggests that gamma and low frequencies band contained information about largely independent stimulus features. Also the redundancy between pairs of frequencies below 5 Hz was close to zero, suggesting that each frequency in this range was modulated independently across signals.<br>
Figure 8C reports the signal correlation between pairs of LFP frequencies, which quantifies the similarity in stimulus tuning of the power of LFPs at different frequencies. Signal correlation was very high (up to 0.8) among frequencies in the gamma range, which means that they are all modulated in a similar way by the naturalistic stimuli, and explains why gamma range LFPs convey mutually redundant information about them. However, the signal correlation between the informative low frequency LFPs and the gamma LFPs was negligible, which means that these two frequency ranges are tuned to very different stimulus features. Figure 8D reports the noise correlation between any pair of different LFP frequencies, which measure if trial-to-trial fluctuations around the mean response are correlated. Noise correlation was negligible in the entire frequency range, which implies that the gamma-range redundancy is entirely attributable to signal correlation. Since low and gamma frequency LFPs shared neither noise nor signal correlation, it means that low frequency LFPs and gamma LFPs are completely decoupled in natural stimulation condition, and this is why they add independent information about the stimulus. This result is again fully consistent with the experimental finding of [14]. The only discrepancy between signal and noise correlation in real data [14] and in the present model is that real data presented strong noise correlation within the low frequency LFP range (&lt;24 Hz) [14]. The significance of this discrepancy will be addressed in Discussion.<br>
As a final step to understand the effect of the input characteristics on the network dynamics, we selectively manipulated different features of the signal and quantified the differential effect of these manipulations on the low and high frequency network LFPs.<br>
First we changed the average rate of the signals leaving their spectral content unchanged. We added a constant value to each signal varying the parameter B in Equation 9. This corresponded to an increase of the average rate of the signal equal to B times the difference between its original average rate and the average rate across all signals. We will refer to the parameter B as ?Baseline level?. In Figure 9A is shown the LFP spectrum for a single stimulus, averaged over 20 trials. When the baseline level was equal to 1, 2 and 3, the signal average value was 1.8, 1.9 and 2 spikes/ms, respectively. Low frequencies are not affected by changes in the baseline level, while the average modulation of the spectra in the 30?100 Hz range was 0.07, with a peak of 0.31 for 55 Hz. The average spectrum still resembled the one of the recorded data (Table 1). In Figure 9B we report the information about the naturalistic input carried by the LFP spectrum when the baselines are changed. When the differences among the average rates of the stimuli were increased, frequencies in the gamma band and above contained 0.1 more bits of information, while changes in the low frequency band were of 0.02 bits only. Both increasing and decreasing the baseline decreased the agreement of the information content of the spectrum of simulated and recorded LFPs, as measured by the reduced ?2 (Table 1).<br>
Second, we did the opposite: we changed the spectral content of the signals and left the average rate unchanged. Each signal was replaced with a constant function with a value equal to the average rate of the signal, therefore erasing all fluctuations. When this input was injected int the network, there was a decrease in the LFP power associated to low frequencies, but the rest of the spectrum did not display significant changes (Figure 9C), showing that signal oscillations determine only a narrow band of the signal output, while the rest is determined by noise and internal dynamics. The average information contained in the low frequency peak decreased from 0.11 from 0.02, below the significance level, while the one contained in the gamma band was left unchanged (Figure 9D).<br>
<br>
Effects of Changing Synaptic Strengths on Information Transfer<br>
Finally, we investigated the effects of varying model parameters on the encoding properties of the network, by changing the values of the GABA and AMPA synaptic strength in equations 2?5. Default values used in the previous sections are displayed in Table 2 and new values are expressed as percentage of the default ones. GABA strength was modulated in the same way both in synapses projecting to interneurons and in those projecting to pyramidal neurons. AMPA strength was modulated in the same way in all AMPA synapses, both corticocortical and thalamocortical ones. These manipulations give extra insight on the differential role of excitatory and inhibitory synapses in determining the oscillations and process sensory information. Furthermore, some of the manipulations of synaptic parameters considered here are in principle reproducible experimentally using AMPA/GABA antagonists/agonists. They can therefore be considered as testable predictions of our model.<br>
Decreasing GABA strength led to an increase of the power associated to all frequencies (Figure 10A, Table 1). The increase was close to zero only for frequencies below 5 Hz, confirming that this range is largely determined by external modulation rather than internal dynamics. The relative increase of the power did not display any peak, suggesting that it could be simply due to the increase in average activity: the overall firing rate of the network increased of more than 50% when GABA was reduced to 60% of the default value. The information transfer was not affected significantly by changing GABA strength in the investigated range (Figure 10B, Table 1).<br>
Increasing AMPA strength led to an increase of the corticocortical excitation but also of the input strength. An increase of 20% of AMPA strength was sufficient to increase the network firing rate of more than 40%. Increasing AMPA strength resulted in a general increase in the power associated to all frequencies (Figure 10C, Table 1). The increase was more pronounced for frequencies in the gamma band. On the other hand, increasing AMPA strength tended to decrease information transfer at high frequencies (Figure 10D, Table 1).<br>
Overall, these simulations show that changing synaptic strengths affects quantitatively, but not qualitatively, our results. This means that our results are robust to parameter changes, provided the network stays in an inhibition-dominated regime in which individual neurons fire at low rates in an irregular fashion. Furthermore, these simulations provide an experimentally testable prediction: specific antagonists or agonists of synaptic transmission can affect the shape of the LFP spectrum without significantly changing its information content.<br>
<br>
<br>
Discussion<br>
In recent years, the relationship between sensory stimuli and the temporal structure of LFPs has been the subject of extensive investigations (e.g., [8]?[10], [14], [45]?[47]). Since LFPs reflect integrative processes in areas such as the dendrite which are otherwise inaccessible, characterizing how LFPs encode sensory stimuli is crucial to understand how the microcircuitry of brain networks participates in sensation and shapes the magnitude and timing of local activity. Characterizing how LFPs encode information is also important to understand how neural signals can optimally communicate with brain-machine interfaces [48], and to better interpret the blood oxygenation level-dependent response, which correlates with several LFP bands [22],[49],[50]. Neurophysiological investigations have revealed that a broad range of LFP frequencies is involved in sensory processing, and that the dependence of LFPs on stimuli is complex. However, this complex dependence between the type of sensory stimuli and LFP frequency responses and its potential function has remained so far unexplained.<br>
Here, we developed a theoretical framework for the understanding of the role of LFPs in sensory coding by studying the behavior of model networks of sparsely connected excitatory and inhibitory neurons that were stimulated dynamically. The interplay of excitation and inhibition captured by these networks is one fundamental feature of the organization of cortical microcircuit which is believed to shape the dynamics of local mass activation. Moreover, these networks intrinsically generate gamma-range oscillations, the most widely reported rhythm generated by sensory cortex. Building on the previous theoretical knowledge of how these networks generate oscillations when stimulated with time independent stimuli, we were able to provide several advances. First, we were able to quantify the information content of the fluctuations generated by the network and determine which LFP frequencies convey most information. Second, we found explicit coding rules between features of the stimulus dynamics and LFP frequency which are compatible with several neurophysiological reports. Third, we demonstrated that these coding rules lead to low and high LFP frequencies acting as largely independent information channels, in agreement with recent experimental data [14]. The significance of these findings and their relation to previous work will be discussed in detail next.<br>
Advances with Respect to Previous Modeling Work<br>
Modeling the mechanisms of generation of oscillations in excitatory and inhibitory networks of spiking neurons is one of the most extensively studied topics in neural network dynamics. In this work, we reported several advances to the understanding of dynamics of recurrent networks. First, most previous model studies focused on the network dynamics under constant stimulation. We generalized these results to characterize the network dynamics to slowly-varying periodic and naturalistic stimuli. Second, rather than focusing only on the spectral structure of the network oscillations, we went a step further and quantified the information content of each band of the LFP spectrum in a way directly comparable to experimental findings. Combining a wide set of stimulations with the information theoretic analysis allowed us to derive simple and novel translation rules between stimuli and LFP responses.<br>
Another advance in recurrent network modeling is that previous studies quantified the network output only as the total firing rate, whereas we quantified its output also in terms of simulated LFPs. This greatly facilitates the comparison with experimental recordings, permits a better validation of the models, and provides a mean to test explicitly some hypotheses on what LFPs reflect and how best to capture their properties with a simple model, which is itself an open question. We found that simulated LFPs based on sum of synaptic currents account for some of the main findings in stimulus encoding of LFPs: the modulation of the LFP gamma band when using stimuli eliciting firing rate modulations [3], [6]?[9], the entrainment of low frequency LFPs to stimulus oscillations [11],[16] and the way the two phenomena contribute to the information content of the whole LFP spectrum [14]. Therefore, despite LFPs potentially reflecting complex slow activity unrelated to synaptic activation such as voltage-dependent membrane oscillations [30] or spike afterpotentials [31], our study suggests that many coding properties of LFPs can be understood with simple models based on massed synaptic activation.<br>
Previous modelling studies have computed local field potentials from detailed 3D models of networks of compartmental model neurons [32],[51],[52]. It would be interesting to investigate in such models how well the very simple LFP model introduced in the present paper correlates with the LFP model based on the detailed geometry of the underlying network. In particular, such a study could shed light on which combination of average AMPA/GABA currents best represents the ?true? LFP.<br>
<br>
Dependence of LFP Frequency on Stimulus<br>
The main result of this study is the derivation of very simple rules of transformation between stimulus characteristics and the dynamics of the evoked LFP responses. Though very simple, these rules account for a large number of experimental observations. The first coding rule is that gamma-range LFPs carry information about sensory stimuli that provokes responses of neurons providing synaptic inputs to the specified area that vary from stimulus to stimulus only in terms of their total spike rate. This rule is in full agreement with the observation that stimuli of different contrast are encoded in V1 as gamma-range changes of LFPs [8], that direction of motion is encoded in area MT in the gamma-range LFPs [9], that orientation of gratings is encoded in V1 by gamma-range LFPs [7], and that sound frequencies are best encoded in the high frequencies of auditory field potentials [3], as all such stimuli elicit mostly changes of firing rate (rather than changes in the temporal response profile) in neurons providing synaptic inputs to the specified areas. As we discussed in Results, the simulations demonstrating this rule also correctly predict that the peak of maximal gamma power happens at a lower frequency than the peak of stimulus selectivity in the gamma range [7],[14]. The second coding rule is that stimulus-related changes of low-frequency cortical fluctuations encode information about slow dynamic features in the sensory or thalamic input that vary at the considered frequency. This is fully consistent with the finding that the low frequencies of LFPs can be entrained by slow periodic stimuli [11],[16], and that LFPs in V1 lock to some slowly varying dynamic features extracted from natural movies [14]. The double peak of information at low frequencies (&lt;10 Hz) and in the gamma range (60?90 Hz) found in response to natural movies [14] can also be explained by this coding rule, since a natural movie contains both temporal frequency changes at low frequencies and objects and features capable of eliciting firing rate changes.<br>
Our model was able to reproduce the most salient coding properties of LFPs in early visual cortex based on the hypothesis that the power modulations of LFPs at low frequencies followed temporal patterns emerging in the stimulus itself rather than being generated ex novo within the brain. This hypothesis stems from the observations that low frequency LFPs lock to slow rhythmic stimuli, and from the observations of [14] that the most informative component of the LFP power during movie stimulation was the stimulus modulation of the additional amount of power evoked during movie presentation with respect to spontaneous power. However, the brain is capable of internally generating rhythms in the low (?10 Hz) frequency range through several cellular and network mechanisms [53] not implemented in our model. It is therefore conceivable that in many circumstances internal sources of slow rhythm generation are modulated by the external stimuli. In such cases, we would expect that additional stimulus-related information reflecting these internal processes may become available in the low frequency LFP modulations.<br>
<br>
Correlation between Stimulus Selectivity of Different Frequency Bands<br>
It was recently reported that, during stimulation with naturalistic movies, low frequency LFPs and gamma-range LFPs in visual cortex are decoupled and act as independent information channels [14]. Our model was able to reproduce this finding and to provide an explanation. The independence between low frequency and gamma LFPs arises because they reflect two different input features (the slow frequency variation of the input rate and the total input spike count respectively) and these two input features appears to be largely independent when computed from LGN responses to natural movies (as demonstrated here).<br>
One potential advantage of this frequency decomposition into independent transmission channels is that it may enable the cortical network to transmit more information by multiplexing it over several nested timescales [54]. Since LFPs reflect largely synaptic activity which may be partly decoupled from spiking activity, it is not guaranteed that all the information encoded in LFP oscillations may be used by other neural systems. The extent to which this information gain could be realized depends on how and whether the information carried by LFP oscillations can be read out by downstream systems. It seems plausible that the amount of gamma oscillations could be effectively read out by a downstream decoder, because gamma oscillations are often found to carry information redundant to that of spiking activity [14] and because gamma oscillations modulate transmission of signal across neural populations [55],[56]. Single cells in downstream networks could have intrinsic resonances at gamma frequencies allowing them to preferentially respond to such inputs [57]. Low frequency oscillations could be potentially read out as well, because these oscillations have greater spatial coherence and can thus be made more widely available to decoding networks. The phase or power of these oscillations may therefore be known to local target populations and it could be used to increase the information content of spikes by means of phase-of-firing or power-of-firing codes [15]. Whatever the extent to which this information may be used within cortex, we note that the independence of information carried by low and high frequency LFPs is potentially relevant to the practical development of brain machine interfaces, as it suggests that simultaneous decoding of different LFP bands may permit to obtain information which cannot be obtained by considering one frequency band only.<br>
Our model did not only reproduce correctly the independence between low frequency LFPs and gamma LFPs, but reproduced well both signal and noise correlations over a wide range of LFPs frequencies. Notably, the only significant discrepancy between signal and noise correlation in real data [14] and in the present model, was that the model reported little or none noise correlations across all frequencies, whereas the real data presented strong noise correlation in the 12?24 Hz frequency range. These strong noise correlations were present also during spontaneous activity and were accompanied by little stimulus selectivity and little signal correlations during movie stimulation: Belitski and coworkers [14] hypothesized that the 12?24 Hz LFP frequency region related mainly to stimulus-independent neuromodulation. Since our model did not include any form of variation of neuromodulation across trials and independent from the stimulus, the fact that we could not find such noise correlations in the simulated data is compatible with their hypothesis that this phenomenon reflects the action of one or more neuromodulation pathway not specifically activated by the type of visual stimulus.<br>
<br>
Functional Characterization of LFP Bands<br>
The analysis of EEGs and LFPs traditionally divides these measurements into a number of frequency bands, which correlate with distinct behavioral states and are thought to originate from different types of neural events triggered by different processing pathways such as sensory pathways or neuromodulation. However, the literature reports widely different, and often somehow arbitrary assumptions about which frequency range to investigate and how to set the boundaries of each band. A potential solution to this ambiguity is to set the boundaries ?functionally? [7], so as to extract as much information as possible about the stimuli. The coding rules obtained here suggest that, if this information theoretically optimal band partitioning is implemented, the optimal gamma range partitioning would remain roughly stable with stimuli and consistent with the one proposed in [7] because gamma range coding happens robustly whenever the network receives input rate modulations. On the other hand, partitioning the low frequency range into maximally informative bands may provide frequency boundaries that are dependent on the stimulus dynamics and not an intrinsic property of the network.<br>
<br>
Experimental Predictions Arising from the Model<br>
An hypothesis of our model is that the stimulus-related changes in the power of low frequency LFPs follow at least in part the dynamics of the stimulus. For example, the high information content of low frequency LFPs found in response to natural movies was attributed to the temporal structure of the image flow, which contains the highest power and information in the low-frequency range. We suggest that a useful experimental paradigm that could help testing the hypotheses and coding rules presented here consists of changing the stimulus dynamics by using faster stimuli than natural movies and studying how this affects the informative LFP-frequency range. If the low frequency band modulations are mostly reproducing the modulations of the input spectrum, the exact position of the low frequency information peak should vary accordingly. Similarly, the interval in the gamma band containing information is predicted to depend on the rate range of the input, that for visual stimuli can be modulated with the image contrast. The study of the dependence of the information peaks on our model predicts also that, in the presence of GABA antagonists reducing but not blocking the inhibitory synaptic transmission, there is an increase of the power in the gamma band and only a very small decrease of the associated information. All of these predictions can be tested with current methodology, and the suggested experiments can help us understanding better the origin and function of the brain dynamics reflected in LFP fluctuations.<br>
<br>
<br>
Methods<br>
Model<br>
The simulated network is composed of N?=?5000 neurons. 80% of the neurons are taken to be excitatory, the remaining 20% are inhibitory [58]. The network is randomly connected: the connection probability between any directed pair of cells is 0.2 [59],[60]. Both pyramidal neurons and interneurons are described by leaky integrate and fire (LIF) dynamics [61]. Each neuron k is described by its membrane potential Vk that evolves according to(1)where ?m is the membrane time constant (20 ms for excitatory neurons, 10 ms for inhibitory neurons, [62]), IAk are the (AMPA-type) excitatory synaptic currents received by neuron k, while IGk are the (GABA-type) inhibitory currents received by neuron k. Note that in Equation 1 we have taken the resting potential to be equal to zero. When the membrane potential crosses the threshold Vthr (18 mV above resting potential) the neuron fires causing the following consequences: i) the neuron potential is reset at a value Vres (11 mV above resting potential), ii) the neuron can not fire again for a refractory time ?rp (2 ms for excitatory neurons, 1 ms for inhibitory neurons).<br>
Synaptic currents are the linear sum of contributions induced by single pre-synaptic spikes, which are described by a difference of exponentials. They can be obtained using auxiliary variables xAk, xGk. AMPA and GABA-type currents of neuron k are described by(2)(3)(4)(5)where tk?pyr/int/ext is the time of the spikes received from pyramidal neurons/interneurons connected to neuron k, or from external inputs (see below). ?dA (?dG) and ?rA (?rG) are respectively the decay and rise time of the AMPA-type (GABA-type) synaptic current. ?L?=?1 ms is the latency of post-synaptic currents. Jk?pyr/int/ext is the efficacy of the connections from pyramidal neurons/interneurons/external inputs on the population of neurons to which k belongs. Most of the external input, i.e. all the signal and the largest part of the noise, is assumed in our model to come from the thalamus. The values of these parameters for all types of synapses are displayed in Tables 2 and 3. These values are of the order of magnitude of experimentally measured values [28], [63]?[66]. Modifying them affects quantitatively, but not qualitatively, our results, provided the network stays in an inhibition-dominated regime, as demonstrated in [26] and in the section Effects of Changing Synaptic Strength on Information Transfer. Changing parameters such as the latency ?L or the synaptic time constants can potentially change both location and shape of the peak in both LFP spectrum and information vs frequency curve. However, changing these parameters in the physiologically relevant range affects only mildly the agreement of the model with the data (see Table 4).<br>
<br>
External Inputs<br>
Each neuron is receiving an external excitatory synaptic input (see previous section, last term in the r.h.s. of Equation 3). These synapses are activated by random Poisson spike trains, with a time-varying rate which is identical for all neurons. This rate is given by(6)where ?signal(t) represents the signal, and n(t) is the noise. [?]. is a threshold-linear function, [x]+?=?x if x&gt;0, [x]+?=?0 otherwise, to avoid negative rates which could arise due to the noise term. Each simulation is repeated 20 times with the same signal and a noise generated independently for each simulation. A single run is called a trial. We now describe signal and noise separately.<br>
<br>
Signal<br>
We use three types of signals: constant; periodic; and ?naturalistic?. All signals last 2 seconds.<br>
The standard high-pass filtering and rectification procedure that we used provides a MUA signal that correlates well with the power of the local spiking activity measured e.g. by detecting spike times of the closest neurons with a threshold crossing criterion [29]. However, the high-pass filtering and the rectification are likely to flatten out differences of the average spike rate between different stimuli with respect to the true underlying spike rate. This is because the rectification reduces the dynamic range and the high pass filtering may preserve some small amount of low frequency noise that end up as spike rate to all stimuli. To compensate for this, we amplified the differences across stimuli of MUA rate by building a signal Qi(t):(9)where Si(t) is the original time series of the stimulus i recorded in the LGN,  is the average value of the stimulus, and  is the average value of the whole recording. This manipulation leaves power of frequencies &gt;0.5 Hz unchanged. To set the value of B, we used the following procedure. We know from simulations with constant inputs that changes in input rate translate into modulations in the output gamma power band of the LFP. We computed then the coefficient of variation (CV) of the gamma power (sum of the power of frequencies in between 30 and 100 Hz) among the different stimuli for the LFP recording from V1. We selected recordings from the same monkey, experiment and movie screening of the MUA recording from LGN we were considering. The resulting CV value, averaged over 7 electrodes, was CV?=?0.26?0.05. Increasing B in Equation 9 leads to an increase in the gamma band LFP CV. We set B?=?2, for which CV?=?0.24, consistent with the data. In Figure 9, we use B?=?1 (corresponding to CV?=?0.18) and B?=?3 (CV?=?3) to investigate the sensitivity of the dynamics to changes in parameters defining external stimuli.<br>
The MUA computed in this way was expressed in mV and needed to be converted into spike rates units to be fed to the network. A series of papers [68]?[70], report similar estimates of ?6 spikes/s for the activity of LGN neurons in the absence of visual stimulations. Anatomical studies estimate that about 130 LGN synapses project to each V1 neuron in the macaque [71],[72]. Multiplying these two numbers we estimated an average baseline of 0.8 spikes/ms reaching V1 from LGN. The same set of papers [68]?[70], shows that during movie stimulations the firing rate of LGN neurons is ?12 spikes/s, so we set the average value of the signal to be 1.6 spikes/ms. The final equation determining the naturalistic signal is then:(10)where k?=?0.8 spikes/ms, and C?=?0.8 spikes/ms.<br>
<br>
Noise<br>
There are two sources of noise in our model. The first is due to the fact that n(t) in Equation 6 is a stochastic variable, generated according to an Ornstein-Uhlenbeck process,(11)where ?n is the standard deviation of the noise, and ?(t) is a Gaussian white noise. The mean value of this process is zero, and its power spectrum is flat up to a cut-off frequency,  and then decays as f?2. The time constant ?n was set to 16 ms to have fc?=?10 Hz, and the standard deviation ?n was set to 0.4 spikes/ms based on CV values for LGN activity [70].<br>
The second source of noise is due to the fact that different neurons receive independent realizations of a Poisson process, with the same time-varying rate n(t).<br>
<br>
Input Parameters<br>
Signal parameters k, C and the noise parameter ?n have been set to be compatible with experimentally inferred values of thalamic activity during the screening of natural movies [68]?[70], B to be compatible with gamma oscillation range in V1 recordings in the same conditions, while we have chosen ?n so that the noise is maximum for frequencies lower than 10 Hz, because spontaneous activity in the visual cortex acts mainly on this timescale [73],[74]. We tested the robustness of our results to changes in these values. As an example, a key result of the present work is the presence of two peaks in the information content of the spectrum, for low frequencies and in the gamma band, when the network is presented with naturalistic stimuli (Figure 7B). Figure 9C shows that modulations of B led to changes in the information content of the gamma band and higher frequencies, but do not affect low frequencies content and the shape of the Information(frequency) function. Figure S3 shows the robustness of our conclusion to variations in the input parameters described in the previous paragraphs.<br>
Table 4 summarizes the effects of changing these parameters on the agreement between model and data, as measured by the reduced ?2. It shows that this agreement is fairly robust to changes in ?n, while both increasing and decreasing ?n and k deteriorates this agreement.<br>
<br>
Numerical Methods<br>
Simulations were done with a Runge-Kutta algorithm with time step ?t. For equations (1?5) ?t?=?0.05 ms. Since the experimental recording frequency is 500 Hz, the input to the network (included the noise) was updated every 2 ms.<br>
<br>
Generation of Simulated Local Field Potentials<br>
LFP is a common measure of neuronal activity, but it is still not completely clear how the LFP is related to single neuron variables like synaptic or ionic currents, and membrane potentials. Computational models sometimes use as a description of the LFP the average membrane potential of the neurons of the network [75], even though it seems definitely more likely that the LFP is rather more directly related to the synaptic activity [29]. The spectrum of the average membrane potential in our model has a faster decay at high frequencies than the measured LFP, and therefore does not reproduce it well (Figure S4). However, the information content of the average membrane potential turns out to be similar to the one of the recorded LFPs (Table 4).<br>
On the opposite side, LFPs have been computed using compartmental neuron models [32],[51]. The model used in [51] adopted the neuronal structure described in [76]: dendritic branches were divided into cilindrical compartments of 50 ?m length. Each compartment contained many synapses, whose characteristics depended on the branch (apical, basal etc). The LFP was computed for every point in the space surrounding the neuron as the total extracellular potential originated by the trasmembrane currents of the hundreds of different compartments. In [32] the procedure was similar but the neuronal structure was reduced to a total of 15 compartments. In both models, LFPs were originated by synaptic currents on pyramidal neurons dendrites.<br>
Here, we resorted to a similar but simpler approach, which takes into account that our model makes no attempt to replicate the spatial organization of cortical neurons, and thus the sum in space of currents has to be abstracted and simplified, as follows. To capture in a simple way the fact that pyramidal cells contribute the most to LFP generation because their apical dendrites are arranged in an approximate open field configuration, we assumed that the LFP is generated by the dipole-like dendrites of pyramidal cells, in which currents flow in the cell through apical excitatory synaptic contacts while they flow out through basal inhibitory contacts [77]. This suggests to model LFPs as the sum of the absolute values of AMPA and GABA currents (|IA|+|IG|) on pyramidal cells, which was the model we adopted in this work, and that was able to reproduce correctly both the power spectrum of recorded LFPs and its information content (Table 4, Figure 7B, and Figure S4). Taking the LFP to be a different linear combination of AMPA and GABA currents give rise to qualitatively similar results (Table 4 and Figure S4). LFP signals are high-passed at 1 Hz with a 4th order Butterworth filter to reproduce experimental recording procedures of [14].<br>
<br>
Spectral Analysis<br>
The power spectrum in each trial obtained in response to each simulated stimulus was obtained using the multitaper technique [78], which provides an efficient way to simultaneously control the bias and variance of spectral estimation by using multiple Slepian data tapers and was the one mostly used in recent neurophysiological studies of LFPs [8],[14]. The use of Slepian functions minimizes the bias, whereas the use of multiple orthogonal tapers on the same data minimizes the variance. The Slepian functions are defined in terms of their length L in time and their bandwidth W in frequency. For each choice of L and W, up to K?=?2LW?1 tapers are highly concentrated in frequency, having 90% of their power within the interval [?W, W], and can be averaged for spectral estimation. To reduce the spectral bias, the average over tapers was computed using the adaptive procedure described by [78]. A simplified way of conceptualizing the multitaper method is that it provides an average over the local frequency ensemble with a range 2W [78]. The value of W should be chosen on the basis of empirical considerations. Here, we chose LW?=?2 because it matches the one used by [14] and thus makes the comparison between simualtion and experiments more transparent.<br>
For the sake of the entrainment analysis only (Figure 6D and Figure S1), both the LFP and the naturalistic stimuli were band-passed at selected frequencies with a Kaiser window with a 2 Hz bandwidth, very small passband ripple (0.01 dB), and high stopband attenuation (60 dB) [14]. Forward and backward filtering was used to eliminate phase shifts introduced by the filter. For each band-passed signal the phase was extracted by the means of the Hilbert transform. The phase of each band-passed LFP was compared with the input signal phase, for periodic signals, or with the phase of the band-passed naturalistic signal. We computed then the circular variance [79] of the input-output phase difference ??(t) as CircVar?=?1?|?expi??(t)?t|. Circular variance ranges from 0 (perfectly locked phases) to 1 (random phase differences uniformly spread over the circle).<br>
<br>
Measures of Information Carried by the Neural Response Power<br>
To determine how well the power of LFPs rf at a certain frequency f encodes the stimuli, we computed the mutual information I(S, Rf) between the power rf at frequency f and the stimuli S [80], as follows:(12)where P(s) is the probability of the presentation of the stimulus s, P(rf) the probability of the frequency f to have power rf over all trials and all stimuli, P(rf|s) the probability of rf to be observed when stimulus s is presented. The above single-frequency information analysis can be extended to compute how much information about the stimulus we can obtain when combining together the power rf1 and rf2 at two different frequencies. The mutual information that the joint knowledge of the powers rf1 and rf2 conveys about the stimulus is as follows:(13)If two frequencies were tuned to completely different stimulus features, and they did not share any source of noise, then we would expect I(S; Rf1Rf2) to be equal to the sum of the information that each frequency conveys separately. It is therefore useful to introduce the following ?information redundancy? [41],[81],[82]:(14)When redundancy is positive, the two frequencies are said to convey redundant information about the stimulus; when redundancy is zero, the two frequencies are said to convey independent information.<br>
Estimates of mutual information often suffer from the limited amount of data available to calculate the conditional probabilities and the resulting statistical errors. These errors translate into a bias in the information estimate. To correct for this bias, we implemented a multi-step procedure, which follows the ideas presented in [15],[83] and was used, described and tested in our previous studies [14],[15]. In brief, the bias estimation of the information contained in the multidimensional responses was greatly reduced at the very source, and made negative, by using the ?shuffling? technique described in [83]. Then, a well-established quadratic extrapolation procedure [84] was used to further reduce the bias. We finally evaluated and subtracted out any (small in this dataset) residual bias by the ?bootstrap? procedure fully reported in [15]. This procedure provides information estimates which are very accurate. The performance of these procedures on simulated data has been reported previously [15],[83]. In particular, when tested on data with statistics similar to the one considered here and with number of trials similar to the one available here, the resulting information estimates are very tight and present a very small residual error in the estimate of the bias.<br>
<br>
Quantification of Signal and Noise Correlation<br>
To determine which frequencies have related stimulus selectivity and which have shared sources of variability, we performed a linear analysis of correlations across frequencies of both the signal and the noise, as follows. The correlation of the mean responses across different stimuli of two frequencies are called ?signal correlations? [40],[41],[85] because they are entirely attributable to stimulus selectivity. The signal correlation coefficient was computed, for each frequency pair and channel, as the Pearson correlation across stimuli of the trial-averaged responses. Positive values indicate that the two frequencies have similar stimulus preference, whereas a zero values indicates that the two frequencies prefer totally uncorrelated stimuli. Correlations manifested as covariations of the trial-by-trial fluctuation around the mean response to the stimulus are traditionally called ?noise correlations? [40],[85]. Since these noise covariations are measured at fixed stimulus, they ignore all effects attributable to shared stimulation. To quantify the strength of noise correlations, we computed the Pearson correlation coefficient (across trials at fixed stimulus) of the trial-average-subtracted powers rf1 and rf2, and then we averaged it over all stimulus windows. This quantifies the correlations of the variations around the mean at each trial and stimulus window. Positive values of noise correlation means that when the power of one frequency fluctuates over its mean values, the power in the other frequency is also more likely to do so.<br>
<br>
Goodness of Fit Measurements<br>
For every set of parameters shown in Tables 1 and 4, the information content of the spectrum of the simulated LFP was compared with the information contained in the spectrum of the real LFPs recorded with seven electrodes in V1 synchronously to the LGN recording used to construct the naturalistic signals. I(S, Rf) is the information about the set of stimuli contained in the power of the frequency f of the simulated LFP. We computed the information associated to the power of each frequency also for the LFP recorded in every electrode and then we computed its mean (IV1(f)) and its standard deviation (?V1(f)) across the electrodes. The measure used to quantify the agreement between the model and the data was the reduced chi squared :(15)using a total number of frequencies F equal to 400 (from 0.5 to 200 Hz in a 0.5 Hz step). A value of  close to 1 suggest that the model is as different from the data as the data are different among different electrodes. The same procedure was then applied to compare the model LFP spectrum and the recorded LFPs spectra, both averaged over all scenes and trials.<br>
Other methods, such as Dynamic Expectation Maximization [86],[87] or Kalman Filtering [88] could be used to obtain a more principled measure of correspondence between model and data, the best fit parameters and the parameter confidence. However, these more sophisticated procedures were in practice not applicable to our simulations because of the high dimensional parameter space and because of the long time taken to run the analysis (6 hours per parameter setting on our workstation). For this reason, we resorted to fix most parameters from plausible literature values, and then tune them by hand to obtain a good fit as measured by . The robustness to parameter variations was empirically determined by starting from the so-determined optimal parameters and checking for biologically plausible values the reduced  of both information and power spectrum.<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2714467</b><br>
A Parsimony Approach to Biological Pathway Reconstruction/Inference for Genomes and Metagenomes<br>
A common biological pathway reconstruction approach?as implemented by many automatic biological pathway services (such as the <software>KAAS</software> and <software>RAST</software> servers) and the functional annotation of metagenomic sequences?starts with the identification of protein functions or families (e.g., KO families for the <database>KEGG</database> database and the FIG families for the <database>SEED</database> database) in the query sequences, followed by a direct mapping of the identified protein families onto pathways. Given a predicted patchwork of individual biochemical steps, some metric must be applied in deciding what pathways actually exist in the genome or metagenome represented by the sequences. Commonly, and straightforwardly, a complete biological pathway can be identified in a dataset if at least one of the steps associated with the pathway is found. We report, however, that this na?ve mapping approach leads to an inflated estimate of biological pathways, and thus overestimates the functional diversity of the sample from which the DNA sequences are derived. We developed a parsimony approach, called <software>MinPath</software> (<software>Minimal set of Pathways</software>), for biological pathway reconstructions using protein family predictions, which yields a more conservative, yet more faithful, estimation of the biological pathways for a query dataset. <software>MinPath</software> identified far fewer pathways for the genomes collected in the <database>KEGG</database> database?as compared to the na?ve mapping approach?eliminating some obviously spurious pathway annotations. Results from applying <software>MinPath</software> to several metagenomes indicate that the common methods used for metagenome annotation may significantly overestimate the biological pathways encoded by microbial communities.<br>
<br>
Introduction<br>
Microbial whole genome sequencing has become a routine practice in recent years, because of the rapid advances of DNA sequencing technologies [1]. One of the first analyses that biologists attempt, once they obtain a complete genome sequence, is to reconstruct the biological pathways encoded by the organism, which is usually accomplished in silico by mapping the protein coding genes onto reference pathway collections, such as <database>KEGG</database> [2] or <database>SEED</database> [3], based on their homology to reference genes with previously characterized functions. For example, <software>KAAS</software>, the pathway annotation system based on the <database>KEGG</database> database [4], first annotates K numbers (each K number represents an ortholog group of genes, and is directly linked to an object (a biochemical step) in the <database>KEGG</database> pathway map), and then reconstructs pathways based on the assigned K numbers. Similarly, the <software>RAST</software> server (and <software>MG-RAST</software>) first annotates FIG families and then maps the identified FIG families onto the <database>SEED</database> subsystems [5],[6]. These automatic methods are promising for the analysis of most genomes, although they may leave ?holes? in the reconstructed pathways, due to either missing genes (i.e. the genes are non-homologous to reference genes of the same specific functions, and thus cannot be identified by a homology-based method, or were simply not annotated as ORFs by annotation pipelines) [7], or alternative and novel pathways (i.e. the target organism adopts variant pathways, which are different from the reference pathway, to accommodate a specific niche or lifestyle) [8]. After all, many bacterial genomes have fewer than 60% of their genes assigned to a proposed function [9],[10].<br>
We note that pathway reconstruction is essential for understanding the biological functions that a newly sequenced genome encodes. For instance, in a recently published report, the coupling of N2 fixation to cellulolysis was revealed within protist cells in the termite gut, based solely on the in silico pathway reconstruction of the complete genome sequence of a bacterial endosymbiont [11].<br>
Moreover, pathway reconstruction based on some new high throughput techniques must provide conclusions from explicitly incomplete information, which poses fresh challenges. For example, in a typical proteomics experiment, the proteins represent a particular biological sample collected under a specific physiological condition or from a specific tissue (e.g. from yeast cells after the heat shock), which are in high enough abundance to be identified by tandem mass spectrometry [12],[13]. Based on these data, one may ask, what biological pathways were activated (or suppressed) under the physiological condition? A similar, but more complicated case is pathway analysis of metagenomic data, to characterize the aggregate metabolic processes of microbial communities in a given environment [14]. Metagenomic profiling data can be viewed as a sampling of the genomic sequences from many kinds of microbes living in a specific environment. Again, the incompleteness of the data makes it difficult to reconstruct the entire pathways encoded by a metagenome. Nevertheless, it is becoming routine to ?reconstruct? pathways for proteomic [15] and metagenomic data [16],[17], by best similarity matches (often derived from <software>BLAST</software> searches): a pathway is inferred to be absent or present in a dataset if highly confident homolog protein hits identify one or more of the protein functions associated with the pathway in other organisms.<br>
In addition to the problems that arise from incomplete data, existing methods of pathway reconstruction or inference may over-estimate the number of pathways because of redundancy in the protein-pathway, at four levels. First, different pathways may share the same biological functions. The partition of pathways (as the entire cellular network is partitioned into several hundreds of biological pathway entities in <database>KEGG</database> database) is extremely important for understanding of biological processes, even though there is only a single large biological network within any cell and all pathways are to some extent connected [18]. It is not surprising that many pathways defined in the pathway databases are overlapping. Second, some proteins carry out multiple biological functions [19], e.g. through different protein domains, active sites, or substrate specificities. Third, neither organisms nor communities are closed boxes, and the products or intermediates of pathways may be exogenously supplied. Finally, homology-based protein searching may map one protein to multiple homologous proteins with different biological functions (i.e. paralogous proteins). In summary, it cannot be safely concluded that a pathway is present, even if one or more proteins are mapped to it.<br>
Even for single complete genomes, pathway reconstruction does not always give a clear picture of the biological functions in an organism, and human curation and experimental verification is often needed [20],[21]. We illustrate this by a rather extreme example found in the pathway analysis of the human genome. The <database>KEGG</database> pathway annotation of the human genome includes the reductive carboxylate cycle, with proteins annotated to 6 steps in this pathway (http://www.genome.jp/kegg-bin/show_organism?menu_type=pathway_maps&amp;org=hsa) (as of July 2nd, 2009). The Calvin cycle is the most common method of carbon fixation, while the reductive carboxylate cycle is an alternative carbon fixation pathway, currently found only in certain autotrophic microorganisms. In fact, the reductive carboxylate cycle is essentially the reverse of the Krebs cycle (citric acid or tricarboxylic acid cycle), the final common pathway in aerobic metabolism for the oxidation of carbohydrates, fatty acids and amino acids, so they share reactions and functional roles. For this reason, the proteins responsible for the normal function of the Krebs cycle can be mistakenly taken as evidence for the existence of a reductive carboxylate cycle in the human genome.<br>
Here we propose a pathway reconstruction/inference method in which we do not attempt to reconstruct entire pathways from a given set of protein sequences (e.g. identified in a proteomics experiment, or encoded by the sequences sampled in a metagenomic project), but to determine the minimal set of biological pathways that must exist in the biological system to explain the input protein sequences sampled from it. In this context, we note pathway inference might be a more suitable terminology than pathway reconstruction. However, considering that pathway inference has been used in a different context to infer networks or pathways from gene express data [22], and pathway reconstruction is commonly used in the field, we use both pathway inference and pathway reconstruction in this paper. To address the issues of both incomplete data, and pathway redundancy, we formulate a parsimony version of the pathway reconstruction/inference problem, called <software>MinPath</software> (<software>Minimal set of Pathways</software>), which can be roughly described as the following: given a set of reference pathways and a set of proteins (and their predicted functions) that can be mapped to one or more pathways, we attempt to find the minimum number of pathways that can explain all proteins (functions) (see Fig. 1). Although this problem is NP-hard in general, we provide an integer programming (IP) framework to solve it.<br>
We focus on analyzing complete genomes in this study because there is a relatively good understanding of the pathways that actually exist in organisms with completely sequenced genomes (as compared to the emerging metagenomes), making this analysis a good test of our method. Besides, the pathway annotations of these genomes are still far from perfection, as in the example of a carbon fixation pathway in the human genome (as well as chickens, mosquitoes, etc). We also applied <software>MinPath</software> to the analyses of several metagenomic datasets, to demonstrate the potential applications of <software>MinPath</software> in metagenome annotation.<br>
<br>
Results<br>
We first revisited the pathway reconstruction of individual genomes using <software>MinPath</software>. The results show that <software>MinPath</software> gave a conservative, but reliable estimation of the pathways of a genome, and therefore the functional ability/diversity encoded by a genome. In addition, <software>MinPath</software> found suspicious pathways in the <database>KEGG</database> database. We then applied <software>MinPath</software> to a set of metagenomic datasets, and the results indicate that the current estimation of functional diversity/ability of studied microbial communities might be overestimated.<br>
Pathway Reconstruction for Genomes<br>
Overview of the performance of <software>MinPath</software><br>
The function annotations of the genes from individual genomes were extracted from the <database>KEGG</database> database, and used as input for <software>MinPath</software> to reconstruct the pathways encoded by each genome. A total of 854 genomes were studied, and the overall performance of <software>MinPath</software> is shown in Fig. 2, compared with the curated <database>KEGG</database> pathways and the pathway reconstructions produced by the na?ve mapping approach (see METHODS for details). The comparison shows that <software>MinPath</software> gives an estimation of functional diversity (measured by the number of pathways constructed) that is closer to the curated <database>KEGG</database> database, as compared to simple pathway construction based on the appearance of families. <software>MinPath</software> gives a more conservative estimation of the pathways than even <database>KEGG</database> in most genomes (with fewer annotated biological pathways), but we would like to argue that even some of the pathways collected in <database>KEGG</database> should be removed (such as the ascorbate and aldarate metabolism pathway in human, as we discuss below).<br>
<br>
The human genome<br>
For the human genome, there are 205 predicted <database>KEGG</database> pathways (as of December 2008), while the na?ve mapping approach identifies 227 pathways. <software>MinPath</software> identified only 191 pathways?these pathways are necessary and sufficient to explain all the annotated human proteins in the <database>KEGG</database> database.<br>
Many of the pathways that are identified by the na?ve mapping approach are spurious and are not curated in the <database>KEGG</database> database (e.g. the penicillin and cephalosporin biosynthesis pathway, and the two-component general and the type II secretion systems), indicating that <software>MinPath</software> can be applied to remove pathways that are otherwise mistakenly annotated using the na?ve mapping approach. More examples are listed in the supplementary website.<br>
Some of the pathways that are curated in the <database>KEGG</database> database are marked by <software>MinPath</software> as spurious (see Table 1). For example, the ascorbate and aldarate metabolism pathway (Fig. 3) is annotated in <database>KEGG</database> as a biological pathway in human, but not by <software>MinPath</software>. In humans there are only three functions (out of 24) annotated for this pathway and these three functions are not unique to the pathway: EC 1.2.1.3 (aldehyde dehydrogenase 2 family) is involved in 15 other pathways, EC 1.1.1.22 (UDP-glucose dehydrogenase) is involved in three other pathways, and myo-inositol oxygenase (EC:1.13.99.1) is involved in both this pathway and the inositol phosphate metabolism pathway. Based on the sparseness of the genes assigned to this pathway and their ubiquitous nature, and the fact that humans require vitamin C in the diet, we believe that the ascorbate and aldarate metabolism pathway should be removed from the pathways reconstructed for the human genome.<br>
<br>
The Escherichia coli genome<br>
<software>MinPath</software> identified 98 biological pathways that are sufficient to explain all the identified functions encoded by the E.coli genome. It is a conservative estimation, as compared to the 125 pathways for Escherichia coli K-12 MG1655 collected in the <database>KEGG</database> database, and 158 pathways that have at least one or more associated functions identified in the genome. Refer to the supplementary webpage for the details of the pathway reconstructions and their comparison for E.coli. It is obvious that the na?ve mapping approach leads to an inflated estimate of biological pathways in E.coli?the list even includes several biological pathways involved with human cancer, including renal cell carcinoma (pathway ID 05211), prostate cancer (pathway ID 05215), and bladder cancer (pathway ID 05219). These pathways were wrongly annotated because one or more predicted functions in these pathways are also involved in other pathways. For example, fumarate hydratase (ko:K01679) is involved in the renal cell carcinoma pathway, as well as in the citrate cycle, a fundamental pathway present in most bacteria. Based on the identification of this enzyme alone, the na?ve mapping approach predicted the presence of the renal cell carcinoma pathway in E. coli, which obviously cannot be true. <software>MinPath</software> removed these spurious pathways from the list of constructed pathways, without human curation.<br>
We argue that <database>KEGG</database> predictions also overestimate the biological pathway encoded by E. coli genome, e.g. the mitochondrial fatty acid elongation pathway and the bile acid biosynthesis pathway (see Table 2).<br>
<br>
<br>
Pathway Reconstruction for Metagenomes<br>
We used <software>MinPath</software> to re-analyze the biological pathways of several metagenomes [17], which were previously analyzed by a na?ve mapping approach. The results are summarized in Table 3. We used both the <database>KEGG</database> and <database>SEED</database> databases in this experiment. For <database>KEGG</database> pathways, we did local <software>BLAST</software> searches, using the criteria as shown in [16] for KO family identification. For <database>SEED</database> subsystems, the FIG annotations were downloaded from the <software>MG-RAST</software> server (http://metagenomics.theseed.org/).<br>
For all the datasets we tested, <software>MinPath</software> reduced the total number of annotated pathways (or subsystems) significantly (as shown in Table 3). For example, for the metagenome sampled from a coral microbial community (Coral-Mic), there are in total 232 <database>KEGG</database> biological pathways annotated in at least one of the 7 sequencing datasets. Based on <software>MinPath</software>, however, only 160 <database>KEGG</database> biological pathways are sufficient to explain all the functions predicted for these datasets. These results indicate that the na?ve mapping of the biological pathways from predicted functions may overestimate the biological pathways (so the functional diversity) of those microbial communities, and we need to be cautious when interpreting the results from such an analysis [16],[17].<br>
We also show the details of pathway reconstruction for a single sequence dataset from the coral biome (4440319.3.dna.fa). The na?ve mapping approach identified 224 <database>KEGG</database> pathways, whereas <software>MinPath</software> identified only 143 <database>KEGG</database> pathways. The pathways eliminated by <software>MinPath</software> include the inositol metabolism pathway, the androgen and estrogen metabolism pathway, the caffeine metabolism pathway, etc (see more examples at the supplementary website). Obviously, comparisons of microbial communities or other biomes will be more telling if spurious pathways are eliminated, and our results suggest that as many as 40% of the 224 pathways could be wrong.<br>
<br>
<br>
Discussion<br>
We have developed the <software>MinPath</software> approach to provide more conservative?but more reliable?estimations of biological pathways from a sequence dataset, and applied this approach to revisit the biological pathway reconstruction problem for genomes as well as metagenomes. Our results show that without further post-processing of the reconstructed pathways, the na?ve mapping strategy may overestimate the biological pathways that are encoded by a genome or metagenome, which could jeopardize any conclusions drawn from the constructed biological pathways (such as the metabolic diversity/capacity of an environmental microbial or viral community, as measured by the Shannon Index) [16],[17], or other downstream analysis based on constructed pathways [23]. It was noted in [16] that most of the microbial communities in that study were approaching saturation for known pathways: more conservative estimates of pathways for each environment may allow real functional differences between the samples to be detected.<br>
Note that <software>MinPath</software> is not designed to directly improve the still imperfect definition of pathways and/or functions in databases such as <database>KEGG</database> or <database>SEED</database>. For example, as a result of how some pathways are grouped in the <database>KEGG</database> database, peptidoglycan biosynthesis is listed for the human genome by <database>KEGG</database> annotation and <software>MinPath</software> does not eliminate this pathway from the list of annotated pathways from human genome. In this sense, efforts are still needed to improve the elucidation and annotation of extent biochemical pathways. But given a database of reference pathways, we feel that <software>MinPath</software> provides a sensible method for inferring the pathways represented in biological sequence samples.<br>
<br>
Materials and Methods<br>
First we will briefly describe the na?ve mapping approach that is commonly used in current automatic biological pathway reconstruction services (e.g., the <software>KAAS</software> and <software>RAST</software> servers), as well as for pathway reconstruction for metagenomic sequences. Then we present a novel minimal pathway reconstruction approach based on a simple yet efficient algorithm for solving this problem.<br>
The Na?ve Mapping Approach to Pathway Reconstruction<br>
Pathway reconstruction has become routine in functional annotation of genomes and metagenomes, in which <database>KEGG</database> pathways (or other biological pathways such as <database>SEED</database> subsystems) are reconstructed based on homology. <database>KEGG</database> and <database>SEED</database> databases collect pathways (or subsystems) curated by experts, each pathway/subsystem consisting of a series of functional roles (enzymes, transporters, etc). Pathway reconstruction consists of two key steps: (1) predicting the functions (represented by protein families) of proteins encoded by the DNA sequences, which is often achieved by similarity searching of the predicted proteins against reference proteins from previously characterized genomes; and (2) predicting the presence or absence of pathways in the query dataset, based on the identified functions associated to the pathways. Conventional pathway reconstruction usually adopts simple criterion in this second step (herein referred to as the na?ve mapping approach), i.e., a pathway is considered to be present if one or more functions in the pathway are identified in the first step. We have shown in this paper that this approach may lead to the identification of spurious pathways and an overestimation of functional ability, which motivated us to develop a novel approach to pathway reconstruction based on the parsimony principle presented below.<br>
<br>
Minimal Pathway Reconstruction Problem<br>
We define the minimal pathway reconstruction problem as the following: given a list of functions annotated for a set of genes (which can be an incomplete set, as we encounter in metagenomic analysis, or a nearly complete set, as in complete genome analysis), find the minimal set of pathways that include all given functions (see Fig 1). Note that this formulation is different from the conventional formulation of the pathway reconstruction problem, which attempts either to reconstruct the complete pathways encoded by a given genomic dataset (in a sense, the pathway holes should to be minimized), or to identify the set of pathways that have at least one associated function annotated (i.e., the na?ve mapping approach).<br>
<br>
Integer Programming Algorithm<br>
We use integer programming to solve the minimal pathway reconstruction problem. Linear programming (LP) is an algorithm for finding the maximum or minimum of a linear function of variables (objective function) that are subject to linear constraints [24]. Simplex and interior point methods are widely used for solving LP problems. The related problem of integer programming (IP) requires some or all of the variables to take integer (whole number) values. Some of the most powerful algorithms for finding exact solutions of combinatorial optimization problems [25] are based on IP. LP and IP have been applied to many fields in the biological sciences, such as the maximum contact map overlap problem for protein structure comparison [26], optimal protein threading [27], probe design for microarray experiments [28], and the pathway variant problem [8].<br>
Here we transform the minimal pathway reconstruction problem to an integer programming problem: Denote the number of functions (protein families) that are annotated in a dataset as n. Let the total number of putative pathways which have at least one component function annotated be p. Denote the mapping of protein functions to the pathways as M, where Mij?=?1 if function i is involved in pathway j, otherwise 0 (note one function may map to multiple pathways or subsystems). Denote if a pathway j is selected in the final list or not as Pj, with Pj?=?1 if selected, Pj?=?0 otherwise. The set of pathways with Pi?=?1 composes the minimal set of pathways that can explain all the functions that are annotated for a dataset.<br>
The objective function for integer programming is,i.e., our goal is to find the minimum number of pathways that can explain all the functions carried by at least one protein from a dataset.<br>
<br>
Protein Function and Function Annotation<br>
We use the KO and FIG protein families defined in the <database>KEGG</database> database and the <database>SEED</database> subsystems, respectively, for this study. Many of the mappings of KO families to <database>KEGG</database> pathways were done manually in the <database>KEGG</database> database. These families are the basic units for pathway reconstruction (or subsystem reconstruction in <database>SEED</database>), in which a pathway (or a subsystem) is composed of a list of functional roles.<br>
<br>
Implementation Details<br>
We use the <package>GLPK</package> package (GNU Linear Programming Kit; http://www.gnu.org/software/glpk/glpk.html) for solving the integer-programming problem; all the other functions are implemented in Python.<br>
The input for <software>MinPath</software> is a list of protein families (e.g., KO and FIG families) annotated in a given dataset of genes (from a genome, or a metagenome), and the output is the list of pathways reconstructed/inferred for the dataset.<br>
Note that in some cases two pathways may share most of their functional roles (for example, the biosynthesis and degradation pathway of the same biological molecule, such as the lysine biosynthesis and degradation pathways). <software>MinPath</software> will keep one of these pathways, because that is sufficient to explain the functional roles identified. We added a post-processing step here to add those pathways that have more than 50% of their functional roles identified back to the pathway pool, even when these functional roles appear in another pathway that is already predicted by <software>MinPath</software>.<br>
<br>
Benchmarking Experiments<br>
We revisited the pathway reconstruction for the 854 genomes in the <database>KEGG</database> database (as of December, 2008) that have at least 20 <database>KEGG</database> pathways annotated for each of these genomes. For these genomes, the function (or protein families) annotations were downloaded from the <database>KEGG</database> database (ftp://ftp.genome.jp/pub/kegg/release/current/).<br>
We also applied <software>MinPath</software> to reanalyze the pathways for nine biome metagenomic datasets [17]. The FIG family annotations for the metagenomic sequences were downloaded from the <software>MG-RAST</software> server (http://metagenomics.theseed.org/). We conducted the KO family annotations of the sequences based on the best <software>blast</software> hits with E-value cutoff of 1e-5, a typical E-value cutoff used for <database>KEGG</database> pathway reconstruction in metagenomes [16].<br>
<br>
Availability and Supplementary Material<br>
<software>MinPath</software> is available as a server and the source codes are available for downloading at <software>MinPath</software> webpage, http://omics.informatics.indiana.edu/MinPath/. Supplementary material is also available at the <software>MinPath</software> website.<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2841612</b><br>
Predicted Auxiliary Navigation Mechanism of Peritrichously Flagellated Chemotactic Bacteria<br>
Chemotactic movement of Escherichia coli is one of the most thoroughly studied paradigms of simple behavior. Due to significant competitive advantage conferred by chemotaxis and to high evolution rates in bacteria, the chemotaxis system is expected to be strongly optimized. Bacteria follow gradients by performing temporal comparisons of chemoeffector concentrations along their runs, a strategy which is most efficient given their size and swimming speed. Concentration differences are detected by a sensory system and transmitted to modulate rotation of flagellar motors, decreasing the probability of a tumble and reorientation if the perceived concentration change during a run is positive. Such regulation of tumble probability is of itself sufficient to explain chemotactic drift of a population up the gradient, and is commonly assumed to be the only navigation mechanism of chemotactic E. coli. Here we use computer simulations to predict existence of an additional mechanism of gradient navigation in E. coli. Based on the experimentally observed dependence of cell tumbling angle on the number of switching motors, we suggest that not only the tumbling probability but also the degree of reorientation during a tumble depend on the swimming direction along the gradient. Although the difference in mean tumbling angles up and down the gradient predicted by our model is small, it results in a dramatic enhancement of the cellular drift velocity along the gradient. We thus demonstrate a new level of optimization in E. coli chemotaxis, which arises from the switching of several flagellar motors and a resulting fine tuning of tumbling angle. Similar strategy is likely to be used by other peritrichously flagellated bacteria, and indicates yet another level of evolutionary development of bacterial chemotaxis.<br>
<br>
Introduction<br>
Many motile unicellular organisms are known to direct their movement in gradients of specific chemical substances ? the process called chemotaxis. Chemotaxis plays an important role in the microbial population dynamics with chemotactic bacteria in a nonmixed environment ? that is in presence of nutrient gradients ? having significant growth advantage [1]?[4]. Modeling of microbial population dynamics indicates that motility and chemotactic ability can be as important for evolutionary competition as cell growth rate [5],[6]. The chemotaxis system is thus expected to be highly optimized, as has been indeed suggested by several studies [7]?[10].<br>
The best example of such optimization is bacterial chemotaxis strategy itself. While eukaryotic cells are able to sense the gradients by direct comparison of concentrations at the opposite sides of the cell [11], bacteria like E. coli employ temporal comparisons along their runs [12]. Theoretical analysis suggested that such strategy is superior to direct spatial comparisons for objects of bacterial size and swimming speed [7]. Adapted E. coli has two swimming modes: runs, which are periods of long straight swimming, and tumbles, when bacterium stops and changes its orientation. The runs of a swimming bacterium are interrupted by tumbles which abruptly change the swimming direction. For cells swimming up an attractant gradient, the runs become longer due to suppression of tumbles, and the cell population migrates up the gradient. The frequency of tumbles is controlled by the chemotaxis network through switching of individual motors. During a run, flagellar motors rotate counter-clockwise (CCW) causing flagella to form a bundle, whereas switching of one or several flagellar motors to clockwise (CW) rotation breaks up the bundle and initiates a tumble. The direction of motor rotation depends on the concentration of phosphorylated CheY molecules, which bind to the motor and switch its direction in a highly cooperative mode. The CheY phosphorylation is controlled by the histidine kinase CheA, which forms sensory clusters together with transmembrane receptors and the adaptor CheW. Each receptor can be either active or inactive, depending on ligand binding and on the methylation level. The active receptor activates CheA, eliciting downstream phosphorylation of the response regulator CheY. Phosphorylated CheY (CheYp) is dephosphorylated by CheZ. Receptors can be methylated by the methyltransferase CheR and demethylated by the methylesterase CheB. Methylation regulates the receptor activity. Because the reaction of receptor methylation is much slower than the initial response, methylation provides chemical ?memory?, which allows the cell to compare the current ligand concentration with the recent past.<br>
Early single-cell tracking experiments reported no dependence of the tumbling angle, i.e. turning angle between consequent runs, on the direction of the gradient and the inclination of a run [12], and it was thus presumed to be random in subsequent modeling of bacterial chemotaxis. However, in recent study that used high-resolution fluorescence video microscopy [13], it was shown that the cell turning angle depends on the number of CW-rotating filaments involved in the tumble, and thereby the turning angle rises proportionally to the number of motors that switched to CW rotation. Because the CW switch probability is set by the chemotaxis system dependent on the cellular swimming direction along the gradient, the tumbling angle can be expected to depend on the swimming direction, too. If the cell swims up a gradient of attractant, the probability of CW rotation is smaller, and fewer motors are likely to change directions. Therefore, even if the cell makes a tumble, the tumbling angle should be small. When the cell swims down the gradient of attractant, the probability of CW rotation is higher and more motors are likely to change directions during a tumble, with the consequence that the tumbling angles will be larger.<br>
The goal of this study was thus to investigate the magnitude of the tumbling angle dependence on the swimming direction and the effect of such dependence on the chemotactic efficiency. We introduced dependence of the turning angle on the number of CW-rotating motors in a recently constructed hybrid model of chemotactic E. coli, <software>RapidCell</software> simulator [14]. Our simulations demonstrate that although the estimated difference of tumbling angles up and down the gradient is only few degrees, even such a small difference significantly improves the chemotactic efficiency of E. coli. We thus suggest that tuning of tumbling angle depending on swimming direction serves as an additional navigation mechanism for E. coli and other peritrichously flagellated bacteria with similar chemotaxis behavior.<br>
<br>
Results/Discussion<br>
Dependence of tumbling angle on the number of CW-rotating motors<br>
The tumbling angle dependence on the number of switching motors was investigated by extending the recently published hybrid model of chemotactic E. coli [14]. First, a more detailed model of tumbling was developed to bring the model in a closer agreement with the tracking experiments of [12]. While previous version of the model relied on a simple voting model of tumbling, which started the tumble as soon as the majority of motors rotate CW, our new model takes into account the duration of CW-rotation of every motor (Fig. 1A). The complex hydrodynamics of multiple flagella is described in simplified form, through a distortion factor which is a function of  of each motor (see Methods). Despite this simplification, the simulated swimming of E. coli is in a very good agreement with the original tracking experiments [12]. The model realistically reproduces nearly all data provided by tracking experiments: mean cellular speed, run times, tumbling angles (Tab. 1), as well as individual motor switching and graduate recovery of cellular speed after a tumble.<br>
Second, we introduced a dependence of tumbling angle on the number of CW-rotating motors that cause the tumble (Fig. 1B). This was done by fitting the experimental data of [13] with a realistic choice of discrete tumbling angles at each number of CW-switched motors (Fig. 1C). To ensure consistency with experimental data, we further assumed dependence of tumbling angle on the total number of motors. This model was called anisotropic, and it was compared to a conventional model of isotropic tumble, which chooses the tumbling angle stochastically. In simulations without a gradient, both models produce equal cellular drift velocities, with the accuracy of estimation error. To keep the mean angles of both models consistent, we defined the frequencies of the discrete angles in the anisotropic model as shown in Fig. 1D.<br>
<br>
Dependence of tumbling angle on swimming direction<br>
The model of swimming proposed here allows tumbling with variable number of motors, as soon as the sum of their CW-rotation times exceeds 0.15 s threshold needed for tumbling (). A cell swimming down the gradient will sooner reach the threshold, because each motor has higher probability of switching to CW. As a first consequence, the average run down the gradient will be shorter. As a second consequence of higher switching probability, the average number of motors that switch CW during that tumbling period will be higher than in case of up-gradient swimming. For example, cells with 3 motors when swimming down the gradient N1 tumble with  motors while up the gradient with  motors (means.e.m.).<br>
Therefore, the tumbling angles for anisotropic model depend on the swimming direction prior to tumbles (Fig. 2A). This dependence naturally arises from the dependence of tumbling angle on the number of CW-rotating motors. The simulated cells which turned with the smallest  were swimming in slightly skewed directions up the gradient before the tumble, whereas the cells which turned with the highest  were swimming with even smaller skew down the gradient before the tumble. A more detailed analysis shows that the total angular difference between tumbling angles that correspond to the movement up and down a gradient is only about 3 (Fig. 2B). Such a small difference is within the error of the early tracking experiments, about  [15], which explains why it remained undetected.<br>
<br>
Effect of anisotropic model on cell drift velocity<br>
Despite such a small difference of mean angles, it can significantly increase the chemotactic performance, with the mean drift velocity being up to two times higher for anisotropically tumbling cells (Fig. 2C). The positive effect of anisotropic tumble becomes more visible in steeper gradients and for higher number of motors, which suggests that highly flagellated cells can adjust their tumbling angle more precisely.<br>
In the case of  motors and moderate gradient (N1), the mean tumbling angle is . This value is only  smaller than the angle in ligand-free simulations, so the increase of the drift velocity in the anisotropic model cannot be attributed to the change of the total mean tumbling angle. The mean tumbling angle up the gradient , while down the gradient it is . Therefore, the  difference in mean tumbling angles causes a 52% increase in the population drift velocity, from 0.92 to 1.4  (Fig. 2C).<br>
<br>
Dependence of anisotropic model effect on the magnitude of angle adjustment and on rotational diffusion<br>
As a control, we simulated chemotactic cells that tumble with a constant angle (67.5 deg.), and compared them to cells that tumble with slightly smaller angle (67.5?), when they swim up the gradient, and with slightly higher angle (67.5+), when they swim down the gradient. Here, the  was a constant parameter changed from 1 to 5 deg. A difference of  degrees increased the drift velocity by about 100% in the gradient N1, and by  50% in the gradient N2 (Fig. 3A). This confirms that the observed increase in drift velocity shown in Fig. 2C is due to small changes in tumbling angles of up- and down-swimming cells, and does not arise from model-specific parameters.<br>
Bacterial movement in gradients is further affected by the Brownian motion for both isotropic and anisotropic tumbling models (Fig. 3B). In our simulations we used  (Tab. 1). At lower coefficients of rotational diffusion, both models demonstrate better chemotaxis, and the advantage of the anisotropic tumbling is most pronounced, which is due to lower noise factor arising from rotational diffusion [16]. Since rotational diffusion depends on the cells size, flagellar length, media viscosity and temperature [17],[18], predicted effects of anisotropic tumbling can be even more pronounced for other bacteria or under different environmental conditions.<br>
<br>
Conclusions<br>
Taken together, our results suggest that in addition to extending the run length while swimming up the gradient, E. coli uses an auxiliary mechanism of tumbling angle tuning according to the swimming direction. This fine tuning of tumble is mediated by the same adjustment of tumbling frequency that underlies the conventional chemotaxis strategy of E. coli (Fig. 4). Since both navigation mechanisms arise from the same basic mechanism of altered motor switching, evolutionary optimization of the basic mechanism depends on both the effect from the tumble frequency and the number of flagella that reverse per tumble. The previously unrecognized mechanism shown here is expected to be shared by other peritrichously flagellated bacteria with similar chemotactic behavior, and it seems to represent yet another level of evolutionary optimization of the chemotaxis system.<br>
<br>
<br>
Methods<br>
Model of chemotaxis signaling network<br>
We applied the recently proposed Monod-Wyman-Changeux (MWC) model for mixed receptor clusters [19],[20], which accounts for the observed experimental dose-response curves of adapted cells measured by in vivo FRET experiments [19],[21], as shown in [20],[22],[23]. According to the MWC model, an individual receptor homodimer is described as a two-state receptor, being either ?on? or ?off?, with the free energy being a function of methylation level  and ligand concentration (1)where  is the ?offset energy?, and ,  are the dissociation constants for the ligand in the ?on? and ?off? state, respectively. Groups of receptors form larger sensory complexes, or signaling teams, with all receptors in a team being either ?on? or ?off? together. The teams are composed of mixtures of Tar  and Tsr  receptors, and the total free energy of the team is given by(2)The probability (A) that a team will be active is a function of its free energy(3)<br>
The adaptation is modeled according to the mean-field theory [24],[25], assuming that the CheB demethylates only active receptors, CheR methylates only inactive receptors, and both enzymes work at saturation(4)This equation implies that both enzymes work in the zero-order regime. The linear products  and () mean that a bound CheR (CheB) can only act if the receptor team is inactive (active), with probability  and , respectively.<br>
The average methylation level  is assumed to be a continuously changing variable within the interval , with linear interpolation between the key offset energies, , as suggested in [25],[26]. The ODE for methylation (Eqn. 4) is integrated using the explicit Euler method to ensure high computational speed of the program, while the time step is chosen as 0.01 s to keep the simulation error low.<br>
The details of network model were previously described in [14]. CheA kinase activity is assumed to be equal to the activity of the receptor complex . The rate of phosphotransfer from active CheA to CheY is much faster than the rate of CheA autophosphorylation [9],[27]. Therefore, the relative concentration of CheYp is obtained as a function of active CheA from the steady-state equation(5)where  is a scaling coefficient, , ,  are the rate constants according to [9],[28],[29].<br>
The relative concentration of CheYp is converted into the CCW-motor bias using a Hill function [30]:(6)where  [30],  [30],[31].<br>
<br>
Model of bacterial swimming<br>
To simulate the experimentally observed hydrodynamics of bacterial swimming and tumbling [13],[32] in simple terms, we introduce a distortion factor  which reflects how one CW-rotating flagellum influences the cellular speed and angular deviation(7)This functional form implies that the distortion rises proportionally to the CW rotation time  as long as it is below the threshold  (the first period). After this threshold is reached, the distortion exponentially decays (the second period). The first period corresponds to unwinding of a flagellum from the bundle and its rotation in the right-handed semicoiled form, which initiates a tumble. In the second period, when the flagellum rotates CW longer than the threshold time, a rapid transformation from semicoiled to curly 1 form occurs, and the flagellum twists around the bundle during the new run, due to high flexibility of the latter form [32].<br>
The influence of several simultaneously CW-rotating motors is assumed to be proportional to the sum of their distortion factors(8)This implies that the tumble can occur if a single motor rotates CW for at least  period, or if two or more motors rotate CW together for a shorter time. Formally, a tumble occurs when , where  is a threshold value. In principle, the threshold depends on the total number of motors: the larger , the higher  is required to generate a tumble. This is consistent with experimental data of [13], Fig. 12 therein, where switching of 1 motor is sufficient for a tumble at , but for  at least 2 motors are necessary for a tumble. However, we keep the same  for  for simplicity, to avoid additional arbitrarily chosen thresholds. The simulated run lengths in a ligand-free medium have distribution close to exponential.<br>
The cellular swimming speed depends on the distortion in a piece-wise linear form(9)In our model, we considered only ?complete? tumbles, which occur when  reaches  and the swimming speed falls to zero: at this time point the cell instantly changes its orientation by the tumbling angle , which is determined by two alternative models, isotropic and anisotropic. For simplicity, we assumed that if the distortion  does not reach , it causes only a drop of speed, without a change of the swimming direction.<br>
During a run, the direction of cellular swimming is affected by the rotational diffusion [12],[17]. After each time step, the swimming direction is changed by adding a stochastic component with normal distribution , where the diffusion coefficient  equals  [17].<br>
Isotropic tumbling<br>
The tumbling angle  is distributed according to the continuous probability density function , , as suggested in [33]. The mean  of this angle distribution, , is close to experimental measurement of  [12], and shapes of the simulated and experimental distributions are simular. The angle distribution does not depend on any external factors.<br>
<br>
Anisotropic tumbling<br>
The tumbling angle  is determined by number of CW-rotating motors  involved in the tumble, and the total number of motors . For each pair of , we simulated the cell swimming in a ligand-free medium and calculated the frequency  of the tumbles which are caused by  CW-rotating motors. Using the frequency , we chose the turning angle  close to the experimental values [13], while keeping the average turning angle constant in all models,(10)Note that here tumbling angles are discrete, as opposed to the continuous probability density function of isotropic tumble.<br>
The program <software>RapidCell</software> is available at www.rapidcell.vladimirov.de.<br>
<br>
<br>
Constant-activity gradient<br>
In order to measure the chemotactic efficiency accurately and to avoid the effects of receptors saturation, we simulated the cells in an artificial constant-activity gradient, which ensures a constant chemotactic response CheYp and a constant cell drift velocity over a wide range of ligand concentrations, in contrast to commonly used Gaussian and linear gradients [14]. Drift velocity in constant-activity gradient was measured by a linear fit of  in the time interval from 200 to 500 s. The constant-activity gradient has the following form:(11)where  is the ligand concentration in position , and  is the geometric mean of Tar methyl-aspartate dissociation constants. Here  is a free parameter which determines the steepness of the gradient, and thereby the drift velocity of cells up the gradient. We compare the drift velocities in three constant-activity gradients, with relative steepness changing two-fold from one to another, and designate them as N0, N1 and N2. The corresponding gradient functions are(12)with  mm for N0, N1 and N2, respectively. Here  is the size of square 2D domain, where cells were simulated starting from the center of left wall .<br>
<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2855319</b><br>
Flow-Based Cytometric Analysis of Cell Cycle via Simulated Cell Populations<br>
We present a new approach to the handling and interrogating of large flow cytometry data where cell status and function can be described, at the population level, by global descriptors such as distribution mean or co-efficient of variation experimental data. Here we link the ?real? data to initialise a computer simulation of the cell cycle that mimics the evolution of individual cells within a larger population and simulates the associated changes in fluorescence intensity of functional reporters. The model is based on stochastic formulations of cell cycle progression and cell division and uses evolutionary algorithms, allied to further experimental data sets, to optimise the system variables. At the population level, the in-silico cells provide the same statistical distributions of fluorescence as their real counterparts; in addition the model maintains information at the single cell level. The cell model is demonstrated in the analysis of cell cycle perturbation in human osteosarcoma tumour cells, using the topoisomerase II inhibitor, ICRF-193. The simulation gives a continuous temporal description of the pharmacodynamics between discrete experimental analysis points with a 24 hour interval; providing quantitative assessment of inter-mitotic time variation, drug interaction time constants and sub-population fractions within normal and polyploid cell cycles. Repeated simulations indicate a model accuracy of ?5%. The development of a simulated cell model, initialized and calibrated by reference to experimental data, provides an analysis tool in which biological knowledge can be obtained directly via interrogation of the in-silico cell population. It is envisaged that this approach to the study of cell biology by simulating a virtual cell population pertinent to the data available can be applied to ?generic? cell-based outputs including experimental data from imaging platforms.<br>
<br>
Introduction<br>
Multiparameter flow cytometry is widely used to study the cell cycle and its perturbation in the context of both basic research and in routine clinical analysis [1]?[6]. Such analyses may use a wide range of fluorescent reporters that correlate to the expression of key molecular components of the cell cycle, such as cyclins and cyclin dependent kinases (CDK), [1] or quantify DNA content [5]. Regardless of the particular fluorophores used the quantitative methodology and the ensuing synthesis of biological knowledge is based on statistical analyses of the experimental data sets. For single variable distributions these may include calculations of moments of increasing orders to provide the mean, variance, skewness etc. or cumulative indices such as the Kolmogorov-Smirnov (K-S) test [7]?[9]. More complex, multi-variate approaches may involve discriminant function, cluster or principal component analysis in an n-dimensional space [10]?[12]. In all of these approaches there is a common procedural thread: acquisition of data is followed by a statistical parameterisation of the measurement set to which biological form or function can be correlated. In this work, we present an alternative, based on computational simulation of the experiment. A stochastic simulation of the cell cycle dynamics within a large population is initialised with reference to a flow cytometry data set and then evolved, using evolutionary computer algorithms, with assessment of fitness measures derived from comparisons to subsequent data sets. The cell-cycle information is then read directly from the in-silico populations.<br>
The development of a simulated cell population approach has been driven by a requirement to track the evolution of large numbers of cells over multiple generations through the cell cycle and provide a means to track progression of both the whole cell population and distinct sub-groups [13],[14]. This is in the context of mapping the heterogeneity of cell cycle response to perturbation events e.g. effects on cell proliferation of anticancer therapeutics designed to block cell division. In this report we present the conceptual basis of this simulated cell cytometry and detail of the methodology adopted. To demonstrate the application of the technique and validate its potential we use it to quantify cell cycle perturbation in a tumour cell line by a topoismerase II inhibitor which causes endocycle routing in the late cell cycle.<br>
The aim of the simulation is to predict the dynamic evolution of a large population of virtual cells (vcells) through a life cycle corresponding to that prescribed by their real-life counterparts (as reported via flow cytometry experiments). Furthermore, the model seeks to account for perturbations in the cell cycle progression of the virtual population (vpopulation). The spatial position of the vcells within the cell cycle is initially determined from a real flow data set. From this information, each vcell is assigned a temporal position within the mean inter-mitotic time (?IMT), allowing cell cycle events such as DNA replication and cell division to be stochastically predicted. After the vpopulation has evolved for a given period, they may be compared with a further experimental data set to enable important simulation parameters, governing their evolution, to be optimised and constrained so that correlations between the respective data sets are maximised. Standard approaches to studying cell cycle involve statistical analysis of distributions either 1D involving nuclear content reporters [5] or 2D when further cell cycle molecular reporters are also included [1],[15]. Thus these are inherently ?whole population? measures and can only describe cell variability via global parameters such as the standard deviation from the mean. Whilst automated analytical approaches have been developed in order to reduce user subjectivity [16]?[18] the majority of flow analyses still involve user-defined gating of the as-measured data set to identify and segment a sub-population of cells. Subsequent mapping of this population onto 2D dot plots of fluorescence provides temporal snapshots (typically with a 24 hour sampling period) and further partitioning of cells to different compartments, G1, S, G2/M within normal and polypoid cycles (see Figure 1(a)). These apparent quantitative assessments become inaccurate and, to varying extent, subjective, as they are based either on user identification of the various components in the dot plot by fitting of Gaussian distributions, representing the G1, S, G2/M fractions, to the DNA content histogram [5]. The challenge of the current investigation is to adopt a computational approach, where the analytical and interpretive steps are implemented at the simulated biology stage and not on the raw data outputs. No new data is added in this approach and the computer simulations could be viewed as an elaborate form of data analysis. However, the methodology does deliver new insight on process, delivering a continuous simulation of the dynamic evolution of the cellular system between fixed sampling points. In this respect, it provides a physical validation when applying various hypotheses to interpret the experimental data. It also goes some way to visualising the variation between individual cells that gives rise to biological heterogeneity as the stochastic simulation delivers a report on population dynamics in which each and every cell can be tracked.<br>
<br>
Materials and Methods<br>
Experimental procedures<br>
Experimental data is obtained using well-established bi-variate cytometric methods for study of the cell cycle: U-2 OS (ATCC HTB-96) cells were transfected with a G2M Cell Cycle Phase Marker (GE Healthcare, UK), yielding stable expression of a GFP-cyclin B1. This provides a green fluorescence signal the intensity of which correlates to position in the cell cycle with a minimum signal at G0 and a peak during the G2/M phase. [19]. The culture was maintained under G418 selection in McCoy's 5a medium supplemented with 10% foetal calf serum (FCS), 1mM glutamine, and antibiotics and incubated at 37?C in an atmosphere of 5% CO2 in air. To obtain fluorescence read-out of DNA content an anthraquinone derivative, DRAQ5? (20 ?M Biostatus Ltd., UK) was used [14]. This binds to DNA providing a fluorescence intensity that can be related to DNA content and thus it reports on cell cycle progression through the S phase to G2/M (&gt;4N) or, in the presence of external perturbing agents, progression through polyploid states as the mitotic stage is by-passed [20] (see Figure 1(a)). To obtain a model system in which we can test the simulated cell population approach we have used a cell division by-pass agent: ICRF-193 [bis(2,6-dioxopiperazine)], a kind gift from Dr A.M. Creighton (ICRF, London, UK). This is a reversible catalytic inhibitor of topoisomerase II that blocks the ability of the enzyme to resolve interlinked DNA replication products [21]. The decatenation of chromosomal replication products is vital for the completing of segregation and hence normal division. ICRF-193, was prepared in DMSO at 2 mg/ml and used at a peak concentration of 2 ?g/ml (equivalent to 7.2 ?M).<br>
To determine the cell population distribution of fluorescence intensity a FACScan flow cytometer was used (Becton Dickinson Inc., Cowley, UK) which was equipped with an air-cooled argon ion laser (with 488 nm output only). GFP-cyclin B1 data was collected using a 30 nm bandpass emission filter centred at 530 nm and the DRAQ5 signal with a 670 nm long pass filter. <software>CELLQuest</software> software (Becton Dickinson Immunocytometry Systems) was used for data acquisition. Flow cytometric analysis was used sample sets of 10,000 cells and the data presented represents the signal peak height. Typically, tracking of the population was carried out at 24 hour intervals.<br>
<br>
Virtual cell populace simulation<br>
The computer simulation consists of two principal components; a cell population model (CPM) and an evolutionary algorithm - Differential Evolution (DE) [22]. The CPM generates a virtual population of cells (vcells), which is initialised using a flow data set. The vpopulation is then evolved and compared to a subsequent flow data set. The CPM evolves each vcell and generates any cell cycle processes deemed relevant to explain the laboratory experiment. A DE algorithm is employed to optimise important ensemble parameters used in the CPM e.g. cell cycle time, enabling the vpopulation to be evolved such that it maximises correlation with the data. A detailed description of the cell population simulation, complete with a full account of the various numerical algorithms and techniques used is given in Text S1. A brief outline of the main components of the cell population model is given in the following sections with reference to the simulation flowchart shown in Figure 2. All numerical algorithms have been written in the <software>MATLAB</software> environment (MathsWorks UK); fragments of pseudo-code for important aspects of the CPM are given in Text S1.<br>
<br>
Cell population model - initialisation<br>
The vpopulation is initialised by reference to a gated 2D flow cytometric data set composing of the cell cycle reporter cyclin B1 (GFP-cyclin B1) and DNA content determination (DRAQ5) (see Figures 1(b) and S1). The data is gated using a simple cell density cut-off technique, where a region is labelled active if its cell density is above a set threshold, (see Text S1). Cells within a contour encapsulating the gated fraction serve to initialise the vpopulation position in the intensity space. The same gating procedure (and threshold value) is also applied to subsequent experimental data sets at later time points. More sophisticated gating techniques could be applied such as the expectation-maximisation algorithms presented by Boedigheimer et al [18]; however, this simple approach is adequate to establish the validity of our methodology.<br>
The gated data is now used to initialise the fluorescence intensities of the modelled cell population, which correspondingly inherits the biological variation seen in Figure 1(b) (each gated data point initialises one cell). The temporal position of each of the virtual cells within the cell cycle is unknown as the flow data (consisting only of only fluorescence intensities) contains no direct cell cycle time information. The time-based information, necessary to model the cell cycle dynamics, is extracted from the intensity signal of the biological markers obtained from the experimental data (see Text S1). The first approximation to assigning a time to each vcell is obtained by considering the DRAQ5 fluorescence intensity, the histogram of which is shown in Figure 3(a). In our approach, we use the DRAQ5, nuclear content indicator to position each vcell in the cell cycle making the following assumptions (i) the vcells are randomly distributed throughout their cycle and (ii) that their DRAQ5 signal is monotonically increasing through the cell cycle as the nuclear content is duplicated. This infers that the minimum and maximum DRAQ5 intensities correlate to the start and finish of the cell cycle and allows us to assign relative position in time to each vcell (see Text S1 and figure S2). The experimental dataset for DRAQ5 intensity is sorted into ascending order and fitted with a polynomial function (see Figure 3(b)). Because of the inherent digitisation produced by data binning, of the measured intensity, by a flow cytometer several cells will be recorded with the same DRAQ5 signal. The intensity sorting procedure assigns increasing sort indices to vcell sets with the same intensity (i.e. all cells within a given bin) the median index value is therefore used when implementing the polynomial fit (see inset in Figure 3(b)). Finally, the polynomial x-axis values are scaled to a range of zero to the inter-mitotic time (IMT - time between successive mitotic events), this gives an absolute time for each cell within its cycle. To relate the GFP signal to cell cycle time the intensity for each cell is plotted against the cell number index obtained from the DRAQ5 sort procedure, again fitted with a polynomial and scaled to give an x-axis running from 0 to the same IMT value (Figure 3(c)). The use of a stoichiometric nuclear content marker (such as DRAQ5) to estimate DNA content and hence cell cycle position is well established and both deterministic and stochastic models have been used previously to obtain continuous temporal descriptions [23]?[25]. Our approach differs from the previous studies in that through the creation of the virtual cell population we model at the level of single cells rather than using population level parameters.<br>
The two fitting polynomials describe the evolution of the fluorescence intensities from cell birth to division as a function of time and are used to produce a median path through the cell cycle shown as the solid black line in the 2D GFP-DRAQ5 intensity plot displayed in Figure 1(b). It is obvious from the plot that many of the cells lie some distance from the median line this is due to natural variability in the measured signals caused by heterogeneity in reporter loading, noise, variation in collection efficiency etc. Each individual vcell is therefore assigned a cell cycle time by choosing a point on the 2D polynomial median line, that minimises the sum difference of the DRAQ5 and GFP intensity values (see Text S1 and figure S3). Therefore vcells at the same point within the cell cycle will display a heterogeneity in fluorescence signal value (corresponding to the width of the population plot in x and y-directions in Figure 1(b). Therefore, to calculate the time-dependent trajectory of each vcell through the 2D intensity space we update the DRAQ5 and GFP intensity values using the median line as time is incremented (see Text S1).<br>
To summarise, the experimentally measured data is used to establish a virtual cell population with exactly the same heterogeneity in fluorescence signal as seen in the experiment. This vpopulation is then evolved within a stochastic simulation allowing for variability in fluorescence intensity and IMT using a pair of polynomial functions that describe the cell cycle dependence of the signal, i.e. the absolute fluorescence is stochastic but the time evolution function is the same for all cells.<br>
<br>
Cell population model - evolution<br>
Once initialised each member of the population has three discriminating properties corresponding to: (i) a time in the cell cycle, (ii) DRAQ5 fluorescence intensity and (iii) GFP-cyclin B1 levels. In order to mimic DNA synthesis and replication, a supplementary parameter, DRAQ5DNA2, is required; DRAQ5DNA2 details the DRAQ5 magnitude at which each vcell has doubled its DRAQ5 intensity (see Text S1 and figures S4 and S5). The CPM directly relates this to the point at which a real cell has doubled its DNA content, i.e. a phase transition to G2. The value of DRAQ5DNA2 is deduced by calculating the initial DRAQ5 intensity of each vcell at the start of the cell cycle (see Figure 1(b), black curve), which from the above is estimated at the effective intensity value just after a mitotic event, then assessing the time at which this initial intensity doubles using the polynomial functions shown in Figures 3(b) and (c).<br>
Monitoring of the simulated DRAQ5 intensity then allows identification of cells that have multiplied their DNA content allowing placement of each into the following sub-groups: normal cycle ? DNA index, DI?=?2N (G1) or 4N (G2/M); polyploidy cycle - DI?=?4Np (G1p) or 8Np (G2p/Mp). Once vcells have entered the G2/M phase the probability of them entering the M phase and undergoing cell division is calculated. This is achieved using a simple stochastic decision process [13], where we define a cumulative Gaussian probability distribution which scales between 0 and 1, defined in terms of a mean inter-mitotic time, with an associated standard deviation, over the cell cycle time. Both these parameters are to be optimised via the evolutionary algorithm to best fit the second set of flow data. At each time step a random number, uniformly distributed in the interval [0 1] is generated and is compared with the cumulative probability distribution value at that time. If the random number is less than the probability distribution value calculated then mitosis is deemed to occur and the simulation generates two daughter cells at t?=?0 in G1/S with the DRAQ5 and GFP-cyclin B1 associated with the parent cell. Otherwise, the cell remains in the G2/M phase for re-analysis at the following time step, which will increase both of its intensity coordinates resulting in a higher probability of mitosis (the cumulative Gaussian distribution tends to 1 with increasing time). The implementation of this mitotic variability produces further heterogeneity in the IMT of the individual vcells.<br>
The cell population model is defined by a set of parameters specific to the flow cytometry experiment conducted. Optimisation of the fit between simulation and experiment is dependent upon selection and minimisation of the population variables, in our case: the mean inter-mitotic time, its standard deviation and a parameter detailing the presence of a drug in the vpopulation. There are several different methods, which could be used to determine the best fit to the experimental data; we choose to use a differential evolutionary technique to optimise these cell cycle parameters. The quality of fit associated with a set of CPM parameters is determined by calculation of the ratio of evolved vcells to that measured experimentally within a numerically deduced gated region. This simple maximisation strategy, works well for both therapeutically (un)perturbed systems, although newer versions of the CPM will explore more sophisticated 2D cross correlative algorithms to infer fitness. Convergence of the differential evolution algorithm is determined true when the quality of fit varies by less than 1% over five subsequent generations (see Text S1).<br>
<br>
<br>
Results<br>
Cell population simulation<br>
To illustrate the evolution of the vcell population, we generate a series of snap-shots derived at different temporal intervals (Figure 4 - green population) demonstrating the simulated intensity dot plot at 6, 12, 18 and 24 hours respectively after initialisation by an experimental data set. Here, the vcell population has a mean IMT of 22 hours and an associated standard deviation of 6 hours; a small subpopulation of vcells can be depicted (red dots) also a contour (dashed black line) is displayed, indicating the extent of the gated experimental data set at initialisation. Given that the cells in these experiments are randomly distributed within their cycle and a statistically relevant data set is sampled the acquired plots appear identical for a control sample with an unperturbed biology. The advantage of the simulated population approach is therefore evident in Figure 4, as a discrete sub-set of cells is identified and its dynamics tracked over a period of time. Despite using a single experimental sample information is obtained across the whole of the cell cycle due to the assumption of random temporal distribution. The fundamental insight gained here is the adoption of a simulated cell approach and subsequently the visualisation of the temporal dimension encoded in the fluorescence intensity distributions.<br>
<br>
Cell cycle perturbation and cell cycle rerouting to polyploidy<br>
To test the ability of the simulation to capture more complex dynamics associated with aberrant cell cycle progression and variance of response across sub-populations a cell cycle perturbation experiment was undertaken using a mitotic by-pass agent ICRF-193. Cells treated with this agent progress through multiple replication cycles without undergoing mitosis, therefore doubling DNA content [21]. This leads to an evolving polyploid population that is identified using the nuclear dye, DRAQ5 to obtain an optical read-out of DNA content. Perturbation of the cell cycle and rerouting of cells in this manner provides a system in which the population dynamics of diverted sub-groups within the normal and polyploidy cycles can be analysed. The challenge for the cell population simulation is to track the inter-related pharmacodynamics, taking full account of the detailed evolution of the accompanying fluorescence data.<br>
<br>
Experimental outputs<br>
A block and chase experiment was conducted in which cells were continuously treated with ICRF-193 for 24 hours (Figure 5(a?c)). A 2D dot plot of the cell cycle (GFP-cyclin B1) and nuclear content (DRAQ5) reporters at the 24 hour time point shows a sub-population of cells with low GFP-cyclin B1 expression and a DNA index of 4N i.e. polyploid cycle cells in the G1/S phase (Figure 5(b)). Compared to control conditions, where all cells were engaged in the normal cell cycle. Following the 24 hour drug treatment with ICRF-193, wash-out allows cells to further cycle unperturbed under normal conditions for a further 18 hours (including cell division). ICRF-193 is a reversible topoisomerase II blocker and so removal of this agent enabled the sub-population of cells within G2/M of the normal cycle to be routed back into normal cycle (i.e. to G1/S). Hence, the 42 hour data shows two distinct population groups describing cells within the normal and polyploid cycle (Figure 5(c)).<br>
<br>
Model outputs<br>
To include the effect of the ICRF-193 in the CPM we include a further optimisation parameter Nbp, which describes the fraction of vcells that have doubled their DNA content (G2/M phase) but have bypassed mitosis. These are selected stochastically and inhibited from undergoing cell division when under drug ?dosing? conditions. This assignment is undertaken at each time step, until the required percentage of vcells in the population have by-passed mitosis. In the drug ?wash-out? conditions, the reduction in Nbp is modelled with a half-life, t1/2, corresponding to the temporal persistence of the drug-induced perturbation. Thus depending on drug administration or wash-out the CPM has three optimisation variables to be minimised through the evolutionary methods described previously. The comparison of real (red population displayed in Figure 5(b)) and vcell populations for selection of the variable parameter values is made 24 hours after initialisation. The optimised vcell population together with the real data contour is shown in Figure 6(a) together with a contour illustrating the position of the initial data set. The simulation clearly captures the key features of the population evolution and given the stochastic nature of both real and virtual cell populations they are well correlated. At this point, following 24 hours of continuous drug treatment, there are large fractions of 4n cells in the normal and 4np polyploid phases as well as a sub-population of polyploid cells progressing to 8np phase. A small population sub-set (located within the black dashed contour in Figure 6(a)) represents the vcells yet to be influenced by the drug. As mentioned above, at each discrete time throughout the simulation the vcells are stochastically tested to see if they have been drug treated, hence, a finite time must elapse before all vcells can be influenced by the action of the drug.<br>
The vcell dynamical parameters corresponding to the fits shown in Figure 6 are indicated in Table 1. In the presence of the drug the simulation indicates a mean inter-mitotic time of 36 hours with a standard deviation of 4 hours. In comparison, the IMT value from fitting to a control set of data is 22?4 hours. Multiple runs (1,000 simulations of the experimental data) of the model indicate that the variation in the tabulated values, due to stochastic variation and evolutionary selection, is less than 5%. The prediction of an extended IMT within drug treated cells is in agreement with previous studies on the effects of ICRF-193, showing delays in progression to the mitotic phase plus extension in the duration of mitosis once initiated. Although, the CPM cannot elucidate on the persistence of individual phase duration it does accurately estimate their combined effect.<br>
During the chase phase of the assay subsequent to drug wash-out, the simulation evolves from 24 to 42 hours in a similar manner to that above, with the difference that the Nbp parameter is indirectly optimised using a half-life to describe its temporal decay; i.e. the fraction of vcells that retain drug-induced division-bypass is  where  and t is the time since wash out. The intensity coordinates of the vpopulation at 42 hours after ICRF-193 washout are displayed in Figure 6(b). The simulation has captured the important features of both the normal and polyploid cycle dynamics. That is, there is a significant sub-population of vcells in each of the four DNA indexed phases. For the 2D fit shown in Figure 6(b) the simulation uses a mean inter-mitotic time of 22?7 hours respectively. This agrees remarkably well with that measured through microscopic techniques for an unperturbed real populace. Furthermore, the simulation gives an insight to the temporal persistence of the drug on the virtual population, indicating that a significant sub-population retain or are committed to the division bypass over the course of a few hours following wash-out. The fact that an effective continuum of intensities straddling the 4np and 8np phases in both real and virtual populations is evident reinforces the simulation result which highlighting of temporal persistence of ICRF-193 post washout. The evolution and perturbation of the vcell population is shown in Video S1.<br>
The continuous population dynamics provided by the simulation are shown in Figure 7. At the initialisation point (t?=?0 hours) we see that a significant fraction of vcells are present in the 2n phase compared to that in the 4n phase (blue and green curves respectively), ?4?1 ratio. Over the first 24 hours, the drug perturbation re-routes cells from the normal into the polyploid cycle. Thus, the 4n population is stable as equal numbers of move in and out of it producing linearly decreasing 2n and linearly increasing 4np sub-populations. The percentage of mitotic-bypass cells therefore increases over time, but due to dynamical constraints and the optimised mean inter-mitotic time of 36 hours, this does not reach 100% (maximum of ?85%) before washout. The vertical dotted line in Figure 7 indicates the initiation of the washout phase of the simulated experiment. Following drug washout at 24 hours the fraction of mitotic-bypass vcells decreases exponentially with an optimised half-life of 3 hours, thus it takes the full 18 hours following drug removal to achieve something near to normality. This same dynamic inevitably affects the re-creation of a 2n population. This gives an insight to the temporal persistence of drug on the vpopulation indicating that a sub-population retains the bypass commitment for a few hours post washout.<br>
<br>
<br>
Discussion<br>
The use of stochastic computing approaches plus evolutionary algorithms to evolve a simulated cell population provides a new approach to the analysis of multi-variate data sets obtained by flow cytometry. In using this simulated biology process to analyse cell cycle perturbation we have obtained detailed information cell cycle time and the detailed dynamics of cell division and proliferation. Furthermore, we have shown that a subpopulation or cohort can be defined and tracked throughout the time course of the experiment without the need for further molecular markers, this can be essentially viewed as an in silico representation of the pulse chase experimental methods such as those incorporating two-parameter flow cytometry analysis: with DNA content and BrdUrd [26]. When applying the technique to drug-treated populations the pharmacodynamic indicators can be tracked and sub-populations within normal and polyploid cycles differentiated. Further, the temporal continuity inherent in the computational assessment also highlights details un-resolvable in the experimental sampling, such as cell cycle traverse (inter-mitotic time variation), cell cycle delays (persistence of drug-induced effects) and has also identified the occurrence and location of cell cycle restriction points, which with additional molecular mapping can be further defined [27]. Also, the simulated experiment permits individual in addition to (sub)population cell tracking allowing single cell lineage tracking and the ensuing generational patterns and relationships to be continually analysed. This is a systems approach to whole tumour population evolution leading to lineages, in contrary to tracking individual lineages and extracting a global population response [28]. We envisage that this approach would be much more easily applied to a screening approach appropriate for sampling tumours both in vitro and in vivo.<br>
In this initial implementation of the technique, we use a cell cycle marker that reports on relative cycle time and a nuclear marker which allows us to discriminate between normal and polyploid cell populations, therefore no further information of the intricate details of the cell cycle (apart from mean IMT distribution) can be deduced. In this respect, the simulated cell methodology provides a framework, describing the relationships between cells within a population, at a system level i.e. in the context of progression through a unitary cycle with associated genetic replication and cell division. Importantly this structure can enhance existing approaches by linking detailed molecular level models of cellular evolution through specific cell cycle phases [26],[27],[29] to cell heterogeneity and its influence on population level dynamics.<br>
We have adopted an approach of minimised complexity in order to clearly demonstrate the concept without the obfuscations of detailed algorithm structures and data filtering. A simple dot density cut-off filter is applied to gate the data, the number of variable parameters within the genetic algorithms is reduced to a minimum of three and goodness of fit assessed by a straightforward maximisation of simulated cells within an experimental data contour. Whilst future work will explore the potential of more sophisticated computational techniques, the simple conceptual base presented here already provides automated, objective data analysis that encapsulates the fundamental biology and delivers statistically robust results. Given the stochastic nature of the simulation it could be argued that a statistical approach should be maintained and increased simulation runs be used to acquire added certainty rather than increased model complexity. The large data sets collected in flow cytometry and the stochastic variation associated with biological systems naturally lead to statistical analysis techniques for data interpretation [23]?[25]. These have proven to be powerful tools in cell biology, however when focussing on individual cell behaviour and heterogeneity expressed at the single cell level the integrative measures of statistics are limiting. The development of a simulated biology, twinned to a real cell population, by fitting experimental data sets, maintains the statistical relevance and provides discrimination via individual cell recognition. The creation of in-silico cells brings the potential for interpolation and extrapolation thus a continuous temporal report of complex population dynamics can be produced from discrete measurements and cellular behaviour predicted beyond the limited time frame imposed by experiment and environment. The temporal continuity inherent in the computational assessment also highlights details of the pharmacodynamics, un-resolvable in the experimental sampling, such as inter-mitotic time variation and persistence of drug-induced effects. Perhaps the most beneficial aspect of the simulated cell approach is its ability to provide direct knowledge of biological state allowing a computational systems approach to inform the biology. This contrasts with traditional flow analysis, which provides information that is primary in relation to data but secondary in relation to cells; i.e. a choice can be made between direct data analysis with interpretation to translate to cell behaviour or direct read-out of cellular information from a data-directed simulation. By ensuring interoperability of the modelling algorithm with experimental cytometry outputs, the simulation provides emergent features of the cell cycle and the functional operation of molecular restrictions and checkpoints; providing further the foundation for considering the evolving asymmetric and symmetric patterns of a dynamic cellular system.<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

<b>PMC2936514</b><br>
Drug-Induced Regulation of Target Expression<br>
Drug perturbations of human cells lead to complex responses upon target binding. One of the known mechanisms is a (positive or negative) feedback loop that adjusts the expression level of the respective target protein. To quantify this mechanism systems-wide in an unbiased way, drug-induced differential expression of drug target mRNA was examined in three cell lines using the Connectivity Map. To overcome various biases in this valuable resource, we have developed a computational normalization and scoring procedure that is applicable to gene expression recording upon heterogeneous drug treatments. In 1290 drug-target relations, corresponding to 466 drugs acting on 167 drug targets studied, 8% of the targets are subject to regulation at the mRNA level. We confirmed systematically that in particular G-protein coupled receptors, when serving as known targets, are regulated upon drug treatment. We further newly identified drug-induced differential regulation of Lanosterol 14-alpha demethylase, Endoplasmin, DNA topoisomerase 2-alpha and Calmodulin 1. The feedback regulation in these and other targets is likely to be relevant for the success or failure of the molecular intervention.<br>
<br>
Introduction<br>
For the future development of new drugs, the understanding of their mechanisms of action is vital. To tackle this in a large-scale, systemic way, the <database>Connectivity Map</database> (<database>CMap</database>) consortium studied the effects of 1309 bioactive small molecules including more than 800 marketed drugs on genome-wide gene expression in four cultured human cells, [1] (http://www.broadinstitute.org/cmap/). Although drugs can perturb biological systems by interacting with different types of biomolecules [2], analysis of successful drugs has shown that generally they bind and alter the activity of proteins (so called drug targets). The monitoring of genome-wide gene expression is likely to reveal insights into the action of drugs and the prediction of additional drug targets [1], [3].<br>
One important aspect of a good target is its reliability and vulnerability over long periods. Biological systems are robust in a way that they restore the perturbations caused by drug treatments. Many drug targets thought to be suitable for therapeutic purposes turn out to be less effective than expected or account for adverse side effects [4]. Overcoming biological robustness, maintained through positive or negative feedback loops of the drug target proteins, might be a key factor for success of the intended therapeutic usage of drugs [4], [5]. The genome-wide transcriptional profiling using microarrays [1] should enable us to specifically monitor the expression changes of drug targets induced by their inhibitors or activators. The essential data required for this data integration are provided by i) <database>STITCH</database>: a drug-target relations resource [6] and ii) the <database>Connectivity Map</database> (<database>CMap</database>) which contains genome-wide expression profiles of cells treated with small-molecules [1].<br>
<database>STITCH</database> [6] is a repository merging multiple sources of protein-chemical interactions providing ?actions? (inhibition/activation) for 81% of the human chemical-protein interactions. Of those, 1290 drug-target interactions are present in the <database>CMap</database> comprising the actions of 466 drugs on 167 drug targets.<br>
<database>CMap</database> is a searchable database of gene expression profiles [1] that builds on the success of gene expression profiles from diverse chemical compounds in predicting the toxicity and/or mechanism of action of a drug [7], [8]. <database>CMap</database> data have been already used to create a human drug-drug and disease-drug network [9], [10]. The similarity of gene expression profiles recorded for unrelated stimuli in cells grown at the same time (also called batch effect) is a phenomenon known for microarray studies that needs to be overcome [11]. In order to remedy the batch effect problem in <database>CMap</database> and to make <database>CMap</database> amendable to various large scale studies, Iorio et al. proposed to construct a ?Prototype List? of the drug by merging its experiments from cell lines, batches, concentrations and microarray platforms [9]. As the signal to noise ratio can still be further improved, we implement here a novel protocol with filtering and normalization steps in order to utilize <database>CMap</database> for the elucidation of drug-induced feedback mechanisms.<br>
<br>
Results/Discussion<br>
Data filtering and expression profile scoring<br>
We obtained reliable expression differences of drug targets by filtering and normalizing the gene expression profiles (Figure 1). In <database>CMap</database>, microarray experiments were collected from four cell lines treated with 1309 small molecules at different ranges of concentrations and only partially with replicates. We performed several filtering and normalization steps leaving a total of 1144 perturbations for further processing (Figure 1). After pre-processing, each probe in the drug-induced gene expression profiles was mean centered using the average of all drug perturbation experiments in the corresponding batch rather than using its biological controls. We calculated pair-wise drug-induced gene expression profile similarity (DIPS) scores using Gene set enrichment analysis (GSEA, [12] with a similar methodology as described in Iorio et al. [9], see Materials and Methods). In total, 4,349,432 DIPS scores were calculated between all drug pairs in three cell lines, that we used to compare gene-expression profiles of pairs of drugs.<br>
<br>
Background estimates and data normalization<br>
To reveal systematic biases, the DIPS scores of drug-induced gene expression profiles using biological controls were classified into four drug and batch categories (Figure 2A). The DIPS scores between different drugs in the same batch are significantly higher than between different drugs from different batches, implying a considerable batch effect as has already been hinted at in the original <database>CMap</database> publication [1] (Figure 2A, Label 3). Still, characteristic drug features are reflected in the gene expression profiles, i.e. the DIPS scores between the same drugs (Figure 2A, Blue and Red) are significantly higher than between different drugs from different batches (Figure 2A, Grey) (t-test p-values&lt;2.2?10?16).<br>
We utilized the large number of treatments to infer the background gene expression (by mean-centering) instead of the few biological controls provided by <database>CMap</database>, in order to eliminate the batch effect. In this way, also common (e.g. stress) responses will be down-weighted to reveal the characteristic expression response of each chemical perturbation.<br>
After this normalization the batch effect was largely eliminated and the data reflect the characteristic features of drug perturbations better. The DIPS scores between different drugs from the same batch are no longer higher than between different drugs from different batches (Figure 2A, Label 3). Additionally, the DIPS scores between the same drugs from different batches (Figure 2A, Red) are higher than the between different drugs from the same batch (Figure 2A, Yellow), revealing the concordance of drug-induced gene expression profiles across batches (t-test p-values&lt;2.2?10?16). Same conclusions were also derived from the distributions of Pearson correlations for drug-induced gene expression profiles across four drug/batch categories (Figure S1).<br>
<br>
Assessment of the drug-induced gene expression profile similarity score<br>
We prove the integrity and reliability of the homogeneous gene expression profiles constructed with mean centering, by employing benchmark sets representing different features of drugs such as chemical structure similarity and shared Anatomical Therapeutic Chemical (ATC) classification of the World Health Organization (WHO) [13]. Chemical structural similarity is an indicator of shared drug targets and mechanism of action [14]?[16]. It is reported that high chemical similarity (i.e. with Tanimoto 2D coefficients &gt;0.85) tends to result in similar biological responses [17]. The ATC classification is based on both the therapeutic and chemical properties of the drug also referred to as the drug mode of action. Thus, we expect that pairs of drugs with high structural similarity or shared ATC classification result in similar gene expression profiles.<br>
Benchmarking shows that the DIPS scores calculated using the mean-centered procedure are clearly superior to the method proposed by Iorio et al. (Figure 2B). The area under the Receiver operator characteristic (ROC) curve (AUC) for the combined DIPS scores for 989 drugs (average over three cell lines), are higher both when using chemical similarity (Tanimoto 2D coefficient &gt;0.8) and the 4th level of the available ATC code shared between drug pairs (Figure S2). This confirms that the mean-centered data reflect the specific response after drug perturbation better than the treatment-control comparisons that were used previously [9].<br>
Finally, the drug induced gene expression profiles were found to be concordant across cell lines (Figure S3). Although only cancer cell lines were used in <database>CMap</database>, the procedure proposed here should be applicable to drug perturbation profiles across multiple tissues and even organ systems.<br>
<br>
Differential expression of drug-induced drug targets<br>
Integrating 4849 <database>CMap</database> arrays with 40,656 drug target relations from <database>STITCH</database> resulted in a set of 1,290 drug-target relations for which a genome-wide cellular response is available. We found that thirteen out of 167 distinct drug targets in this set (8%; 86 drug target relations) are subjected to significant differential expression upon drug treatment (Figure 3) by comparing the drug-induced expression changes of the drug target against all other treatments present in <database>CMap</database> (see methods). We found supporting evidence in the literature for seven out of thirteen (q-value &lt;0.05) significant differential regulations of drug targets shown in Figure 3, confirming the rationale and predictive power of our systematic approach. For the remaining six targets we can predict a hitherto unknown drug-induced differential regulation.<br>
The identified, differentially regulated drug targets are enriched in G-protein coupled receptors (GPCRs) (Figure S4), in agreement with previous reports that members of the GPCR family are generally regulated by several mechanisms including receptor desensitization, endocytosis at the protein level and regulation of the cellular receptor content [18], [19]. In the three cancer cell lines used, we observe agonist-induced down-regulation of GPCR mRNAs for beta-2 adrenergic receptor (ADRB2), prostaglandin E2 receptor subtype EP2 and prostaglandin E4 receptor subtype EP4 (Figure 3, Genes 3,4,12), which were previously reported in DDT1 MF-2 smooth muscle cells (ADRB2) and 293-EBNA human embryonic kidney cells (prostaglandin E2/E4 receptor subtypes EP2/EP4) [20], [21]. This indicates that drugs can induce similar feedback loops in a wide variety of cell types. However, we cannot rule out that cross-regulation among signaling pathways may be responsible for the regulation of GPCR mRNAs as it has been described before [22]. For example, it has been shown that a beta adrenergic mRNA-binding protein, ELAV-like protein 1 (ELAVL1) can be induced by ADRB2 agonist or elevated levels of cyclic adenosine monophosphate (cAMP) [22], [23] and destabilizes ADRB2 mRNA. The ELAVL1 protein binds to GPCR mRNAs and recognizes a cognate sequence located at the 3?-UTR of ADRB2, proteinase-activated receptor and M2, M3 muscarinic acetylcholine receptor mRNAs [24], [25]. Therefore cAMP provides cross-talk among GPCR regulatory networks. However, it is shown that intracellular cAMP accumulation is not the only factor contributing to the reduction of ADRB2 mRNA levels [20]. Moreover, we find that GPCR-targeting drugs regulate the transcription of their specific targets (Figure S5). We conclude that in addition to the cross-regulation of G-protein signaling pathways drug target-specific feedback loops are also responsible for the regulation of drug target mRNAs.<br>
In addition to cross-regulation of multiple drugs through the same signaling pathways, promiscuous drugs targeting multiple proteins may cause complex regulatory networks. In order to explore the cross-regulation of drug targets induced by a promiscuous drug, we searched and found that 259 out of 466 total drugs are multi-target drugs and 4 of these drugs act on multiple differentially regulated drug targets upon drug treatment. For example, Podophyllotoxin used in various chemotherapies is known to target both tubulin beta 2C and DNA topoisomerase 2-alpha. The tubulin beta 2C and DNA topoisomerase 2-alpha mRNAs are both down-regulated upon drug treatment in three cell lines (Figure 3, Genes 8,13). Tubulin beta 2C inhibitors induce microtubule depolymerization that leads to the specific down-regulation of tubulin beta 2C mRNAs preventing the translational synthesis and thus the further accumulation of abundant tubulin monomers [26]. Moreover, we found that DNA topoisomerase 2-alpha mRNAs are not down-regulated upon treatment of other tubulin inhibitors (Figure S5). Therefore, we conclude that feedback loops of tubulin beta 2C and DNA topoisomerase 2-alpha are not cross-regulated. Two other examples of multi-target drugs are vorinostat used for the treatment of cutaneous T cell lymphoma and trichostatin A that serves as an antifungal antibiotic. Vorinostat and trichostatin A are considered to be nonspecific histone deacetylase inhibitors. These drugs lead to the up-regulation of histone deacetylase 3 (HDAC3) and down-regulation of histone deacetylase 7 (HDAC7) (Figure 3, Genes 1,5). In this case it is unclear whether there is cross-regulation, although HDAC7 siRNA experiments failed to induce the up-regulation of HDAC3 mRNAs [27], a result that is disfavoring the cross-regulation.<br>
While the above cases only confirm literature reports in other cell lines or tissues, we also identified new cases of drug-induced expression regulation of drug targets. The significant novel findings are the inhibitor-induced down-regulation of calmodulin 1, DNA topoisomerase 2-alpha and up-regulation of endoplasmin, lanosterol 14-alpha demethylase and cAMP-specific phosphodiesterase 4D (Figure 3, Genes 9,8,2,7,10). Lanosterol 14-alpha demethylase is actually an off-target of antifungal drugs that bind the mammalian version with lower affinity than the fungal lanosterol 14-alpha demethylase. Probably, the up-regulation of the mammalian lanosterol 14-alpha demethylase compensates for the undesired inhibition and modulates the adverse effects. On the contrary, we observed a feedback loop that accelerates the down regulation of calmodulin 1 mRNA induced by calmodulin inhibitors. Calmodulin targeting drugs can provide a rapid and effective therapeutic effect, while at the same time small variations of drug concentrations can increase adverse effects. Therefore, it would be interesting to study further the functional effects upon target inhibition of lanosterol 14-alpha demethylase, endoplasmin and calmodulin 1 to elucidate the roles of feedback loops in drug mode of action and adverse effects.<br>
Drug-induced target regulation might be implicated in tolerance development and thus identifying potential target regulation should be an integral part of drug discovery to prevent failures in later stages of clinical trials. For example, we have observed the inhibitor-induced up-regulation of ADRB2 and thymidylate synthetase (TYMS)(Figure 3, Genes 3,11) [28]. TYMS is an essential enzyme for DNA replication/repair and an important drug target in cancerous cells. Indeed, it has been shown that inhibitor-induced TYMS over-expression obstructs the clinical efficiency by inducing tumor drug resistance [29]. In addition to over-expression, down-regulation of drug targets upon agonist treatment may also cause treatment tolerance as observed for ADRB2 long-acting agonist treatment. ADRB2 is a therapeutic target activated to treat the symptoms of asthma. We observe the agonist-induced up-regulation of ADRB2 and already in 2005, the FDA warned patients that ADRB2 might be down-regulated (desensitization) and be unresponsive for asthma treatment due to long-acting agonist exposure [20], [30]. Thus, robustness in biological systems could prevent the applicability of the long-term treatments via positive/negative feedback loops of the drug target affecting the clinical efficiency of drugs in trial and on the market. Drug-induced regulation of drug targets can thus be linked to tolerance development, which restricts the efficiency of clinical treatments where the drug concentration is limited to avoid an excess of adverse drug reactions.<br>
Taken together, we have identified drug-induced differential regulation of drug targets. Due to the limited signal to noise ratio in the data at hand, the identified 8% of all drug targets that show feedback loops has to be seen as a lower limit, i.e. target-regulation appears as a wide-spread biological phenomenon that has to be taken into account during drug development.<br>
<br>
<br>
Materials and Methods<br>
Data source<br>
<database>Connectivity Map</database> (<database>CMap</database>, build 02) contains 6100 gene expression profiles of 4 cell lines treated with 1309 distinct small molecules. Data from <database>Connectivity Map</database> was downloaded from the <database>CMAP</database> main website (http://www.broadinstitute.org/cmap/). On this data set, filtering was performed in multiple steps as follows: Treatment instances (i.e. one treatment versus vehicle control pair) from three cell lines (HL60, Human promyelocytic leukemia cell line; MCF7, Human breast adenocarcinoma cell line; PC3, Human prostate cancer cell line) were taken into consideration. Only the treatment instances from the production batches containing over twenty-five treatments in HT_HG-U133A platform were selected (with the exception of HL60 cell line where HG-U133A microarray platform was also included.) Lastly, for each cell line, the highest concentration of treatments was selected discarding lower concentration treatments. Figure 1 shows the number of treatment instances used in this study before and after filtering. In total, we analyze here a total of 4849 treatment instances in three cell lines corresponding to 1144 small molecules, in which 989 of them are tested in each cell line.<br>
<br>
Data pre-processing<br>
Treatment arrays were grouped based on the cell line. For the HL60 cell line, treatments from different microarray platforms were further classified in separate groups. Each group was pre-processed separately using RMA [31]. Vehicle controls from <database>CMap</database> were discarded and for each batch individual probes of each treatment were mean centered to calculate the average difference values within the batch. To construct a unique gene expression profile of a small molecule for each cell line, replicate treatments were merged into one averaging their probe sets values. For the HL60 cell line, profiles from multiple microarray platforms were not merged because there was not any experiment with the same drug treatment from different microarray platforms.<br>
The probe sets for the small-molecule gene expression profile were ranked based on both their detection call and their average log-signal difference value for the probe set [32]. Detection calls were assigned on the probe sets for individual experiments. A probe set was labeled to be ?Present in a cell line? if the detection call algorithm had assigned ?present? for that probe set in at least half of the drug treatment experiments in that corresponding cell line. Ranking was performed in two steps. First, for probe sets assigned to be ?Absent? in tested cell line, the average difference was set to 0. Next, all probe sets were ranked in the descending order of their average difference. Last, to get the final ranked gene expression profile, the probe sets, which were set to 0, were sub-sorted based on their initial average difference.<br>
<br>
Pairwise similarity score of drugs<br>
Pairwise similarity scores between small-molecule gene expression profiles were calculated using a similar method presented in Iorio et al. [9]. An optimal signature was created for each gene expression profile of the drug. This optimal signature consists of the top 250 and bottom 250 ranked probe sets in the gene expression profile. These probe sets are the characteristic cellular response of the drug treatment that might be specific to cell line. To get the similarity score between drug X and Y: The down-regulated and up-regulated features of the optimal signature from drug X were searched within the weighted gene expression profile of drug Y. In same respect, top and down regulated signature genes of drug Y were also searched within the weighted gene expression profile of Drug X. To quantify a similarity score, gene set enrichment analysis (GSEA) based on Kolmogorov-Smirnov statistics were used [12]. All results obtained through GSEA were averaged to obtain the final score of gene expression profile comparison for a drug pair in a specific cell line.<br>
As we might have up to three expression profiles of the small-molecule treatment from three cell lines, it is possible to compare gene expression profiles of the drug pairs within and between cell lines. In this study, drug-induced gene expression profile similarity scores were only calculated within cell lines. To increase reliability of DIPS score, a combined similarity score was calculated as the average of the similarity scores for the drug pair from multiple cell lines.<br>
<br>
Drug target expression changes<br>
A drug can act on the protein if the protein is physically present in the cell. A drug target was considered to be ?expressed? and present if the detection call algorithm [32] reports it as such for one-tenth of the treatment experiments in the cell line. Drug targets that were not expressed and labeled ?absent? were excluded. Drug target information was gathered from <database>STITCH</database> 2.0 including actions of the interaction. These interactions were labeled as ?activation? or ?inhibition? (including ?binding?). To minimize indirect associations, only the drug-target relations from experimental and curated database annotations over 0.7 threshold were taken into consideration. For the significant cases of drug-induced differential regulation of drug targets, ?binding? associations are manually corrected and the results are re-calculated.<br>
Drug-induced gene-expression of a drug target used in this context indicates the expression change of the target mRNA upon drug treatment acting on the corresponding target. Significance of the expression changes of the drug target were evaluated by comparing the drug-induced expression changes of target mRNAs with the expression changes of the target upon all other chemical treatments present in <database>CMap</database>. ANOVA was used to assess the significance for the expressional change of individual drug targets from multiple cell lines. Table S1 provides the t-test results for the expression changes of drug targets for individual cell lines.<br>
<br>
<br>
Supporting Information<br>
<br>
<br>
<br>
<p><hr><p>

</body></html>
